---
title: Crawl4AI ì™„ë²½ ê°€ì´ë“œ - í•œê¸€íŒ
created: 2025-06-24
last_modified: 2025-06-24
tags:
  - AI/ì›¹í¬ë¡¤ë§
  - Python/ë¼ì´ë¸ŒëŸ¬ë¦¬
  - ë°ì´í„°ìˆ˜ì§‘
  - LLM/RAG
  - ìë™í™”/ë„êµ¬
  - ì˜¤í”ˆì†ŒìŠ¤
  - ì›¹ìŠ¤í¬ë˜í•‘
  - Claude-Code
status: ì™„ë£Œ
type: ê°€ì´ë“œ
priority: high
share_link: https://share.note.sx/30t04dar#bZihAi6H3McLlTgnt0wfZvxP2abeWmELQlSiR6Cl9/0
share_updated: 2025-06-24T23:47:15+09:00
---

# ğŸš€ Crawl4AI ì™„ë²½ ê°€ì´ë“œ - í•œê¸€íŒ

> **Crawl4AI**ëŠ” LLM ì¹œí™”ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ì›¹ í¬ë¡¤ëŸ¬ ë° ìŠ¤í¬ë˜í¼ì…ë‹ˆë‹¤. AI ì—ì´ì „íŠ¸ì™€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ ì„¤ê³„ëœ ì´ˆê³ ì† ì›¹ í¬ë¡¤ë§ ë„êµ¬ë¡œ, ê¹ƒí—ˆë¸Œ #1 íŠ¸ë Œë”© ì €ì¥ì†Œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
1. [[#Crawl4AIë€ ë¬´ì—‡ì¸ê°€?]]
2. [[#ì™œ Crawl4AIë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?]]
3. [[#ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •]]
4. [[#ê¸°ë³¸ ì‚¬ìš©ë²•]]
5. [[#ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©]]
6. [[#ì‹¤ì „ ì˜ˆì œ ëª¨ìŒ]]
7. [[#ë¬¸ì œ í•´ê²° ë° íŒ]]
8. [[#ê²°ë¡  ë° ì¶”ê°€ ìë£Œ]]

## Crawl4AIë€ ë¬´ì—‡ì¸ê°€?

Crawl4AIëŠ” 2023ë…„ ê°œë°œìê°€ ì§ì ‘ ê²ªì€ ë¶ˆí¸í•¨ì—ì„œ ì‹œì‘ëœ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ë“¤ì´ ìœ ë£Œí™”ë˜ê±°ë‚˜ í’ˆì§ˆì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ **ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬**ì…ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- ğŸ¤– **LLM ìµœì í™”**: RAG ë° íŒŒì¸íŠœë‹ì— ìµœì í™”ëœ ê¹”ë”í•œ ë§ˆí¬ë‹¤ìš´ ìƒì„±
- âš¡ **ì´ˆê³ ì† ì„±ëŠ¥**: ê¸°ì¡´ ë„êµ¬ ëŒ€ë¹„ 6ë°° ë¹ ë¥¸ ì‹¤ì‹œê°„ ì„±ëŠ¥
- ğŸ”“ **ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤**: API í‚¤ ë¶ˆí•„ìš”, Docker ë° í´ë¼ìš°ë“œ ë°°í¬ ê°€ëŠ¥
- ğŸŒ **ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì € ì§€ì›**: Chromium, Firefox, WebKit í˜¸í™˜
- ğŸ¯ **ìŠ¤í…”ìŠ¤ ëª¨ë“œ**: ë´‡ ê°ì§€ íšŒí”¼ ê¸°ëŠ¥ ë‚´ì¥

## ì™œ Crawl4AIë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?

### 1. LLM ì‹œëŒ€ì˜ í•„ìˆ˜ ë„êµ¬
- ì›¹ ì½˜í…ì¸ ë¥¼ AIê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ìë™ ë³€í™˜
- ë…¸ì´ì¦ˆ ì œê±° ë° í•µì‹¬ ì½˜í…ì¸  ì¶”ì¶œ
- êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì§€ì›

### 2. ê°œë°œì ì¹œí™”ì 
- ê°„ë‹¨í•œ Python API
- í’ë¶€í•œ ì˜ˆì œì™€ ë¬¸ì„œ
- í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ì§€ì›

### 3. ë¬´ë£Œ ë° íˆ¬ëª…ì„±
- ì™„ì „ ë¬´ë£Œ, ìˆ¨ê²¨ì§„ ë¹„ìš© ì—†ìŒ
- ì†ŒìŠ¤ ì½”ë“œ 100% ê³µê°œ
- ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´ì¥

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- Python 3.8 ì´ìƒ
- macOS, Linux, Windows ì§€ì›
- ìµœì†Œ 4GB RAM (8GB ì´ìƒ ê¶Œì¥)

### ê¸°ë³¸ ì„¤ì¹˜

```bash
# 1. pipë¥¼ í†µí•œ ì„¤ì¹˜
pip install -U crawl4ai

# í”„ë¦¬ë¦´ë¦¬ì¦ˆ ë²„ì „ ì„¤ì¹˜ (ìµœì‹  ê¸°ëŠ¥)
pip install crawl4ai --pre

# 2. ë¸Œë¼ìš°ì € ì„¤ì • (ìë™ìœ¼ë¡œ Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜)
crawl4ai-setup

# 3. ì„¤ì¹˜ í™•ì¸
crawl4ai-doctor
```

### Playwright ìˆ˜ë™ ì„¤ì¹˜ (í•„ìš”ì‹œ)

```bash
# crawl4ai-setupì´ ì‹¤íŒ¨í•  ê²½ìš° ìˆ˜ë™ ì„¤ì¹˜
python -m playwright install --with-deps chromium

# ë˜ëŠ” íŠ¹ì • ë¸Œë¼ìš°ì €ë§Œ ì„¤ì¹˜
python -m playwright install chromium
```

### ê°€ìƒí™˜ê²½ ì„¤ì • (ê¶Œì¥)

```bash
# í”„ë¡œì íŠ¸ í´ë” ìƒì„±
mkdir crawl4ai-project
cd crawl4ai-project

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv crawl4ai-env
source crawl4ai-env/bin/activate  # Windows: crawl4ai-env\Scripts\activate

# Crawl4AI ì„¤ì¹˜
pip install -U crawl4ai
crawl4ai-setup

# ì„¤ì¹˜ í™•ì¸ (Health Check)
crawl4ai-doctor
```

### ê°œë°œ í™˜ê²½ ì„¤ì¹˜

```bash
# ì†ŒìŠ¤ì½”ë“œë¡œë¶€í„° ì„¤ì¹˜ (ê¸°ì—¬ììš©)
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .  # í¸ì§‘ ê°€ëŠ¥ ëª¨ë“œë¡œ ì„¤ì¹˜

# ì„ íƒì  ê¸°ëŠ¥ ì„¤ì¹˜
pip install -e ".[torch]"       # PyTorch ê¸°ëŠ¥
pip install -e ".[transformer]" # Transformer ê¸°ëŠ¥
pip install -e ".[cosine]"      # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ëŠ¥
pip install -e ".[all]"         # ëª¨ë“  ì„ íƒì  ê¸°ëŠ¥
```

### Docker ì„¤ì¹˜ (v0.6.0 ìµœì‹ )

```bash
# ìµœì‹  Docker ì´ë¯¸ì§€ ì‹¤í–‰
docker pull unclecode/crawl4ai:0.6.0-r5  # ë²„ì „ ë²ˆí˜¸ëŠ” ìµœì‹  í™•ì¸
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-r5

# ì›¹ í”Œë ˆì´ê·¸ë¼ìš´ë“œ ì ‘ì†
# http://localhost:11235/playground

# Docker Compose ì‚¬ìš©
docker-compose up -d
```

### ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ (CLI)

```bash
# ê¸°ë³¸ í¬ë¡¤ë§
crwl https://example.com -o markdown

# ì‹¬ì¸µ í¬ë¡¤ë§ (BFS ì „ëµ)
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# LLM ì¶”ì¶œ ì‚¬ìš©
crwl https://example.com/products -q "ëª¨ë“  ìƒí’ˆ ê°€ê²© ì¶”ì¶œ"

# ë„ì›€ë§
crwl --help
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê°„ë‹¨í•œ ì›¹í˜ì´ì§€ í¬ë¡¤ë§

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def basic_crawl():
    """ê¸°ë³¸ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì œ"""
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com"
        )
        
        print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {result.success}")
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ê¸¸ì´: {len(result.markdown)} ê¸€ì")
        print(f"\në‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{result.markdown[:500]}...")
        
        # íŒŒì¼ë¡œ ì €ì¥
        with open("example_content.md", "w", encoding="utf-8") as f:
            f.write(result.markdown)

# ì‹¤í–‰
asyncio.run(basic_crawl())
```

### 2. ë¸Œë¼ìš°ì € ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from crawl4ai import AsyncWebCrawler, BrowserConfig

async def custom_browser_crawl():
    """ë¸Œë¼ìš°ì € ì„¤ì •ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•œ í¬ë¡¤ë§"""
    browser_config = BrowserConfig(
        headless=True,      # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (í™”ë©´ í‘œì‹œ ì•ˆí•¨)
        verbose=True,       # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        viewport_width=1920,
        viewport_height=1080
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.naver.com"
        )
        print(f"âœ… ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ!")

asyncio.run(custom_browser_crawl())
```

### 3. ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜

```python
from crawl4ai import CrawlerRunConfig

async def capture_screenshot():
    """ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜"""
    run_config = CrawlerRunConfig(
        screenshot=True,  # ìŠ¤í¬ë¦°ìƒ· í™œì„±í™”
        pdf=True         # PDFë¡œë„ ì €ì¥
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://github.com/unclecode/crawl4ai",
            config=run_config
        )
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        if result.screenshot:
            with open("github_crawl4ai.png", "wb") as f:
                f.write(result.screenshot)
            print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì™„ë£Œ!")

asyncio.run(capture_screenshot())
```

## ë²„ì „ 0.6.0 ì‹ ê¸°ëŠ¥

### 1. ì„¸ê³„ ì¸ì‹ í¬ë¡¤ë§ (World-aware Crawling)

```python
from crawl4ai import GeolocationConfig

async def location_aware_crawl():
    """ì§€ì—­ë³„ ë§ì¶¤ ì½˜í…ì¸  í¬ë¡¤ë§"""
    
    run_config = CrawlerRunConfig(
        # í•œêµ­ ì„œìš¸ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        locale="ko-KR",                          # ì–¸ì–´ ë° ì§€ì—­ ì„¤ì •
        timezone_id="Asia/Seoul",                # ì‹œê°„ëŒ€ ì„¤ì •
        geolocation=GeolocationConfig(
            latitude=37.5665,                    # ì„œìš¸ ìœ„ë„
            longitude=126.9780,                  # ì„œìš¸ ê²½ë„
            accuracy=10.0
        )
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.google.com",        # ì§€ì—­ë³„ë¡œ ë‹¤ë¥¸ ì½˜í…ì¸  í‘œì‹œ
            config=run_config
        )
        print("ğŸŒ í•œêµ­ ì§€ì—­ ë§ì¶¤ ì½˜í…ì¸  í¬ë¡¤ë§ ì™„ë£Œ!")

asyncio.run(location_aware_crawl())
```

### 2. í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ì¶”ì¶œ

```python
import pandas as pd

async def extract_tables_to_dataframe():
    """HTML í…Œì´ë¸”ì„ pandas DataFrameìœ¼ë¡œ ì¶”ì¶œ"""
    
    run_config = CrawlerRunConfig(
        table_score_threshold=8,  # í…Œì´ë¸” ê°ì§€ ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì—„ê²©)
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://finance.yahoo.com/markets/stocks/most-active/",
            config=run_config
        )
        
        # í…Œì´ë¸”ì´ ìˆëŠ” ê²½ìš° DataFrameìœ¼ë¡œ ë³€í™˜
        if result.media and result.media.get("tables"):
            for i, table in enumerate(result.media["tables"]):
                df = pd.DataFrame(
                    table["rows"],
                    columns=table["headers"]
                )
                print(f"\nğŸ“Š í…Œì´ë¸” {i+1}:")
                print(df.head())
                
                # CSVë¡œ ì €ì¥
                df.to_csv(f"table_{i+1}.csv", index=False, encoding="utf-8")
                print(f"ğŸ’¾ table_{i+1}.csv ì €ì¥ ì™„ë£Œ!")

asyncio.run(extract_tables_to_dataframe())
```

### 3. ë„¤íŠ¸ì›Œí¬ ë° ì½˜ì†” íŠ¸ë˜í”½ ìº¡ì²˜

```python
async def capture_network_traffic():
    """ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë° ì½˜ì†” ë©”ì‹œì§€ ìº¡ì²˜"""
    
    run_config = CrawlerRunConfig(
        capture_network=True,    # ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ìº¡ì²˜
        capture_console=True,    # ì½˜ì†” ë©”ì‹œì§€ ìº¡ì²˜
        mhtml=True              # MHTML ìŠ¤ëƒ…ìƒ· ì €ì¥
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
        if hasattr(result, 'network_requests'):
            print("ğŸŒ ë„¤íŠ¸ì›Œí¬ ìš”ì²­:")
            for req in result.network_requests[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                print(f"  - {req['method']} {req['url']}")
        
        # ì½˜ì†” ë©”ì‹œì§€ í™•ì¸
        if hasattr(result, 'console_messages'):
            print("\nğŸ’¬ ì½˜ì†” ë©”ì‹œì§€:")
            for msg in result.console_messages[:5]:
                print(f"  - [{msg['level']}] {msg['text']}")

asyncio.run(capture_network_traffic())
```

### 4. ë¸Œë¼ìš°ì € í’€ë§ (ì„±ëŠ¥ ìµœì í™”)

```python
async def browser_pooling_example():
    """ë¸Œë¼ìš°ì € í’€ì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ í¬ë¡¤ë§"""
    
    # ë¸Œë¼ìš°ì € í’€ ì„¤ì •
    browser_config = BrowserConfig(
        pool_size=3,           # ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜
        reuse_browser=True,    # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©
    )
    
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com",
        "https://example4.com",
        "https://example5.com",
    ]
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # ì—¬ëŸ¬ URL ë™ì‹œ í¬ë¡¤ë§
        tasks = [crawler.arun(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        print(f"âœ… {len(results)}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ!")
        for i, result in enumerate(results):
            if result.success:
                print(f"  - {urls[i]}: {len(result.markdown)} ê¸€ì")

asyncio.run(browser_pooling_example())
```

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### 1. ìŠ¤ë§ˆíŠ¸ ë§ˆí¬ë‹¤ìš´ ìƒì„± (ë…¸ì´ì¦ˆ ì œê±°)

```python
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def smart_markdown_generation():
    """ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œ ê¹”ë”í•œ ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    
    browser_config = BrowserConfig(headless=True)
    
    # ì½˜í…ì¸  í•„í„° ì„¤ì • - ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
    run_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(
                threshold=0.48,  # ê´€ë ¨ì„± ì„ê³„ê°’
                threshold_type="fixed",
                min_word_threshold=50  # ìµœì†Œ ë‹¨ì–´ ìˆ˜
            )
        )
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://ko.wikipedia.org/wiki/ì¸ê³µì§€ëŠ¥",
            config=run_config
        )
        
        print(f"ğŸ§¹ ì›ë³¸ ë§ˆí¬ë‹¤ìš´: {len(result.markdown.raw_markdown)} ê¸€ì")
        print(f"âœ¨ ì •ì œëœ ë§ˆí¬ë‹¤ìš´: {len(result.markdown.fit_markdown)} ê¸€ì")
        print(f"ğŸ“‰ ì¶•ì†Œìœ¨: {100 - (len(result.markdown.fit_markdown) / len(result.markdown.raw_markdown) * 100):.1f}%")
        
        # ì •ì œëœ ì½˜í…ì¸  ì €ì¥
        with open("ai_wikipedia_cleaned.md", "w", encoding="utf-8") as f:
            f.write(result.markdown.fit_markdown)

asyncio.run(smart_markdown_generation())
```

### 2. ì¿¼ë¦¬ ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì¶œ (BM25)

```python
from crawl4ai.content_filter_strategy import BM25ContentFilter

async def query_based_extraction():
    """íŠ¹ì • ì£¼ì œì— ì§‘ì¤‘í•œ ì½˜í…ì¸  ì¶”ì¶œ"""
    
    # ê´€ì‹¬ ì£¼ì œ ì„¤ì •
    query = "ë¨¸ì‹ ëŸ¬ë‹ ì˜ë£Œ ì‘ìš©"
    
    run_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=BM25ContentFilter(
                user_query=query,
                bm25_threshold=1.0
            )
        )
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://ko.wikipedia.org/wiki/ì¸ê³µì§€ëŠ¥",
            config=run_config
        )
        
        print(f"ğŸ¯ ê²€ìƒ‰ì–´: '{query}'")
        print(f"ğŸ“Š ê´€ë ¨ ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ!")
        
        with open("ai_medical_focused.md", "w", encoding="utf-8") as f:
            f.write(f"# {query} ê´€ë ¨ ë‚´ìš©\n\n")
            f.write(result.markdown.fit_markdown)

asyncio.run(query_based_extraction())
```

### 3. JavaScript ì‹¤í–‰ ë° ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

```python
async def handle_dynamic_content():
    """JavaScriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§"""
    
    run_config = CrawlerRunConfig(
        # JavaScript ì½”ë“œ ì‹¤í–‰
        js_code=["""
            // ëª¨ë“  íƒ­ í´ë¦­í•˜ì—¬ ì½˜í…ì¸  ë¡œë“œ
            const tabs = document.querySelectorAll('.tab-button');
            for(let tab of tabs) {
                tab.click();
                await new Promise(r => setTimeout(r, 1000));
            }
        """],
        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        wait_for="css:.content-loaded",
        delay_after_load=2.0  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example-spa.com",
            config=run_config
        )
        print("ğŸ¯ ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§ ì™„ë£Œ!")

asyncio.run(handle_dynamic_content())
```

### 4. LLMì„ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ

```python
import os
from crawl4ai import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

# ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
class Product(BaseModel):
    name: str = Field(..., description="ìƒí’ˆëª…")
    price: str = Field(..., description="ê°€ê²©")
    description: str = Field(..., description="ìƒí’ˆ ì„¤ëª…")
    availability: str = Field(..., description="ì¬ê³  ìƒíƒœ")

async def extract_structured_data():
    """LLMì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ"""
    
    run_config = CrawlerRunConfig(
        extraction_strategy=LLMExtractionStrategy(
            # OpenAI API ì‚¬ìš© (ë‹¤ë¥¸ LLMë„ ê°€ëŠ¥)
            llm_config=LLMConfig(
                provider="openai/gpt-4o-mini",
                api_token=os.getenv('OPENAI_API_KEY')
            ),
            schema=Product.schema(),
            extraction_type="schema",
            instruction="""
            ì›¹í˜ì´ì§€ì—ì„œ ëª¨ë“  ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
            ê° ìƒí’ˆì˜ ì´ë¦„, ê°€ê²©, ì„¤ëª…, ì¬ê³  ìƒíƒœë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
            """
        )
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://shopping-site.com/products",
            config=run_config
        )
        
        # ì¶”ì¶œëœ ë°ì´í„° ì¶œë ¥
        import json
        products = json.loads(result.extracted_content)
        print(f"ğŸ›ï¸ {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ ì™„ë£Œ!")
        
        for product in products[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\nìƒí’ˆëª…: {product['name']}")
            print(f"ê°€ê²©: {product['price']}")
            print(f"ì¬ê³ : {product['availability']}")

asyncio.run(extract_structured_data())
```

### 5. ì‚¬ìš©ì í”„ë¡œí•„ ë° ì„¸ì…˜ ê´€ë¦¬

```python
import os
from pathlib import Path

async def use_persistent_profile():
    """ë¸Œë¼ìš°ì € í”„ë¡œí•„ì„ ì‚¬ìš©í•œ ì„¸ì…˜ ìœ ì§€"""
    
    # í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ìƒì„±
    user_data_dir = os.path.join(Path.home(), ".crawl4ai", "browser_profile")
    os.makedirs(user_data_dir, exist_ok=True)
    
    browser_config = BrowserConfig(
        headless=False,  # ë¡œê·¸ì¸ì„ ìœ„í•´ í™”ë©´ í‘œì‹œ
        user_data_dir=user_data_dir,
        use_persistent_context=True,  # ì˜êµ¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # ì²« ë²ˆì§¸ ë°©ë¬¸ - ë¡œê·¸ì¸
        print("ğŸ” ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™...")
        result = await crawler.arun(
            url="https://github.com/login",
            config=CrawlerRunConfig(
                wait_for="css:input[name='login']",  # ë¡œê·¸ì¸ í¼ ëŒ€ê¸°
                delay_after_load=2.0
            )
        )
        
        # ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸ í›„ Enter í‚¤ ì…ë ¥ ëŒ€ê¸°
        input("ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸ í›„ Enter í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        # ë‘ ë²ˆì§¸ ë°©ë¬¸ - ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€
        print("âœ… í”„ë¡œí•„ í˜ì´ì§€ ì ‘ê·¼...")
        result = await crawler.arun(
            url="https://github.com/settings/profile"
        )
        
        if "Your profile" in result.markdown:
            print("ğŸ‰ ë¡œê·¸ì¸ ìƒíƒœê°€ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")

asyncio.run(use_persistent_profile())
```

### 6. í”„ë¡ì‹œ ì„¤ì •

```python
async def crawl_with_proxy():
    """í”„ë¡ì‹œë¥¼ í†µí•œ í¬ë¡¤ë§"""
    
    # í”„ë¡ì‹œ ì„¤ì •
    browser_config = BrowserConfig(
        proxy="http://proxy.example.com:8080",
        # ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
        proxy_username="username",
        proxy_password="password"
    )
    
    # ë˜ëŠ” ë” ì„¸ë¶€ì ì¸ í”„ë¡ì‹œ ì„¤ì •
    browser_config = BrowserConfig(
        proxy={
            "server": "http://proxy.example.com:8080",
            "username": "user",
            "password": "pass",
            "bypass": ["localhost", "127.0.0.1"]  # í”„ë¡ì‹œ ìš°íšŒ ì£¼ì†Œ
        }
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://httpbin.org/ip"  # IP í™•ì¸
        )
        print(f"ğŸŒ í”„ë¡ì‹œ IP: {result.markdown}")

# asyncio.run(crawl_with_proxy())
```

### 7. CSS ì„ íƒìë¥¼ ì´ìš©í•œ ë°ì´í„° ì¶”ì¶œ

```python
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def css_based_extraction():
    """CSS ì„ íƒìë¡œ ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ"""
    
    # ì¶”ì¶œ ìŠ¤í‚¤ë§ˆ ì •ì˜
    schema = {
        "name": "ë‰´ìŠ¤ ê¸°ì‚¬",
        "baseSelector": "article.news-item",
        "fields": [
            {
                "name": "title",
                "selector": "h2.article-title",
                "type": "text"
            },
            {
                "name": "author",
                "selector": ".author-name",
                "type": "text"
            },
            {
                "name": "date",
                "selector": ".publish-date",
                "type": "text"
            },
            {
                "name": "content",
                "selector": ".article-content",
                "type": "text"
            },
            {
                "name": "image_url",
                "selector": "img.article-image",
                "type": "attribute",
                "attribute": "src"
            }
        ]
    }
    
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
    
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news-site.com",
            config=run_config
        )
        
        articles = json.loads(result.extracted_content)
        print(f"ğŸ“° {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ!")
        
        # ì¶”ì¶œëœ ê¸°ì‚¬ ì €ì¥
        with open("news_articles.json", "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

asyncio.run(css_based_extraction())
```

## ì‹¤ì „ ì˜ˆì œ ëª¨ìŒ

### 1. ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ëª¨ë‹ˆí„°ë§

```python
async def monitor_news_site():
    """ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì •ê¸° ëª¨ë‹ˆí„°ë§"""
    
    from datetime import datetime
    import asyncio
    
    async def crawl_news():
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(
                url="https://news.ycombinator.com",
                config=CrawlerRunConfig(
                    css_selector=".athing"  # íŠ¹ì • ìš”ì†Œë§Œ ì¶”ì¶œ
                )
            )
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"hn_news_{timestamp}.md"
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"# Hacker News - {timestamp}\n\n")
                f.write(result.markdown)
            
            print(f"âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ: {filename}")
    
    # 30ë¶„ë§ˆë‹¤ í¬ë¡¤ë§
    while True:
        await crawl_news()
        print("â° 30ë¶„ í›„ ë‹¤ì‹œ í¬ë¡¤ë§...")
        await asyncio.sleep(1800)  # 30ë¶„

# ì‹¤í–‰ (Ctrl+Cë¡œ ì¤‘ì§€)
# asyncio.run(monitor_news_site())
```

### 2. ì „ììƒê±°ë˜ ê°€ê²© ì¶”ì 

```python
async def track_product_prices():
    """ìƒí’ˆ ê°€ê²© ë³€ë™ ì¶”ì """
    
    products = [
        {
            "name": "ë§¥ë¶ í”„ë¡œ",
            "url": "https://www.apple.com/kr/shop/buy-mac/macbook-pro",
            "price_selector": ".price"
        },
        {
            "name": "ê°¤ëŸ­ì‹œ S24",
            "url": "https://www.samsung.com/kr/smartphones/galaxy-s24/",
            "price_selector": ".product-price"
        }
    ]
    
    price_history = []
    
    async with AsyncWebCrawler() as crawler:
        for product in products:
            result = await crawler.arun(
                url=product["url"],
                config=CrawlerRunConfig(
                    css_selector=product["price_selector"]
                )
            )
            
            # ê°€ê²© ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            price_text = result.markdown.strip()
            
            price_history.append({
                "product": product["name"],
                "price": price_text,
                "timestamp": datetime.now().isoformat()
            })
            
            print(f"ğŸ’° {product['name']}: {price_text}")
    
    # CSVë¡œ ì €ì¥
    import csv
    with open("price_history.csv", "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["product", "price", "timestamp"])
        writer.writerows(price_history)

asyncio.run(track_product_prices())
```

### 3. í•™ìˆ  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

```python
async def collect_paper_metadata():
    """arXiv ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
    
    schema = {
        "name": "arXiv ë…¼ë¬¸",
        "baseSelector": ".arxiv-result",
        "fields": [
            {
                "name": "title",
                "selector": ".title",
                "type": "text"
            },
            {
                "name": "authors",
                "selector": ".authors",
                "type": "text"
            },
            {
                "name": "abstract",
                "selector": ".abstract",
                "type": "text"
            },
            {
                "name": "pdf_link",
                "selector": "a.pdf",
                "type": "attribute",
                "attribute": "href"
            }
        ]
    }
    
    run_config = CrawlerRunConfig(
        extraction_strategy=JsonCssExtractionStrategy(schema)
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://arxiv.org/list/cs.AI/recent",
            config=run_config
        )
        
        papers = json.loads(result.extracted_content)
        print(f"ğŸ“š {len(papers)}ê°œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # Obsidian ë…¸íŠ¸ë¡œ ì €ì¥
        for paper in papers[:5]:
            note_content = f"""---
title: "{paper['title']}"
authors: "{paper['authors']}"
source: "arXiv"
type: "ë…¼ë¬¸"
tags:
  - AI/ë…¼ë¬¸
  - ì—°êµ¬/ìµœì‹ 
---

# {paper['title']}

## ì €ì
{paper['authors']}

## ì´ˆë¡
{paper['abstract']}

## ë§í¬
- [PDF ë‹¤ìš´ë¡œë“œ]({paper['pdf_link']})
"""
            # íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ë³€í™˜
            safe_title = "".join(c for c in paper['title'] if c.isalnum() or c in " -_")[:50]
            with open(f"paper_{safe_title}.md", "w", encoding="utf-8") as f:
                f.write(note_content)

asyncio.run(collect_paper_metadata())
```

### 4. ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„

```python
async def analyze_social_trends():
    """ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶„ì„"""
    
    from collections import Counter
    import re
    
    async with AsyncWebCrawler() as crawler:
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        sources = [
            "https://trends.google.com/trends/trendingsearches/daily?geo=KR",
            "https://www.daum.net",
            "https://www.naver.com"
        ]
        
        all_text = ""
        
        for url in sources:
            result = await crawler.arun(url)
            if result.success:
                all_text += result.markdown + " "
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', all_text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì˜', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ê³¼', 'ì™€', 'í•œ', 'í•˜ë‹¤'}
        filtered_words = [word for word in korean_words if len(word) > 1 and word not in stopwords]
        
        # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ
        word_counts = Counter(filtered_words)
        top_keywords = word_counts.most_common(20)
        
        print("ğŸ”¥ í˜„ì¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œ TOP 20:")
        for i, (word, count) in enumerate(top_keywords, 1):
            print(f"{i}. {word} ({count}íšŒ)")
        
        # ê²°ê³¼ ì €ì¥
        with open("trend_keywords.txt", "w", encoding="utf-8") as f:
            f.write(f"íŠ¸ë Œë“œ ë¶„ì„ - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            for word, count in top_keywords:
                f.write(f"{word}: {count}\n")

asyncio.run(analyze_social_trends())
```

## ë¬¸ì œ í•´ê²° ë° íŒ

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ë°©ë²•

#### 1. Playwright ì„¤ì¹˜ ì˜¤ë¥˜
```bash
# ìˆ˜ë™ìœ¼ë¡œ Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜
python -m playwright install chromium
python -m playwright install-deps
```

#### 2. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
```python
# ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ìµœì í™”
browser_config = BrowserConfig(
    headless=True,
    browser_type="chromium",  # Firefoxë³´ë‹¤ ê°€ë²¼ì›€
    extra_args=["--disable-gpu", "--no-sandbox"]
)
```

#### 3. ë´‡ ê°ì§€ íšŒí”¼
```python
# ìŠ¤í…”ìŠ¤ ëª¨ë“œ í™œì„±í™”
run_config = CrawlerRunConfig(
    magic=True,  # ìë™ ë´‡ ê°ì§€ íšŒí”¼
    delay_after_load=3.0,  # ì‚¬ëŒì²˜ëŸ¼ ëŒ€ê¸°
    js_code=["window.scrollTo(0, document.body.scrollHeight);"]  # ìŠ¤í¬ë¡¤ ë™ì‘
)
```

#### 4. ì¸ì½”ë”© ë¬¸ì œ
```python
# UTF-8 ì¸ì½”ë”© ëª…ì‹œ
with open("output.md", "w", encoding="utf-8") as f:
    f.write(result.markdown)
```

### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ìºì‹± í™œìš©**
```python
from crawl4ai import CacheMode

run_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED  # ìºì‹œ í™œì„±í™”
)
```

2. **ë™ì‹œ í¬ë¡¤ë§**
```python
async def concurrent_crawl(urls):
    async with AsyncWebCrawler() as crawler:
        tasks = [crawler.arun(url) for url in urls]
        results = await asyncio.gather(*tasks)
    return results
```

3. **ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ**
```python
run_config = CrawlerRunConfig(
    css_selector="article.main-content",  # í•„ìš”í•œ ë¶€ë¶„ë§Œ
    exclude_selectors=["nav", "footer", ".ads"]  # ì œì™¸í•  ìš”ì†Œ
)
```

## ì‹¤ì „ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""Crawl4AI ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""

import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def test_basic_functionality():
    """ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê¸°ë³¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
    
    async with AsyncWebCrawler() as crawler:
        # 1. ê¸°ë³¸ í¬ë¡¤ë§
        result = await crawler.arun("https://example.com")
        assert result.success, "ê¸°ë³¸ í¬ë¡¤ë§ ì‹¤íŒ¨"
        print("  âœ… ê¸°ë³¸ í¬ë¡¤ë§ ì„±ê³µ")
        
        # 2. ì†ì„± í™•ì¸
        print(f"  - URL: {result.url}")
        print(f"  - ì„±ê³µ ì—¬ë¶€: {result.success}")
        print(f"  - ì½˜í…ì¸  ê¸¸ì´: {len(result.markdown)} ê¸€ì")
        
        # 3. ìŠ¤í¬ë¦°ìƒ· í…ŒìŠ¤íŠ¸
        result_with_screenshot = await crawler.arun(
            "https://example.com",
            config=CrawlerRunConfig(screenshot=True)
        )
        assert result_with_screenshot.screenshot, "ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨"
        print("  âœ… ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì„±ê³µ")

async def test_advanced_features():
    """ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
    
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # 1. CSS ì„ íƒì í…ŒìŠ¤íŠ¸
        result = await crawler.arun(
            "https://example.com",
            config=CrawlerRunConfig(
                css_selector="body"
            )
        )
        print("  âœ… CSS ì„ íƒì í¬ë¡¤ë§ ì„±ê³µ")
        
        # 2. ìºì‹± í…ŒìŠ¤íŠ¸
        import time
        start = time.time()
        result1 = await crawler.arun(
            "https://example.com",
            config=CrawlerRunConfig(cache_mode=CacheMode.ENABLED)
        )
        time1 = time.time() - start
        
        start = time.time()
        result2 = await crawler.arun(
            "https://example.com",
            config=CrawlerRunConfig(cache_mode=CacheMode.ENABLED)
        )
        time2 = time.time() - start
        
        print(f"  âœ… ìºì‹± í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì²« ë²ˆì§¸: {time1:.2f}ì´ˆ, ë‘ ë²ˆì§¸: {time2:.2f}ì´ˆ)")
        assert time2 < time1, "ìºì‹±ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ"

async def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Crawl4AI ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        await test_basic_functionality()
        await test_advanced_features()
        print("\nâœ¨ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ë²„ì „ ì •ë³´ ë° ì—…ë°ì´íŠ¸

### í˜„ì¬ ë²„ì „ í™•ì¸

```python
import crawl4ai
print(f"Crawl4AI ë²„ì „: {crawl4ai.__version__}")
```

### ë²„ì „ë³„ ì£¼ìš” ë³€ê²½ì‚¬í•­
- **v0.6.3** (ìµœì‹ ): ì•ˆì •ì„± ê°œì„ 
- **v0.6.0**: World-aware í¬ë¡¤ë§, í…Œì´ë¸” ì¶”ì¶œ, ë¸Œë¼ìš°ì € í’€ë§
- **v0.5.0**: Deep í¬ë¡¤ë§, CLI ë„êµ¬, ë¸Œë¼ìš°ì € í”„ë¡œí•„
- **v0.4.x**: LLM í†µí•©, êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ

### ì—…ë°ì´íŠ¸ ë°©ë²•

```bash
# ìµœì‹  ì•ˆì • ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
pip install -U crawl4ai

# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pip install crawl4ai==0.6.3

# í”„ë¦¬ë¦´ë¦¬ì¦ˆ ë²„ì „ ì„¤ì¹˜
pip install crawl4ai --pre
```

## ê²°ë¡  ë° ì¶”ê°€ ìë£Œ

Crawl4AIëŠ” í˜„ëŒ€ì ì¸ ì›¹ í¬ë¡¤ë§ì˜ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. íŠ¹íˆ AI/LLM ì‹œëŒ€ì— ë§ì¶° ì„¤ê³„ë˜ì–´ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì „ì²˜ë¦¬ê¹Œì§€ ì›ìŠ¤í†±ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì¥ì  ìš”ì•½
- âœ… **ë¬´ë£Œ & ì˜¤í”ˆì†ŒìŠ¤**: ì™„ì „ ë¬´ë£Œ, ì†ŒìŠ¤ì½”ë“œ ê³µê°œ
- âœ… **LLM ìµœì í™”**: AIê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë°ì´í„° ë³€í™˜
- âœ… **ì´ˆê³ ì† ì„±ëŠ¥**: ë™ì‹œì„± ì§€ì›, íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©
- âœ… **ë‹¤ì–‘í•œ ê¸°ëŠ¥**: ìŠ¤í¬ë¦°ìƒ·, PDF, êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
- âœ… **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**: ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ì™€ ì§€ì›

### í”„ë¡œì íŠ¸ í†µê³„ (2025ë…„ ê¸°ì¤€)
- â­ **GitHub Stars**: 46.3k+
- ğŸ´ **Forks**: 4.4k+
- ğŸ‘¥ **Contributors**: 38+
- ğŸ“¦ **ì‚¬ìš©ì**: 2,400+ í”„ë¡œì íŠ¸

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- ğŸ“š [ê³µì‹ ë¬¸ì„œ](https://docs.crawl4ai.com/)
- ğŸ’» [GitHub ì €ì¥ì†Œ](https://github.com/unclecode/crawl4ai)
- ğŸ® [ì˜¨ë¼ì¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ](http://localhost:11235/playground) (Docker ì‹¤í–‰ ì‹œ)
- ğŸ’¬ [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/jP8KfhDhyN)
- ğŸ“º [ì˜ˆì œ ì½”ë“œ](https://github.com/unclecode/crawl4ai/tree/main/docs/examples)

### ë‹¤ìŒ ë‹¨ê³„
1. ì‹¤ì œ í”„ë¡œì íŠ¸ì— Crawl4AI ì ìš©í•´ë³´ê¸°
2. ê³ ê¸‰ ê¸°ëŠ¥ íƒìƒ‰ (í”„ë¡ì‹œ, ë¸Œë¼ìš°ì € í”„ë¡œí•„ ë“±)
3. ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬ ë° ê¸°ì—¬

### ë¼ì´ì„ ìŠ¤
Apache License 2.0 - ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥, ì¶œì²˜ í‘œì‹œ í•„ìš”

---

## ì—°ê²°ëœ ë…¸íŠ¸
- [[Python ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ ë¹„êµ]]
- [[LLMì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ]]
- [[ì›¹ í¬ë¡¤ë§ ìœ¤ë¦¬ì™€ ë²•ì  ê³ ë ¤ì‚¬í•­]]
- [[RAG ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ]]
- [[ìë™í™” ë„êµ¬ ëª¨ìŒ]]
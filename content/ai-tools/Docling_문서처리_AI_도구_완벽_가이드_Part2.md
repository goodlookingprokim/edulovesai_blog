---
title: "Docling - ë¬¸ì„œ ì²˜ë¦¬ AI ë„êµ¬ ì™„ë²½ ê°€ì´ë“œ (Part 2: ì‹¤ì „í¸)"
created: '2025-10-14'
last_modified: '2025-10-14'
tags:
  - AI/ë¬¸ì„œì²˜ë¦¬
  - Docling
  - ê³ ê¸‰í™œìš©
  - ì‹¤ì „ì‚¬ë¡€
  - AIì—°ë™
  - ìë™í™”
  - í”„ë¡œë•ì…˜
status: "ì™„ë£Œ"
type: "ë¶„ì„"
priority: "high"
source: "https://github.com/docling-project/docling"
---

# Docling - ë¬¸ì„œ ì²˜ë¦¬ AI ë„êµ¬ ì™„ë²½ ê°€ì´ë“œ (Part 2: ì‹¤ì „í¸)

> **Part 2 ìš”ì•½**: Doclingì˜ ê³ ê¸‰ ê¸°ëŠ¥, AI ì—°ë™, ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ë¥¼ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [[#ê³ ê¸‰ ë³€í™˜ ì˜µì…˜ê³¼ ì„¤ì •]]
2. [[#ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬]]
3. [[#OCRê³¼ ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬]]
4. [[#AI í”„ë ˆì„ì›Œí¬ ì—°ë™]]
5. [[#CLI ëª…ë ¹ì¤„ ë„êµ¬ í™œìš©]]
6. [[#ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ì™€ ìµœì í™”]]
7. [[#ì‹¤ë¬´ í™œìš© ì‚¬ë¡€]]
8. [[#ë¬¸ì œ í•´ê²°ê³¼ íŒ]]

---

## ê³ ê¸‰ ë³€í™˜ ì˜µì…˜ê³¼ ì„¤ì •

### ğŸ›ï¸ DocumentConverter ì„¸ë¶€ ì„¤ì •

#### Level 1 (ì´ˆë³´ì): ê¸°ë³¸ ì˜µì…˜ ì´í•´í•˜ê¸°

**ë³€í™˜ê¸° ìƒì„± ì‹œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤**:
```python
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±
converter = DocumentConverter()

# ë˜ëŠ” íŠ¹ì • í˜•ì‹ë§Œ í—ˆìš©
converter = DocumentConverter(
    allowed_formats=[
        InputFormat.PDF,
        InputFormat.DOCX,
        InputFormat.PPTX
    ]
)
```

**ì™œ ì´ë ‡ê²Œ í• ê¹Œ?**
```python
# ì‹œë‚˜ë¦¬ì˜¤: PDFë§Œ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
converter = DocumentConverter(
    allowed_formats=[InputFormat.PDF]
)

# ì‹¤ìˆ˜ë¡œ ë‹¤ë¥¸ íŒŒì¼ì„ ë„£ìœ¼ë©´:
try:
    converter.convert("image.png")
except Exception as e:
    print("PNGëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
    # ì—ëŸ¬ë¥¼ ë¯¸ë¦¬ ì¡ì•„ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
```

#### Level 2 (ì¤‘ê¸‰ì): ë³€í™˜ í’ˆì§ˆ ì¡°ì ˆ

**í•´ìƒë„ì™€ í’ˆì§ˆ ì„¤ì •**:
```python
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions

# PDF ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
pdf_options = PdfPipelineOptions()
pdf_options.do_ocr = True  # OCR í™œì„±í™”
pdf_options.do_table_structure = True  # í‘œ êµ¬ì¡° ì¸ì‹

converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(
            pipeline_options=pdf_options
        )
    }
)
```

**ì‹¤ì „ ì˜ˆì‹œ - ìŠ¤ìº” ë¬¸ì„œ vs ë””ì§€í„¸ ë¬¸ì„œ**:
```python
# ìŠ¤ìº”ëœ PDF (OCR í•„ìˆ˜)
scanned_options = PdfPipelineOptions()
scanned_options.do_ocr = True
scanned_options.ocr_engine = "tesseract"  # OCR ì—”ì§„ ì„ íƒ

# ë””ì§€í„¸ PDF (OCR ë¶ˆí•„ìš”)
digital_options = PdfPipelineOptions()
digital_options.do_ocr = False  # ë¹ ë¥¸ ì²˜ë¦¬

# ìƒí™©ì— ë§ëŠ” ë³€í™˜ê¸° ì‚¬ìš©
if is_scanned(pdf_file):
    converter = DocumentConverter(
        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=scanned_options)}
    )
else:
    converter = DocumentConverter(
        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=digital_options)}
    )
```

#### Level 3 (ê³ ê¸‰ì): ì„±ëŠ¥ ìµœì í™” ì„¤ì •

**ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •**:
```python
from docling.document_converter import DocumentConverter
from concurrent.futures import ThreadPoolExecutor
import os

# CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ë³‘ë ¬ ì²˜ë¦¬
num_workers = os.cpu_count()

converter = DocumentConverter()

# ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
files = ["doc1.pdf", "doc2.pdf", "doc3.pdf", "doc4.pdf"]

with ThreadPoolExecutor(max_workers=num_workers) as executor:
    results = list(executor.map(converter.convert, files))

print(f"{len(results)}ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬ ì™„ë£Œ!")
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬**:
```python
from docling.document_converter import DocumentConverter

def process_large_pdf_batch(pdf_files):
    """
    ëŒ€ëŸ‰ì˜ PDFë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
    """
    converter = DocumentConverter()

    for pdf_file in pdf_files:
        # íŒŒì¼ í•˜ë‚˜ì”© ì²˜ë¦¬
        result = converter.convert(pdf_file)

        # ì¦‰ì‹œ ì €ì¥í•˜ê³  ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ
        output_file = pdf_file.replace('.pdf', '.md')
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result.document.export_to_markdown())

        # ëª…ì‹œì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
        del result

        print(f"âœ… {pdf_file} ì²˜ë¦¬ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ
large_batch = [f"document_{i}.pdf" for i in range(1000)]
process_large_pdf_batch(large_batch)
```

### ğŸ¨ ì¶œë ¥ í˜•ì‹ ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### Markdown ì¶œë ¥ ì˜µì…˜

```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("document.pdf")

# ê¸°ë³¸ Markdown
basic_md = result.document.export_to_markdown()

# ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€ ì„ íƒ
md_with_images = result.document.export_to_markdown(
    include_images=True  # ì´ë¯¸ì§€ í¬í•¨
)

md_without_images = result.document.export_to_markdown(
    include_images=False  # í…ìŠ¤íŠ¸ë§Œ
)
```

#### HTML ì¶œë ¥ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
# HTMLë¡œ ë³€í™˜
html = result.document.export_to_html()

# ìŠ¤íƒ€ì¼ ì¶”ê°€
styled_html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }}
        table {{
            border-collapse: collapse;
            width: 100%;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }}
    </style>
</head>
<body>
    {html}
</body>
</html>
"""

with open("styled_output.html", "w", encoding="utf-8") as f:
    f.write(styled_html)
```

#### JSON êµ¬ì¡° í™œìš©

```python
import json

# JSONìœ¼ë¡œ ë³€í™˜
json_output = result.document.export_to_json()
doc_data = json.loads(json_output)

# êµ¬ì¡° íƒìƒ‰
print("=== ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ===")

# ì œëª© ì¶”ì¶œ
if 'title' in doc_data:
    print(f"ì œëª©: {doc_data['title']}")

# ëª¨ë“  í‘œ ì¶”ì¶œ
tables = [elem for elem in doc_data['elements']
          if elem['type'] == 'table']
print(f"í‘œ ê°œìˆ˜: {len(tables)}")

# ì²« ë²ˆì§¸ í‘œ ë‚´ìš© ì¶œë ¥
if tables:
    first_table = tables[0]
    print("\nì²« ë²ˆì§¸ í‘œ:")
    print(json.dumps(first_table, indent=2, ensure_ascii=False))

# ì´ë¯¸ì§€ ëª©ë¡
images = [elem for elem in doc_data['elements']
          if elem['type'] == 'image']
print(f"\nì´ë¯¸ì§€ ê°œìˆ˜: {len(images)}")

# ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
code_blocks = [elem for elem in doc_data['elements']
               if elem['type'] == 'code']
print(f"ì½”ë“œ ë¸”ë¡ ê°œìˆ˜: {len(code_blocks)}")
```

---

## ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬

### ğŸ“„ Word ë¬¸ì„œ (DOCX) ì²˜ë¦¬

#### ê¸°ë³¸ ì²˜ë¦¬
```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()

# Word ë¬¸ì„œ ë³€í™˜
result = converter.convert("report.docx")

# Markdownìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
markdown = result.document.export_to_markdown()
print(markdown)
```

#### ì‹¤ì „ ì˜ˆì‹œ: íšŒì‚¬ ë³´ê³ ì„œ ì¼ê´„ ì²˜ë¦¬
```python
import os
from docling.document_converter import DocumentConverter

def convert_all_docx_in_folder(folder_path):
    """
    í´ë” ë‚´ ëª¨ë“  Word ë¬¸ì„œë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
    """
    converter = DocumentConverter()

    # .docx íŒŒì¼ ì°¾ê¸°
    docx_files = [f for f in os.listdir(folder_path)
                  if f.endswith('.docx') and not f.startswith('~$')]

    print(f"ë°œê²¬í•œ íŒŒì¼: {len(docx_files)}ê°œ")

    for docx_file in docx_files:
        file_path = os.path.join(folder_path, docx_file)

        print(f"\nì²˜ë¦¬ ì¤‘: {docx_file}")

        try:
            result = converter.convert(file_path)

            # ê°™ì€ ì´ë¦„ìœ¼ë¡œ .md íŒŒì¼ ìƒì„±
            output_name = docx_file.replace('.docx', '.md')
            output_path = os.path.join(folder_path, output_name)

            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(result.document.export_to_markdown())

            print(f"âœ… ì™„ë£Œ: {output_name}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {docx_file} - {str(e)}")

# ì‚¬ìš© ì˜ˆì‹œ
convert_all_docx_in_folder("./íšŒì‚¬ë¬¸ì„œ")
```

### ğŸ“Š PowerPoint (PPTX) ì²˜ë¦¬

#### ìŠ¬ë¼ì´ë“œë¥¼ Markdownìœ¼ë¡œ
```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("presentation.pptx")

# ìŠ¬ë¼ì´ë“œë³„ë¡œ êµ¬ë¶„ëœ Markdown
markdown = result.document.export_to_markdown()

print(markdown)
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```markdown
# í”„ë ˆì  í…Œì´ì…˜ ì œëª©

---

## ìŠ¬ë¼ì´ë“œ 1: ì†Œê°œ
ë³¸ë¬¸ ë‚´ìš©...

---

## ìŠ¬ë¼ì´ë“œ 2: ì£¼ìš” í¬ì¸íŠ¸
- í¬ì¸íŠ¸ 1
- í¬ì¸íŠ¸ 2

![ì°¨íŠ¸](chart_1.png)

---

## ìŠ¬ë¼ì´ë“œ 3: ê²°ë¡ 
ê²°ë¡  ë‚´ìš©...
```

#### ì‹¤ì „ ì˜ˆì‹œ: ê°•ì˜ ìë£Œ ë³€í™˜
```python
from docling.document_converter import DocumentConverter
import re

def pptx_to_study_notes(pptx_file):
    """
    PowerPointë¥¼ í•™ìŠµ ë…¸íŠ¸ë¡œ ë³€í™˜
    """
    converter = DocumentConverter()
    result = converter.convert(pptx_file)

    markdown = result.document.export_to_markdown()

    # ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸ ì¶”ê°€
    slides = markdown.split('\n---\n')

    study_notes = "# í•™ìŠµ ë…¸íŠ¸\n\n"
    study_notes += f"ì›ë³¸ íŒŒì¼: {pptx_file}\n\n"
    study_notes += "---\n\n"

    for i, slide in enumerate(slides, 1):
        study_notes += f"## ğŸ“ ìŠ¬ë¼ì´ë“œ {i}\n\n"
        study_notes += slide + "\n\n"
        study_notes += "### ë…¸íŠ¸:\n\n\n\n"
        study_notes += "---\n\n"

    return study_notes

# ì‚¬ìš©
notes = pptx_to_study_notes("lecture.pptx")
with open("lecture_notes.md", "w", encoding="utf-8") as f:
    f.write(notes)

print("âœ… í•™ìŠµ ë…¸íŠ¸ ìƒì„± ì™„ë£Œ!")
```

### ğŸ“ˆ Excel (XLSX) ì²˜ë¦¬

```python
from docling.document_converter import DocumentConverter
import json

converter = DocumentConverter()
result = converter.convert("data.xlsx")

# JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
json_output = json.loads(result.document.export_to_json())

# í‘œ ë°ì´í„° ì¶”ì¶œ
tables = [elem for elem in json_output['elements']
          if elem['type'] == 'table']

print(f"ë°œê²¬í•œ ì‹œíŠ¸/í‘œ: {len(tables)}ê°œ")

# ì²« ë²ˆì§¸ ì‹œíŠ¸ ë°ì´í„°
if tables:
    first_sheet = tables[0]
    print("\nì²« ë²ˆì§¸ ì‹œíŠ¸ ë‚´ìš©:")
    print(json.dumps(first_sheet, indent=2, ensure_ascii=False))
```

### ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¬¸ì„œ ì²˜ë¦¬

#### ë‹¨ì¼ ì´ë¯¸ì§€
```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()

# PNG, JPEG, TIFF ë“± ì§€ì›
result = converter.convert("document_scan.png")

# OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = result.document.export_to_markdown()
print(text)
```

#### ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ
```python
from docling.document_converter import DocumentConverter

def images_to_document(image_files):
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ í†µí•©
    """
    converter = DocumentConverter()

    combined_text = "# ìŠ¤ìº” ë¬¸ì„œ\n\n"

    for i, img_file in enumerate(image_files, 1):
        print(f"ì²˜ë¦¬ ì¤‘: {img_file}")

        result = converter.convert(img_file)
        text = result.document.export_to_markdown()

        combined_text += f"## í˜ì´ì§€ {i}\n\n"
        combined_text += text + "\n\n"
        combined_text += "---\n\n"

    return combined_text

# ì‚¬ìš© ì˜ˆì‹œ
image_pages = [
    "scan_page1.jpg",
    "scan_page2.jpg",
    "scan_page3.jpg"
]

document = images_to_document(image_pages)

with open("scanned_document.md", "w", encoding="utf-8") as f:
    f.write(document)
```

### ğŸ™ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ìŒì„± ì¸ì‹)

```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()

# ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
result = converter.convert("interview.mp3")

# ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ
transcript = result.document.export_to_markdown()

print("=== ìŒì„± ì¸ì‹ ê²°ê³¼ ===")
print(transcript)
```

**ì‹¤ì „ ì˜ˆì‹œ: íšŒì˜ë¡ ìë™ ìƒì„±**
```python
from docling.document_converter import DocumentConverter
from datetime import datetime

def audio_to_meeting_minutes(audio_file):
    """
    íšŒì˜ ë…¹ìŒ íŒŒì¼ì„ íšŒì˜ë¡ìœ¼ë¡œ ë³€í™˜
    """
    converter = DocumentConverter()
    result = converter.convert(audio_file)

    transcript = result.document.export_to_markdown()

    # íšŒì˜ë¡ í…œí”Œë¦¿
    minutes = f"""# íšŒì˜ë¡

## íšŒì˜ ì •ë³´
- ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d')}
- ë…¹ìŒ íŒŒì¼: {audio_file}

## íšŒì˜ ë‚´ìš©

{transcript}

## ì•¡ì…˜ ì•„ì´í…œ
- [ ] TODO 1
- [ ] TODO 2

## ë‹¤ìŒ íšŒì˜
- ë‚ ì§œ:
- ì•ˆê±´:
"""

    return minutes

# ì‚¬ìš©
minutes = audio_to_meeting_minutes("team_meeting.mp3")
with open("meeting_minutes.md", "w", encoding="utf-8") as f:
    f.write(minutes)

print("âœ… íšŒì˜ë¡ ìƒì„± ì™„ë£Œ!")
```

---

## OCRê³¼ ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬

### ğŸ” OCRì´ë€?

**ì‰¬ìš´ ì„¤ëª…**:
```
ì¼ë°˜ í…ìŠ¤íŠ¸: ë³µì‚¬ ê°€ëŠ¥ âœ…
"ì•ˆë…•í•˜ì„¸ìš”" â†’ Ctrl+C ê°€ëŠ¥

ìŠ¤ìº”/ì´ë¯¸ì§€: ë³µì‚¬ ë¶ˆê°€ âŒ
[ì´ë¯¸ì§€: ì•ˆë…•í•˜ì„¸ìš”] â†’ Ctrl+C ë¶ˆê°€

OCRì˜ ì—­í• :
[ì´ë¯¸ì§€: ì•ˆë…•í•˜ì„¸ìš”] â†’ OCR â†’ "ì•ˆë…•í•˜ì„¸ìš”" âœ…
```

### ğŸ“¸ ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°

#### Level 1 (ì´ˆë³´ì): ê¸°ë³¸ OCR ì‚¬ìš©
```python
from docling.document_converter import DocumentConverter
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import PdfFormatOption
from docling.datamodel.base_models import InputFormat

# OCR í™œì„±í™”
pdf_options = PdfPipelineOptions()
pdf_options.do_ocr = True

converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(
            pipeline_options=pdf_options
        )
    }
)

# ìŠ¤ìº”ëœ PDF ì²˜ë¦¬
result = converter.convert("scanned_document.pdf")
text = result.document.export_to_markdown()

print(text)
```

#### Level 2 (ì¤‘ê¸‰ì): ì´ë¯¸ì§€ í’ˆì§ˆì— ë”°ë¥¸ ì²˜ë¦¬
```python
from docling.document_converter import DocumentConverter
from docling.datamodel.pipeline_options import PdfPipelineOptions

def process_scanned_pdf(pdf_file, quality='high'):
    """
    ìŠ¤ìº” í’ˆì§ˆì— ë”°ë¼ ë‹¤ë¥¸ ì„¤ì • ì ìš©
    """
    pdf_options = PdfPipelineOptions()
    pdf_options.do_ocr = True

    if quality == 'high':
        # ê³ í’ˆì§ˆ ìŠ¤ìº”: ë¹ ë¥¸ ì²˜ë¦¬
        pdf_options.ocr_options = {
            'lang': 'kor+eng',  # í•œêµ­ì–´ + ì˜ì–´
            'psm': 3  # Fully automatic page segmentation
        }

    elif quality == 'low':
        # ì €í’ˆì§ˆ ìŠ¤ìº”: ì •í™•ë„ ìš°ì„ 
        pdf_options.ocr_options = {
            'lang': 'kor+eng',
            'psm': 6,  # Uniform block of text
            'oem': 1   # Neural nets LSTM OCR Engine
        }

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pdf_options
            )
        }
    )

    return converter.convert(pdf_file)

# ì‚¬ìš©
result = process_scanned_pdf("old_document.pdf", quality='low')
```

#### Level 3 (ê³ ê¸‰ì): ì „ì²˜ë¦¬ + OCR íŒŒì´í”„ë¼ì¸
```python
from PIL import Image, ImageEnhance
import pdf2image
from docling.document_converter import DocumentConverter

def preprocess_and_ocr(pdf_file):
    """
    ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í›„ OCR ì‹¤í–‰
    """
    # 1. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    pages = pdf2image.convert_from_path(pdf_file, dpi=300)

    processed_pages = []

    for i, page in enumerate(pages):
        # 2. ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 
        # ëŒ€ë¹„ ì¦ê°€
        enhancer = ImageEnhance.Contrast(page)
        page = enhancer.enhance(1.5)

        # ì„ ëª…ë„ ì¦ê°€
        enhancer = ImageEnhance.Sharpness(page)
        page = enhancer.enhance(1.2)

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = f"temp_page_{i}.png"
        page.save(temp_file, 'PNG')
        processed_pages.append(temp_file)

    # 3. Doclingìœ¼ë¡œ OCR ì‹¤í–‰
    converter = DocumentConverter()

    full_text = ""
    for temp_file in processed_pages:
        result = converter.convert(temp_file)
        full_text += result.document.export_to_markdown() + "\n\n"

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        import os
        os.remove(temp_file)

    return full_text

# ì‚¬ìš©
text = preprocess_and_ocr("poor_quality_scan.pdf")
```

### ğŸŒ ë‹¤êµ­ì–´ OCR

```python
from docling.document_converter import DocumentConverter

# í•œêµ­ì–´ ë¬¸ì„œ
korean_result = converter.convert("í•œêµ­ì–´ë¬¸ì„œ.pdf")

# ì¼ë³¸ì–´ ë¬¸ì„œ
japanese_result = converter.convert("æ—¥æœ¬èªæ–‡æ›¸.pdf")

# ì¤‘êµ­ì–´ ë¬¸ì„œ
chinese_result = converter.convert("ä¸­æ–‡æ–‡æ¡£.pdf")

# ì—¬ëŸ¬ ì–¸ì–´ í˜¼í•© ë¬¸ì„œ
mixed_result = converter.convert("multilingual.pdf")
```

---

## AI í”„ë ˆì„ì›Œí¬ ì—°ë™

### ğŸ¦œ LangChain í†µí•©

#### Level 1 (ì´ˆë³´ì): ê¸°ë³¸ í†µí•©
```python
from docling.document_converter import DocumentConverter
from langchain.text_splitter import MarkdownTextSplitter
from langchain.schema import Document

# 1. Doclingìœ¼ë¡œ PDF ë³€í™˜
converter = DocumentConverter()
result = converter.convert("textbook.pdf")
markdown_text = result.document.export_to_markdown()

# 2. LangChain Document ê°ì²´ ìƒì„±
doc = Document(
    page_content=markdown_text,
    metadata={"source": "textbook.pdf"}
)

# 3. í…ìŠ¤íŠ¸ ë¶„í•  (AIê°€ ì²˜ë¦¬í•˜ê¸° ì¢‹ì€ í¬ê¸°ë¡œ)
splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = splitter.split_documents([doc])

print(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")
print(f"\nì²« ë²ˆì§¸ ì²­í¬:\n{chunks[0].page_content[:200]}...")
```

#### Level 2 (ì¤‘ê¸‰ì): RAG ì‹œìŠ¤í…œ êµ¬ì¶•
```python
from docling.document_converter import DocumentConverter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
import os

def build_rag_system(pdf_files):
    """
    PDF íŒŒì¼ë“¤ë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    """
    # 1. ëª¨ë“  PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
    converter = DocumentConverter()
    documents = []

    for pdf_file in pdf_files:
        print(f"ë³€í™˜ ì¤‘: {pdf_file}")
        result = converter.convert(pdf_file)
        markdown = result.document.export_to_markdown()

        doc = Document(
            page_content=markdown,
            metadata={"source": pdf_file}
        )
        documents.append(doc)

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(documents)
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

    # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(chunks, embeddings)
    print("ë²¡í„° DB ìƒì„± ì™„ë£Œ")

    # 4. RAG ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=OpenAI(),
        chain_type="stuff",
        retriever=vectorstore.as_retriever()
    )

    return qa_chain

# ì‚¬ìš© ì˜ˆì‹œ
pdf_files = [
    "manual1.pdf",
    "manual2.pdf",
    "manual3.pdf"
]

qa_system = build_rag_system(pdf_files)

# ì§ˆë¬¸í•˜ê¸°
question = "ì œí’ˆ ë³´ì¦ ê¸°ê°„ì€ ì–¼ë§ˆì¸ê°€ìš”?"
answer = qa_system.run(question)
print(f"\nì§ˆë¬¸: {question}")
print(f"ë‹µë³€: {answer}")
```

#### Level 3 (ê³ ê¸‰ì): ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ RAG ì‹œìŠ¤í…œ
```python
from docling.document_converter import DocumentConverter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time

class PDFWatcher(FileSystemEventHandler):
    """
    PDF íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ìë™ ì—…ë°ì´íŠ¸
    """
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.converter = DocumentConverter()

    def on_created(self, event):
        if event.src_path.endswith('.pdf'):
            print(f"ìƒˆ PDF ë°œê²¬: {event.src_path}")
            self.add_to_vectorstore(event.src_path)

    def add_to_vectorstore(self, pdf_path):
        """PDFë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
        result = self.converter.convert(pdf_path)
        markdown = result.document.export_to_markdown()

        doc = Document(
            page_content=markdown,
            metadata={"source": pdf_path}
        )

        # ë²¡í„° DBì— ì¶”ê°€
        self.vectorstore.add_documents([doc])
        print(f"âœ… {pdf_path} ì¶”ê°€ ì™„ë£Œ")

def create_live_rag_system(watch_folder):
    """
    ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë˜ëŠ” RAG ì‹œìŠ¤í…œ
    """
    # ë²¡í„° DB ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory="./chroma_db"
    )

    # íŒŒì¼ ê°ì‹œì ì„¤ì •
    event_handler = PDFWatcher(vectorstore)
    observer = Observer()
    observer.schedule(event_handler, watch_folder, recursive=False)
    observer.start()

    print(f"ğŸ“ {watch_folder} í´ë” ê°ì‹œ ì‹œì‘...")
    print("ìƒˆ PDF íŒŒì¼ì´ ì¶”ê°€ë˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")

    return vectorstore, observer

# ì‚¬ìš©
vectorstore, observer = create_live_rag_system("./documents")

# ì‹œìŠ¤í…œ ì‹¤í–‰ (Ctrl+Cë¡œ ì¢…ë£Œí•  ë•Œê¹Œì§€)
try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    observer.stop()
    observer.join()
```

### ğŸ¦™ LlamaIndex í†µí•©

```python
from docling.document_converter import DocumentConverter
from llama_index import VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.node_parser import SimpleNodeParser

def create_llamaindex_system(pdf_files):
    """
    LlamaIndex ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•
    """
    converter = DocumentConverter()
    documents = []

    # PDF ë³€í™˜
    for pdf_file in pdf_files:
        result = converter.convert(pdf_file)
        markdown = result.document.export_to_markdown()

        doc = Document(
            text=markdown,
            metadata={"file_name": pdf_file}
        )
        documents.append(doc)

    # ì¸ë±ìŠ¤ ìƒì„±
    index = VectorStoreIndex.from_documents(documents)

    # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
    query_engine = index.as_query_engine()

    return query_engine

# ì‚¬ìš©
pdf_files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
engine = create_llamaindex_system(pdf_files)

# ê²€ìƒ‰
response = engine.query("ì£¼ìš” ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€?")
print(response)
```

---

## CLI ëª…ë ¹ì¤„ ë„êµ¬ í™œìš©

### ğŸ’» ê¸°ë³¸ CLI ì‚¬ìš©ë²•

#### ë‹¨ì¼ íŒŒì¼ ë³€í™˜
```bash
# ê¸°ë³¸ ì‚¬ìš©
docling document.pdf

# ê²°ê³¼ëŠ” ê°™ì€ í´ë”ì— document.mdë¡œ ì €ì¥ë¨
```

#### ì¶œë ¥ ê²½ë¡œ ì§€ì •
```bash
# íŠ¹ì • í´ë”ì— ì €ì¥
docling document.pdf --output ./converted/

# íŒŒì¼ëª… ì§€ì •
docling document.pdf --output result.md
```

#### ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬
```bash
# ì—¬ëŸ¬ íŒŒì¼ í•œ ë²ˆì—
docling file1.pdf file2.pdf file3.pdf

# ì™€ì¼ë“œì¹´ë“œ ì‚¬ìš©
docling *.pdf

# íŠ¹ì • íŒ¨í„´
docling report_*.pdf
```

### ğŸ¨ ê³ ê¸‰ CLI ì˜µì…˜

#### Visual Language Model ì‚¬ìš©
```bash
# GraniteDocling ëª¨ë¸ë¡œ ì²˜ë¦¬ (ë” ì •í™•í•œ ì´í•´)
docling --pipeline vlm --vlm-model granite_docling document.pdf

# Apple Siliconì—ì„œ MLX ê°€ì† ìë™ ì‚¬ìš©
```

#### OCR ì˜µì…˜
```bash
# OCR í™œì„±í™”
docling --ocr document.pdf

# íŠ¹ì • ì–¸ì–´ ì§€ì •
docling --ocr --ocr-lang kor+eng document.pdf
```

#### ì¶œë ¥ í˜•ì‹ ì„ íƒ
```bash
# Markdown (ê¸°ë³¸)
docling document.pdf

# HTMLë¡œ ì¶œë ¥
docling --format html document.pdf

# JSONìœ¼ë¡œ ì¶œë ¥
docling --format json document.pdf
```

### ğŸ”§ ì‹¤ì „ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ

#### Bash ìŠ¤í¬ë¦½íŠ¸: í´ë” ì¼ê´„ ì²˜ë¦¬
```bash
#!/bin/bash
# convert_all.sh

INPUT_DIR="./input"
OUTPUT_DIR="./output"

# ì¶œë ¥ í´ë” ìƒì„±
mkdir -p "$OUTPUT_DIR"

# ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬
for pdf in "$INPUT_DIR"/*.pdf; do
    echo "ì²˜ë¦¬ ì¤‘: $(basename "$pdf")"

    # Doclingìœ¼ë¡œ ë³€í™˜
    docling "$pdf" --output "$OUTPUT_DIR/"

    echo "ì™„ë£Œ: $(basename "$pdf")"
done

echo "ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!"
```

**ì‚¬ìš©ë²•**:
```bash
chmod +x convert_all.sh
./convert_all.sh
```

#### Python ìŠ¤í¬ë¦½íŠ¸: ì§„í–‰ ìƒí™© í‘œì‹œ
```python
#!/usr/bin/env python3
# batch_convert.py

import subprocess
import os
from tqdm import tqdm

def batch_convert(input_folder, output_folder):
    """
    ì§„í–‰ í‘œì‹œì¤„ê³¼ í•¨ê»˜ ë°°ì¹˜ ë³€í™˜
    """
    # PDF íŒŒì¼ ëª©ë¡
    pdf_files = [f for f in os.listdir(input_folder)
                 if f.endswith('.pdf')]

    print(f"ë°œê²¬í•œ PDF: {len(pdf_files)}ê°œ")

    # ì§„í–‰ í‘œì‹œì¤„
    for pdf_file in tqdm(pdf_files, desc="ë³€í™˜ ì¤‘"):
        input_path = os.path.join(input_folder, pdf_file)
        output_path = os.path.join(output_folder, pdf_file.replace('.pdf', '.md'))

        # Docling CLI ì‹¤í–‰
        subprocess.run([
            'docling',
            input_path,
            '--output', output_path
        ], check=True, capture_output=True)

    print("\nâœ… ëª¨ë“  íŒŒì¼ ë³€í™˜ ì™„ë£Œ!")

if __name__ == "__main__":
    batch_convert("./pdfs", "./markdown")
```

**ì‚¬ìš©ë²•**:
```bash
python batch_convert.py
```

---

## ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ì™€ ìµœì í™”

### âš¡ ì„±ëŠ¥ ìµœì í™” ì „ëµ

#### ì „ëµ 1: ë³‘ë ¬ ì²˜ë¦¬
```python
from docling.document_converter import DocumentConverter
from concurrent.futures import ProcessPoolExecutor
import os

def convert_single_file(pdf_file):
    """ë‹¨ì¼ íŒŒì¼ ë³€í™˜ (ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
    converter = DocumentConverter()
    result = converter.convert(pdf_file)

    output_file = pdf_file.replace('.pdf', '.md')
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(result.document.export_to_markdown())

    return output_file

def parallel_convert(pdf_files, max_workers=4):
    """
    ì—¬ëŸ¬ CPU ì½”ì–´ë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    """
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(convert_single_file, pdf_files))

    return results

# ì‚¬ìš©
pdf_files = [f"document_{i}.pdf" for i in range(100)]
results = parallel_convert(pdf_files, max_workers=8)

print(f"âœ… {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
```

#### ì „ëµ 2: ìºì‹± ì‹œìŠ¤í…œ
```python
import hashlib
import json
import os
from docling.document_converter import DocumentConverter

class CachedConverter:
    """
    ë³€í™˜ ê²°ê³¼ë¥¼ ìºì‹±í•˜ëŠ” ë³€í™˜ê¸°
    """
    def __init__(self, cache_dir=".cache"):
        self.converter = DocumentConverter()
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def _get_file_hash(self, file_path):
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def convert(self, file_path):
        """ìºì‹œë¥¼ í™•ì¸í•˜ê³  ë³€í™˜"""
        file_hash = self._get_file_hash(file_path)
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.json")

        # ìºì‹œ í™•ì¸
        if os.path.exists(cache_file):
            print(f"ğŸ’¾ ìºì‹œ ì‚¬ìš©: {file_path}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        # ìƒˆë¡œ ë³€í™˜
        print(f"ğŸ”„ ë³€í™˜ ì¤‘: {file_path}")
        result = self.converter.convert(file_path)
        markdown = result.document.export_to_markdown()

        # ìºì‹œ ì €ì¥
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({'markdown': markdown}, f)

        return {'markdown': markdown}

# ì‚¬ìš©
cached_converter = CachedConverter()

# ì²« ì‹¤í–‰: ë³€í™˜ ìˆ˜í–‰
result1 = cached_converter.convert("large_doc.pdf")  # ëŠë¦¼

# ë‘ ë²ˆì§¸ ì‹¤í–‰: ìºì‹œ ì‚¬ìš©
result2 = cached_converter.convert("large_doc.pdf")  # ë§¤ìš° ë¹ ë¦„!
```

#### ì „ëµ 3: ë°°ì¹˜ ì²˜ë¦¬ with ì§„í–‰ë¥ 
```python
from docling.document_converter import DocumentConverter
from tqdm import tqdm
import time

def batch_convert_with_progress(pdf_files):
    """
    ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬
    """
    converter = DocumentConverter()

    results = []
    failed = []

    # ì§„í–‰ í‘œì‹œì¤„
    pbar = tqdm(total=len(pdf_files), desc="ì „ì²´ ì§„í–‰ë¥ ")

    for pdf_file in pdf_files:
        try:
            # ë³€í™˜ ì‹œì‘ ì‹œê°„
            start_time = time.time()

            result = converter.convert(pdf_file)

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            elapsed = time.time() - start_time

            results.append({
                'file': pdf_file,
                'status': 'success',
                'time': elapsed
            })

            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'current': pdf_file,
                'time': f'{elapsed:.2f}s'
            })

        except Exception as e:
            failed.append({
                'file': pdf_file,
                'error': str(e)
            })

        pbar.update(1)

    pbar.close()

    # ê²°ê³¼ ìš”ì•½
    print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
    print(f"âœ… ì„±ê³µ: {len(results)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len(failed)}ê°œ")

    if failed:
        print(f"\nì‹¤íŒ¨í•œ íŒŒì¼:")
        for item in failed:
            print(f"  - {item['file']}: {item['error']}")

    return results, failed

# ì‚¬ìš©
pdf_files = [f"doc_{i}.pdf" for i in range(50)]
results, failed = batch_convert_with_progress(pdf_files)
```

### ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬

#### ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
```python
from docling.document_converter import DocumentConverter
import gc

def process_large_files(pdf_files):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
    """
    converter = DocumentConverter()

    for pdf_file in pdf_files:
        print(f"ì²˜ë¦¬ ì¤‘: {pdf_file}")

        # ë³€í™˜
        result = converter.convert(pdf_file)

        # ì¦‰ì‹œ ì €ì¥
        output_file = pdf_file.replace('.pdf', '.md')
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result.document.export_to_markdown())

        # ë©”ëª¨ë¦¬ í•´ì œ
        del result
        gc.collect()

        print(f"âœ… ì™„ë£Œ: {output_file}")

# ì‚¬ìš©
large_files = ["huge1.pdf", "huge2.pdf", "huge3.pdf"]
process_large_files(large_files)
```

---

## ì‹¤ë¬´ í™œìš© ì‚¬ë¡€

### ğŸ“š ì‚¬ë¡€ 1: ì—°êµ¬ ë…¼ë¬¸ ê´€ë¦¬ ì‹œìŠ¤í…œ

```python
from docling.document_converter import DocumentConverter
import os
import json
from datetime import datetime

class ResearchPaperManager:
    """
    ì—°êµ¬ ë…¼ë¬¸ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ
    """
    def __init__(self, library_path="./papers"):
        self.library_path = library_path
        self.converter = DocumentConverter()
        self.index_file = os.path.join(library_path, "index.json")

        os.makedirs(library_path, exist_ok=True)

        # ì¸ë±ìŠ¤ ë¡œë“œ
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r') as f:
                self.index = json.load(f)
        else:
            self.index = {}

    def add_paper(self, pdf_file, metadata=None):
        """
        ë…¼ë¬¸ ì¶”ê°€ ë° ë³€í™˜
        """
        print(f"ë…¼ë¬¸ ì¶”ê°€ ì¤‘: {pdf_file}")

        # ë³€í™˜
        result = self.converter.convert(pdf_file)
        markdown = result.document.export_to_markdown()

        # íŒŒì¼ëª… ìƒì„±
        paper_id = os.path.basename(pdf_file).replace('.pdf', '')
        md_file = os.path.join(self.library_path, f"{paper_id}.md")

        # Markdown ì €ì¥
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown)

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        paper_info = {
            'id': paper_id,
            'original_file': pdf_file,
            'markdown_file': md_file,
            'added_date': datetime.now().isoformat(),
            'metadata': metadata or {}
        }

        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self.index[paper_id] = paper_info
        self._save_index()

        print(f"âœ… ì¶”ê°€ ì™„ë£Œ: {paper_id}")

        return paper_id

    def search(self, keyword):
        """
        í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰
        """
        results = []

        for paper_id, info in self.index.items():
            md_file = info['markdown_file']

            # Markdown íŒŒì¼ ì½ê¸°
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            if keyword.lower() in content.lower():
                results.append({
                    'paper_id': paper_id,
                    'file': md_file,
                    'metadata': info['metadata']
                })

        return results

    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f, indent=2)

# ì‚¬ìš© ì˜ˆì‹œ
manager = ResearchPaperManager("./my_papers")

# ë…¼ë¬¸ ì¶”ê°€
manager.add_paper(
    "transformer_paper.pdf",
    metadata={
        'title': 'Attention Is All You Need',
        'authors': ['Vaswani et al.'],
        'year': 2017,
        'tags': ['NLP', 'Transformer', 'Attention']
    }
)

# ê²€ìƒ‰
results = manager.search("attention mechanism")
print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
```

### ğŸ“‹ ì‚¬ë¡€ 2: ê³„ì•½ì„œ ë¶„ì„ ì‹œìŠ¤í…œ

```python
from docling.document_converter import DocumentConverter
import re
from dataclasses import dataclass
from typing import List

@dataclass
class ContractClause:
    """ê³„ì•½ ì¡°í•­"""
    type: str
    content: str
    page: int

class ContractAnalyzer:
    """
    ê³„ì•½ì„œ ìë™ ë¶„ì„ ì‹œìŠ¤í…œ
    """
    def __init__(self):
        self.converter = DocumentConverter()

    def analyze(self, contract_pdf):
        """
        ê³„ì•½ì„œ ë¶„ì„
        """
        print(f"ê³„ì•½ì„œ ë¶„ì„ ì¤‘: {contract_pdf}")

        # PDF ë³€í™˜
        result = self.converter.convert(contract_pdf)
        markdown = result.document.export_to_markdown()

        # ì£¼ìš” ì¡°í•­ ì¶”ì¶œ
        clauses = self._extract_clauses(markdown)

        # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        report = self._generate_report(contract_pdf, clauses)

        return report

    def _extract_clauses(self, markdown_text):
        """ì£¼ìš” ì¡°í•­ ì¶”ì¶œ"""
        clauses = []

        # ê¸°ê°„ ê´€ë ¨
        period_pattern = r'(\d+ë…„|\d+ê°œì›”|\d+ì¼)'
        if re.search(period_pattern, markdown_text):
            clauses.append(ContractClause(
                type='ê³„ì•½ ê¸°ê°„',
                content=self._extract_context(markdown_text, period_pattern),
                page=1
            ))

        # ê¸ˆì•¡ ê´€ë ¨
        money_pattern = r'(\d{1,3}(,\d{3})*ì›|\$\d{1,3}(,\d{3})*)'
        if re.search(money_pattern, markdown_text):
            clauses.append(ContractClause(
                type='ê¸ˆì•¡',
                content=self._extract_context(markdown_text, money_pattern),
                page=1
            ))

        # í•´ì§€ ì¡°ê±´
        if 'í•´ì§€' in markdown_text or 'ì¢…ë£Œ' in markdown_text:
            clauses.append(ContractClause(
                type='í•´ì§€ ì¡°ê±´',
                content=self._extract_context(markdown_text, 'í•´ì§€|ì¢…ë£Œ'),
                page=1
            ))

        return clauses

    def _extract_context(self, text, pattern, context_size=100):
        """íŒ¨í„´ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ"""
        match = re.search(pattern, text)
        if match:
            start = max(0, match.start() - context_size)
            end = min(len(text), match.end() + context_size)
            return text[start:end]
        return ""

    def _generate_report(self, contract_file, clauses):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""# ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸

## ê³„ì•½ì„œ ì •ë³´
- íŒŒì¼: {contract_file}
- ë¶„ì„ ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d')}

## ì£¼ìš” ì¡°í•­

"""
        for clause in clauses:
            report += f"""### {clause.type}
```
{clause.content}
```

"""

        report += """## ì£¼ì˜ì‚¬í•­
- [ ] ê³„ì•½ ê¸°ê°„ í™•ì¸
- [ ] ê¸ˆì•¡ ë° ì§€ë¶ˆ ì¡°ê±´ í™•ì¸
- [ ] í•´ì§€ ì¡°ê±´ ê²€í† 
- [ ] ë²•ë¥  ìë¬¸ í•„ìš” ì‹œ ì „ë¬¸ê°€ ìƒë‹´

"""

        return report

# ì‚¬ìš© ì˜ˆì‹œ
analyzer = ContractAnalyzer()
report = analyzer.analyze("contract.pdf")

# ë¦¬í¬íŠ¸ ì €ì¥
with open("contract_analysis.md", "w", encoding="utf-8") as f:
    f.write(report)

print("âœ… ê³„ì•½ì„œ ë¶„ì„ ì™„ë£Œ!")
```

### ğŸ“– ì‚¬ë¡€ 3: êµìœ¡ ì½˜í…ì¸  ë³€í™˜ ì‹œìŠ¤í…œ

```python
from docling.document_converter import DocumentConverter
import os

class EducationContentConverter:
    """
    êµìœ¡ ìë£Œë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    def __init__(self):
        self.converter = DocumentConverter()

    def convert_lesson(self, source_file, output_format='markdown'):
        """
        ê°•ì˜ ìë£Œ ë³€í™˜
        """
        print(f"ë³€í™˜ ì¤‘: {source_file}")

        result = self.converter.convert(source_file)

        if output_format == 'markdown':
            content = self._to_student_friendly_markdown(result)
        elif output_format == 'html':
            content = self._to_interactive_html(result)
        elif output_format == 'study_guide':
            content = self._to_study_guide(result)

        return content

    def _to_student_friendly_markdown(self, result):
        """í•™ìƒ ì¹œí™”ì  Markdown"""
        markdown = result.document.export_to_markdown()

        # í•™ìŠµ í¬ì¸íŠ¸ ì¶”ê°€
        enhanced = f"""# ğŸ“š í•™ìŠµ ìë£Œ

## í•™ìŠµ ëª©í‘œ
- ì´ ë‚´ìš©ì„ í†µí•´ ë¬´ì—‡ì„ ë°°ìš¸ ìˆ˜ ìˆë‚˜ìš”?
- [ ] ëª©í‘œ 1
- [ ] ëª©í‘œ 2

---

{markdown}

---

## ğŸ“ ì •ë¦¬ ë…¸íŠ¸

**ì£¼ìš” ê°œë…:**
-

**ì–´ë ¤ìš´ ë¶€ë¶„:**
-

**ì§ˆë¬¸:**
-

## ğŸ¯ ë³µìŠµ ë¬¸ì œ
1.
2.
3.

"""
        return enhanced

    def _to_interactive_html(self, result):
        """ì¸í„°ë™í‹°ë¸Œ HTML"""
        markdown = result.document.export_to_markdown()

        html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>í•™ìŠµ ìë£Œ</title>
    <style>
        body {{
            font-family: 'Noto Sans KR', sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }}
        .highlight {{
            background-color: #fff3cd;
            padding: 2px 5px;
            cursor: pointer;
        }}
        .note-box {{
            border-left: 4px solid #007bff;
            padding-left: 15px;
            margin: 20px 0;
            background: #f8f9fa;
        }}
    </style>
    <script>
        function addNote(element) {{
            const note = prompt("ë©”ëª¨ ì…ë ¥:");
            if (note) {{
                element.title = note;
                element.style.backgroundColor = '#ffc107';
            }}
        }}
    </script>
</head>
<body>
    <h1>ğŸ“– ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ ìë£Œ</h1>
    <p>ì¤‘ìš”í•œ ë¶€ë¶„ì„ í´ë¦­í•˜ë©´ ë©”ëª¨ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
    <hr>
    <div id="content">
        {markdown}
    </div>
</body>
</html>
"""
        return html

    def _to_study_guide(self, result):
        """í•™ìŠµ ê°€ì´ë“œ ìƒì„±"""
        markdown = result.document.export_to_markdown()

        # ì„¹ì…˜ ë¶„í• 
        sections = markdown.split('\n## ')

        guide = """# ğŸ“– í•™ìŠµ ê°€ì´ë“œ

## ì‚¬ìš© ë°©ë²•
1. ê° ì„¹ì…˜ì„ ì²œì²œíˆ ì½ìŠµë‹ˆë‹¤
2. ì´í•´ê°€ ì•ˆ ë˜ëŠ” ë¶€ë¶„ì€ í‘œì‹œí•©ë‹ˆë‹¤
3. ì˜ˆì œë¥¼ ì§ì ‘ ì‹¤ìŠµí•©ë‹ˆë‹¤
4. ë³µìŠµ ë¬¸ì œë¥¼ í’‰ë‹ˆë‹¤

---

"""

        for i, section in enumerate(sections[1:], 1):
            guide += f"""## {i}. {section.split('\n')[0]}

{section}

### âœ… ì²´í¬í¬ì¸íŠ¸
- [ ] ì´ ì„¹ì…˜ì˜ í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆë‚˜ìš”?
- [ ] ì˜ˆì œë¥¼ ì§ì ‘ ì‹¤ìŠµí•´ë´¤ë‚˜ìš”?
- [ ] ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆë‚˜ìš”?

---

"""

        return guide

# ì‚¬ìš©
converter = EducationContentConverter()

# Markdownìœ¼ë¡œ
markdown_content = converter.convert_lesson(
    "lecture.pdf",
    output_format='markdown'
)

with open("student_material.md", "w", encoding="utf-8") as f:
    f.write(markdown_content)

# HTMLë¡œ
html_content = converter.convert_lesson(
    "lecture.pdf",
    output_format='html'
)

with open("interactive_lesson.html", "w", encoding="utf-8") as f:
    f.write(html_content)

print("âœ… êµìœ¡ ì½˜í…ì¸  ë³€í™˜ ì™„ë£Œ!")
```

---

## ë¬¸ì œ í•´ê²°ê³¼ íŒ

### ğŸ”§ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### ë¬¸ì œ 1: ì„¤ì¹˜ ì˜¤ë¥˜
```bash
# ì˜¤ë¥˜: "No module named 'docling'"

# í•´ê²°ì±… 1: pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip
pip install docling

# í•´ê²°ì±… 2: ê°€ìƒ í™˜ê²½ ì¬ìƒì„±
python -m venv new_env
source new_env/bin/activate  # Windows: new_env\Scripts\activate
pip install docling

# í•´ê²°ì±… 3: ì‚¬ìš©ì ë””ë ‰í† ë¦¬ì— ì„¤ì¹˜
pip install --user docling
```

#### ë¬¸ì œ 2: ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ì¦ìƒ: í° PDF ì²˜ë¦¬ ì‹œ "MemoryError"

# í•´ê²°ì±…: í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬ ë˜ëŠ” íŒŒì¼ ë¶„í• 
import PyPDF2

def split_large_pdf(input_pdf, pages_per_chunk=50):
    """
    í° PDFë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë¶„í• 
    """
    pdf_reader = PyPDF2.PdfReader(input_pdf)
    total_pages = len(pdf_reader.pages)

    for chunk_num in range(0, total_pages, pages_per_chunk):
        pdf_writer = PyPDF2.PdfWriter()

        for page_num in range(chunk_num, min(chunk_num + pages_per_chunk, total_pages)):
            pdf_writer.add_page(pdf_reader.pages[page_num])

        output_filename = f"chunk_{chunk_num//pages_per_chunk + 1}.pdf"
        with open(output_filename, 'wb') as output_pdf:
            pdf_writer.write(output_pdf)

        print(f"ìƒì„±ë¨: {output_filename}")

# ì‚¬ìš©
split_large_pdf("huge_document.pdf", pages_per_chunk=30)
```

#### ë¬¸ì œ 3: í•œê¸€ ê¹¨ì§
```python
# ì˜ëª»ëœ ì˜ˆ
with open("output.md", "w") as f:
    f.write(markdown)  # í•œê¸€ ê¹¨ì§

# ì˜¬ë°”ë¥¸ ì˜ˆ
with open("output.md", "w", encoding="utf-8") as f:
    f.write(markdown)  # ì •ìƒ ì¶œë ¥

# ë˜ëŠ” ëª…ì‹œì  ì„¤ì •
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
```

#### ë¬¸ì œ 4: OCR ì •í™•ë„ ë‚®ìŒ
```python
# í•´ê²°ì±…: ì´ë¯¸ì§€ ì „ì²˜ë¦¬

from PIL import Image, ImageEnhance, ImageFilter

def improve_ocr_accuracy(image_path):
    """
    OCR ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    """
    img = Image.open(image_path)

    # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    img = img.convert('L')

    # 2. ëŒ€ë¹„ í–¥ìƒ
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(2.0)

    # 3. ì„ ëª…ë„ í–¥ìƒ
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(1.5)

    # 4. ë…¸ì´ì¦ˆ ì œê±°
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # ì €ì¥
    processed_path = "processed_" + image_path
    img.save(processed_path)

    return processed_path

# ì‚¬ìš©
processed_image = improve_ocr_accuracy("scan.png")
result = converter.convert(processed_image)
```

### ğŸ’¡ ì„±ëŠ¥ í–¥ìƒ íŒ

#### íŒ 1: íŒŒì¼ íƒ€ì… ì‚¬ì „ í•„í„°ë§
```python
def filter_valid_files(files):
    """
    ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ë§Œ ì„ íƒ
    """
    valid_extensions = {
        '.pdf', '.docx', '.pptx', '.xlsx',
        '.png', '.jpg', '.jpeg', '.tiff'
    }

    valid_files = []

    for file in files:
        ext = os.path.splitext(file)[1].lower()
        if ext in valid_extensions:
            valid_files.append(file)
        else:
            print(f"âš ï¸ ê±´ë„ˆëœ€: {file} (ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹)")

    return valid_files

# ì‚¬ìš©
all_files = os.listdir("./documents")
valid_files = filter_valid_files(all_files)
```

#### íŒ 2: ì—ëŸ¬ ë¡œê¹…
```python
import logging
from datetime import datetime

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    filename=f'docling_{datetime.now().strftime("%Y%m%d")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def safe_convert(pdf_file):
    """
    ì—ëŸ¬ë¥¼ ë¡œê¹…í•˜ë©´ì„œ ì•ˆì „í•˜ê²Œ ë³€í™˜
    """
    try:
        logging.info(f"ë³€í™˜ ì‹œì‘: {pdf_file}")
        converter = DocumentConverter()
        result = converter.convert(pdf_file)
        logging.info(f"ë³€í™˜ ì„±ê³µ: {pdf_file}")
        return result

    except Exception as e:
        logging.error(f"ë³€í™˜ ì‹¤íŒ¨: {pdf_file} - {str(e)}")
        return None

# ì‚¬ìš©
result = safe_convert("document.pdf")
if result:
    print("ì„±ê³µ!")
else:
    print("ì‹¤íŒ¨! ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
```

#### íŒ 3: ì§„í–‰ ìƒí™© ì €ì¥
```python
import json
import os

class ProgressTracker:
    """
    ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì¶”ì 
    """
    def __init__(self, progress_file="progress.json"):
        self.progress_file = progress_file

        if os.path.exists(progress_file):
            with open(progress_file, 'r') as f:
                self.progress = json.load(f)
        else:
            self.progress = {
                'completed': [],
                'failed': [],
                'remaining': []
            }

    def mark_completed(self, file):
        """ì™„ë£Œ í‘œì‹œ"""
        if file in self.progress['remaining']:
            self.progress['remaining'].remove(file)
        if file not in self.progress['completed']:
            self.progress['completed'].append(file)
        self._save()

    def mark_failed(self, file, error):
        """ì‹¤íŒ¨ í‘œì‹œ"""
        if file in self.progress['remaining']:
            self.progress['remaining'].remove(file)
        self.progress['failed'].append({
            'file': file,
            'error': str(error)
        })
        self._save()

    def get_remaining(self):
        """ë‚¨ì€ íŒŒì¼ ëª©ë¡"""
        return self.progress['remaining']

    def _save(self):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        with open(self.progress_file, 'w') as f:
            json.dump(self.progress, f, indent=2)

# ì‚¬ìš©
tracker = ProgressTracker()

# ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ì„¤ì •
all_files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
tracker.progress['remaining'] = all_files
tracker._save()

# ì²˜ë¦¬
converter = DocumentConverter()
for file in tracker.get_remaining():
    try:
        result = converter.convert(file)
        tracker.mark_completed(file)
        print(f"âœ… {file}")
    except Exception as e:
        tracker.mark_failed(file, e)
        print(f"âŒ {file}: {e}")

# ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥!
```

---

## ë§ˆë¬´ë¦¬

### ğŸ“ í•™ìŠµ ìš”ì•½

**Part 2ì—ì„œ ë°°ìš´ ë‚´ìš©**:
1. âœ… ê³ ê¸‰ ë³€í™˜ ì˜µì…˜ ë° ì„±ëŠ¥ ìµœì í™”
2. âœ… ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬ (DOCX, PPTX, XLSX, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤)
3. âœ… OCRê³¼ ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬ ê¸°ë²•
4. âœ… LangChain, LlamaIndex ë“± AI í”„ë ˆì„ì›Œí¬ í†µí•©
5. âœ… CLI ë„êµ¬ í™œìš© ë° ìŠ¤í¬ë¦½íŒ…
6. âœ… ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ìµœì í™” ì „ëµ
7. âœ… ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ (ë…¼ë¬¸ ê´€ë¦¬, ê³„ì•½ì„œ ë¶„ì„, êµìœ¡ ì½˜í…ì¸ )
8. âœ… ë¬¸ì œ í•´ê²° ë° ì‹¤ì „ íŒ

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

#### ì´ˆë³´ì
- [ ] ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ì‹¤ìŠµ
- [ ] CLI ëª…ë ¹ì–´ ì—°ìŠµ
- [ ] ê°„ë‹¨í•œ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

#### ì¤‘ê¸‰ì
- [ ] AI í”„ë ˆì„ì›Œí¬ ì—°ë™ í”„ë¡œì íŠ¸
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì„±ëŠ¥ ìµœì í™” ì‹¤í—˜

#### ê³ ê¸‰ì
- [ ] í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ ì„¤ê³„
- [ ] ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ê°œë°œ
- [ ] ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬

### ğŸ”— ê´€ë ¨ ë…¸íŠ¸

- [[Docling_ë¬¸ì„œì²˜ë¦¬_AI_ë„êµ¬_ì™„ë²½_ê°€ì´ë“œ_Part1|Part 1: ê¸°ì´ˆí¸]]
- [[LangChain_ì‹¤ì „_í™œìš©_ê°€ì´ë“œ]]
- [[LlamaIndex_ê²€ìƒ‰_ì‹œìŠ¤í…œ_êµ¬ì¶•]]
- [[Python_ìë™í™”_ìŠ¤í¬ë¦½íŠ¸_ì‘ì„±]]
- [[OCR_ê¸°ìˆ _ì™„ë²½_ê°€ì´ë“œ]]
- [[ë°°ì¹˜_ì²˜ë¦¬_ìµœì í™”_ì „ëµ]]

### ğŸ“š ì¶”ê°€ ìë£Œ

- **ê³µì‹ ë¬¸ì„œ**: https://docling-project.github.io/docling/
- **ì˜ˆì œ ëª¨ìŒ**: https://docling-project.github.io/docling/examples/
- **í†µí•© ê°€ì´ë“œ**: https://docling-project.github.io/docling/integrations/
- **GitHub Issues**: https://github.com/docling-project/docling/issues

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-14
**ì‘ì„±ì**: Claude AI (Obsidian Vault ìë™í™” ì‹œìŠ¤í…œ)
**ë‚œì´ë„**: ì¤‘ê¸‰-ê³ ê¸‰
**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 1-2ì‹œê°„
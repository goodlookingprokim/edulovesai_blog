---
title: Scraperr - ë…¸ì½”ë“œ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ ì™„ë²½ ê°€ì´ë“œ
created: 2025-10-08
last_modified: 2025-10-08
tags:
  - ì›¹ìŠ¤í¬ë˜í•‘/ë„êµ¬
  - ë°ì´í„°ìˆ˜ì§‘/ìë™í™”
  - XPath/ì„ íƒì
  - ë…¸ì½”ë“œ/ê°œë°œ
  - ì´ˆë³´ì/ê°€ì´ë“œ
  - Scraperr
status: ì™„ë£Œ
type: ê°€ì´ë“œ
priority: high
share_link: https://share.note.sx/lll81942#gQON324o/ZApzwVihUi5FKegADWz9ZwmdUcSK9qYc2Q
share_updated: 2025-10-08T04:54:52+09:00
---

# Scraperr - ë…¸ì½”ë“œ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ ì™„ë²½ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨ (ë‚´ë¶€ ë§í¬ í™œìš©)
1. [[#ìŠ¤í† ë¦¬ë¡œ ì‹œì‘í•˜ê¸° - ì—°êµ¬ì› ì§€í›ˆì˜ ê³ ë¯¼]]
2. [[#Scraperrë€ ë¬´ì—‡ì¸ê°€? (íŒŒì¸ë§Œ ê¸°ë²•)]]
3. [[#í•µì‹¬ ê°œë… ì‰½ê²Œ ì´í•´í•˜ê¸°]]
4. [[#ì‹¤ì „ ë‹¨ê³„ë³„ ì˜ˆì œ]]
5. [[#ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤]]
6. [[#ë¬¸ì œ í•´ê²° ê°€ì´ë“œ]]
7. [[#ê³ ê¸‰ í™œìš© íŒ¨í„´]]
8. [[#ìš©ì–´ ì‚¬ì „]]

---

## ìŠ¤í† ë¦¬ë¡œ ì‹œì‘í•˜ê¸° - ì—°êµ¬ì› ì§€í›ˆì˜ ê³ ë¯¼

### ğŸ“– ì§€í›ˆì˜ ë°ì´í„° ìˆ˜ì§‘ ì•…ëª½

ì§€í›ˆì€ ë§ˆì¼€íŒ… ë¦¬ì„œì¹˜ íšŒì‚¬ì˜ ì£¼ë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ê²½ìŸì‚¬ ë¶„ì„ í”„ë¡œì íŠ¸ê°€ ì‹œì‘ë˜ì—ˆëŠ”ë°ìš”...

**ğŸ¤¯ ì§€í›ˆì´ ë§ˆì£¼í•œ í˜„ì‹¤:**

```
ìƒì‚¬: "ì§€í›ˆì”¨, ê²½ìŸì‚¬ 100ê°œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ
      ì œí’ˆ ê°€ê²© ì •ë³´ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.
      ë‚´ì¼ê¹Œì§€ìš”!"

ì§€í›ˆ: "ë„¤... ì•Œê² ìŠµë‹ˆë‹¤..." ğŸ˜°
```

**ì§€í›ˆì˜ í•˜ë£¨:**

```
ì˜¤ì „ 9ì‹œ: ì²« ë²ˆì§¸ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
â†’ Ctrl+C, Ctrl+V (ë³µì‚¬ ë¶™ì—¬ë„£ê¸°)
â†’ ì—‘ì…€ì— ìˆ˜ë™ ì…ë ¥

ì˜¤ì „ 10ì‹œ: ë‹¤ì„¯ ë²ˆì§¸ ì›¹ì‚¬ì´íŠ¸...
â†’ ì†ëª©ì´ ì•„í”„ê¸° ì‹œì‘
â†’ ì‹¤ìˆ˜ë¡œ ì˜ëª»ëœ ë°ì´í„° ë³µì‚¬

ì ì‹¬ ì‹œê°„: 15ê°œ ì›¹ì‚¬ì´íŠ¸ ì™„ë£Œ
â†’ 85ê°œ ë‚¨ìŒ...
â†’ ë‚´ì¼ê¹Œì§€ ë¶ˆê°€ëŠ¥! ğŸ˜±

ì˜¤í›„ 2ì‹œ: "íŒŒì´ì¬ìœ¼ë¡œ í¬ë¡¤ë§í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ?"
â†’ Python ì„¤ì¹˜
â†’ BeautifulSoup íŠœí† ë¦¬ì–¼ ê²€ìƒ‰
â†’ ì½”ë“œ ì´í•´ ì•ˆ ë¨ ğŸ˜µ

ì˜¤í›„ 5ì‹œ: ì—¬ì „íˆ 20ê°œ ì›¹ì‚¬ì´íŠ¸...
â†’ ì•¼ê·¼ í™•ì •
â†’ ì£¼ë§ ì¶œê·¼ ì˜ˆìƒ
```

**ğŸ˜« ì§€í›ˆì˜ ì§„ì§œ ê³ ë¯¼:**
```
ë¬¸ì œ 1: ìˆ˜ë™ ì‘ì—… = ë„ˆë¬´ ëŠë¦¼
- 100ê°œ ì›¹ì‚¬ì´íŠ¸ = 3ì¼ ê±¸ë¦¼
- ë°˜ë³µ ì‘ì—… = ì†ëª© í„°ë„ ì¦í›„êµ°

ë¬¸ì œ 2: ì½”ë”© = ë„ˆë¬´ ì–´ë ¤ì›€
- Python ë°°ìš°ê¸° = ì‹œê°„ ì—†ìŒ
- XPath, CSS Selector = ë­”ì§€ ëª¨ë¦„
- ì˜¤ë¥˜ ë””ë²„ê¹… = ë¶ˆê°€ëŠ¥

ë¬¸ì œ 3: ë°ì´í„° ì •ë¦¬ = ë„ˆë¬´ ë³µì¡
- ê° ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë‹¤ë¦„
- ì—‘ì…€ ì •ë¦¬ = ë˜ ë‹¤ë¥¸ ì‘ì—…
- ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ = ì¼ì¼ì´ í´ë¦­
```

### ğŸ˜Š Scraperrë¥¼ ë§Œë‚œ í›„ì˜ ì§€í›ˆ

```
[Scraperr ì›¹ ì¸í„°í˜ì´ìŠ¤]

1ï¸âƒ£ URL ì…ë ¥: competitor1.com
2ï¸âƒ£ ì¶”ì¶œí•  ë°ì´í„° í´ë¦­: ì œí’ˆ ì´ë¦„, ê°€ê²©
3ï¸âƒ£ "ìŠ¤í¬ë˜í•‘ ì‹œì‘" ë²„íŠ¼ í´ë¦­
4ï¸âƒ£ â˜• ì»¤í”¼ ë§ˆì‹œë©° ëŒ€ê¸°
5ï¸âƒ£ ğŸ“Š ë°ì´í„° ìë™ ìˆ˜ì§‘ ì™„ë£Œ!
6ï¸âƒ£ CSV ë‹¤ìš´ë¡œë“œ â†’ ì—‘ì…€ì—ì„œ ë°”ë¡œ ì—´ë¦¼

ì‹œê°„: 100ê°œ ì›¹ì‚¬ì´íŠ¸ = 2ì‹œê°„
ì½”ë“œ: í•œ ì¤„ë„ ì•ˆ ì”€!
ì •í™•ë„: 100%
```

**ì§€í›ˆì˜ ë³€í™”:**
- â° 3ì¼ ì‘ì—… â†’ 2ì‹œê°„ìœ¼ë¡œ ë‹¨ì¶•
- ğŸ’» ì½”ë”© ì§€ì‹ ë¶ˆí•„ìš”
- ğŸ“Š ë°ì´í„° ìë™ ì •ë¦¬
- ğŸ–¼ï¸ ì´ë¯¸ì§€ë„ ìë™ ë‹¤ìš´ë¡œë“œ
- ğŸ”„ ë§¤ì¼ ìë™ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
- ğŸš€ ìƒì‚°ì„± 1500% í–¥ìƒ!

**ìƒì‚¬ì˜ ë°˜ì‘:**
```
ìƒì‚¬: "ì§€í›ˆì”¨, ë²Œì¨ ë‹¤ í–ˆì–´ìš”?
      ë°ì´í„°ë„ ê¹”ë”í•˜ê³ ...
      ë¹„ê²°ì´ ë­ì—ìš”?"

ì§€í›ˆ: "Scraperrë¼ëŠ” ë„êµ¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤!
      ì´ì œ ë§¤ì¼ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ìˆ˜ë„ ìˆì–´ìš”!"

ìƒì‚¬: "ìš°ì™€! ìš°ë¦¬ íŒ€ ì „ì²´ê°€ ì¨ì•¼ê² ëŠ”ë°ìš”!"
```

> **ğŸ’¡ ì¼ìƒ ë¹„ìœ :** ScraperrëŠ” ë§ˆì¹˜ "ë¡œë´‡ ë¹„ì„œ"ì™€ ê°™ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ì†ìœ¼ë¡œ ì¼ì¼ì´ ë³µì‚¬ ë¶™ì—¬ë„£ê¸° í•˜ë˜ ì‘ì—…ì„, ë¡œë´‡ì´ 24ì‹œê°„ ì‰¬ì§€ ì•Šê³  ëŒ€ì‹ í•´ì£¼ëŠ” ë˜‘ë˜‘í•œ ë„êµ¬ì˜ˆìš”!

---

## Scraperrë€ ë¬´ì—‡ì¸ê°€? (íŒŒì¸ë§Œ ê¸°ë²•)

### 5ì‚´ ì•„ì´ì—ê²Œ ì„¤ëª…í•œë‹¤ë©´...

"ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ëŠ” ê²Œì„ì´ ìˆë‹¤ê³  ìƒìƒí•´ë´. ë„ˆëŠ” 'ë¹¨ê°„ ì‚¬ê³¼' ì‚¬ì§„ 100ê°œë¥¼ ì°¾ì•„ì•¼ í•´. ì†ìœ¼ë¡œ í•˜ë‚˜í•˜ë‚˜ ì°¾ìœ¼ë©´ í•˜ë£¨ ì¢…ì¼ ê±¸ë¦¬ì§€?

ScraperrëŠ” **ë§ˆë²• ë¡œë´‡**ì´ì•¼! ë„¤ê°€ 'ë¹¨ê°„ ì‚¬ê³¼ ì°¾ì•„ì¤˜'ë¼ê³  í•˜ë©´, ë¡œë´‡ì´ ì™ì™ ì°¾ì•„ì„œ ì˜ˆìœ ì•¨ë²”ì— ì •ë¦¬í•´ì£¼ëŠ” ê±°ì•¼!"

### ì¤‘í•™ìƒì—ê²Œ ì„¤ëª…í•œë‹¤ë©´...

ì¸í„°ë„·ì—ëŠ” ì—„ì²­ë‚˜ê²Œ ë§ì€ ì •ë³´ê°€ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ì‹ ë°œ ì‡¼í•‘ëª° 100ê°œì—ì„œ ê°€ê²© ë¹„êµë¥¼ í•˜ê³  ì‹¶ë‹¤ë©´?

**ìˆ˜ë™ ë°©ë²•:**
1. ê° ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸
2. ê°€ê²© í™•ì¸í•˜ê³  ë©”ëª¨ì¥ì— ì ê¸°
3. 100ë²ˆ ë°˜ë³µ â†’ 3ì‹œê°„ ì†Œìš” ğŸ˜°

**Scraperr ë°©ë²•:**
1. ì›¹ì‚¬ì´íŠ¸ ëª©ë¡ ì…ë ¥
2. "ê°€ê²©" ìœ„ì¹˜ ì§€ì •
3. ë²„íŠ¼ í´ë¦­
4. 10ë¶„ í›„ ëª¨ë“  ë°ì´í„° ìë™ ìˆ˜ì§‘! ğŸ‰

ScraperrëŠ” ì´ëŸ° **ë°˜ë³µ ì‘ì—…ì„ ìë™í™”**í•´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”. ì½”ë”© ëª°ë¼ë„ ê´œì°®ì•„ìš”!

### ê°œë°œì/ì „ë¬¸ê°€ì—ê²Œ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ë©´...

**Scraperr = ì…€í”„ í˜¸ìŠ¤íŒ… ì›¹ ìŠ¤í¬ë˜í•‘ í”Œë«í¼ (No-Code)**

**í•µì‹¬ êµ¬ì„±:**
```
[ì•„í‚¤í…ì²˜]
Frontend (Next.js + Tailwind CSS)
    â†•ï¸ REST API
Backend (FastAPI + Python)
    â†•ï¸ ë°ì´í„° ì €ì¥
Database (MongoDB)
    â†•ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
Scrapy/Playwright (ì—”ì§„)
```

**ì£¼ìš” ê¸°ëŠ¥:**
- **XPath ê¸°ë°˜ ì¶”ì¶œ**: ì •í™•í•œ DOM ìš”ì†Œ íƒ€ê²ŸíŒ…
- **í ì‹œìŠ¤í…œ**: ì—¬ëŸ¬ ì‘ì—… ë™ì‹œ ê´€ë¦¬ ë° ìš°ì„ ìˆœìœ„ ì§€ì •
- **ë„ë©”ì¸ ìŠ¤íŒŒì´ë”ë§**: ì‚¬ì´íŠ¸ ì „ì²´ ì¬ê·€ì  í¬ë¡¤ë§
- **ì»¤ìŠ¤í…€ í—¤ë”**: User-Agent, Cookie, Auth ì„¤ì •
- **ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ**: ì´ë¯¸ì§€/ë™ì˜ìƒ ìë™ ìˆ˜ì§‘
- **ê²°ê³¼ ì‹œê°í™”**: ì›¹ UIì—ì„œ ì¦‰ì‹œ í™•ì¸
- **ë°ì´í„° ë‚´ë³´ë‚´ê¸°**: CSV, Markdown í˜•ì‹ ì§€ì›
- **ì•Œë¦¼ ì±„ë„**: ì‘ì—… ì™„ë£Œ ì‹œ Slack/Email ë“± í†µì§€

**ê¸°ìˆ  ìŠ¤íƒ:**
- **Frontend**: Next.js 13+ (React Server Components), TypeScript
- **Backend**: FastAPI (async), Pydantic ëª¨ë¸
- **Database**: MongoDB (ë¬¸ì„œ ì§€í–¥ DB)
- **Scraping Engine**: Playwright (ë¸Œë¼ìš°ì € ìë™í™”), Requests
- **Deployment**: Docker Compose, Kubernetes Helm Charts

---

## í•µì‹¬ ê°œë… ì‰½ê²Œ ì´í•´í•˜ê¸°

### 1. ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ (Web Scraping)

**ê°œë…:** ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ 

**ì‹¤ìƒí™œ ë¹„ìœ :**
```
ë„ì„œê´€ì—ì„œ ì±… ì •ë³´ ìˆ˜ì§‘í•˜ê¸°:

ìˆ˜ë™ ë°©ë²• (ì‚¬ëŒ):
- ì±…ì¥ ì•ì— ì„œê¸°
- ì±… í•œ ê¶Œì”© êº¼ë‚´ê¸°
- ì œëª©, ì €ì, ì¶œíŒì‚¬ ë©”ëª¨
- 100ê¶Œ = 3ì‹œê°„

ìë™ ë°©ë²• (ë¡œë´‡):
- ì¹´ë©”ë¼ë¡œ ì±…ì¥ ìŠ¤ìº”
- AIê°€ í…ìŠ¤íŠ¸ ì¸ì‹
- ìë™ìœ¼ë¡œ í‘œ ì‘ì„±
- 100ê¶Œ = 5ë¶„

ì›¹ ìŠ¤í¬ë˜í•‘ = ë¡œë´‡ ë°©ì‹!
```

**ê¸°ìˆ ì  ë™ì‘:**
```python
# ìˆ˜ë™ ë°©ì‹ (ì‚¬ëŒì´ í•˜ëŠ” ì¼)
1. ë¸Œë¼ìš°ì €ë¡œ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
2. ì›í•˜ëŠ” ì •ë³´ ì°¾ê¸°
3. Ctrl+C (ë³µì‚¬)
4. Excelì— Ctrl+V (ë¶™ì—¬ë„£ê¸°)
5. ë‹¤ìŒ ì›¹ì‚¬ì´íŠ¸ë¡œ ì´ë™
6. ë°˜ë³µ...

# ìŠ¤í¬ë˜í•‘ (Scraperrê°€ í•˜ëŠ” ì¼)
1. HTTP ìš”ì²­ìœ¼ë¡œ HTML ê°€ì ¸ì˜¤ê¸°
2. HTML íŒŒì‹± (êµ¬ì¡° ë¶„ì„)
3. ì›í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
4. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
5. ë‹¤ìŒ í˜ì´ì§€ ìë™ ì´ë™
6. ì „ì²´ ìë™í™”!
```

**í•©ë²•ì  ì‚¬ìš© ì˜ˆì‹œ:**
```
âœ… ê³µê°œëœ ê°€ê²© ì •ë³´ ìˆ˜ì§‘
âœ… ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª© ìˆ˜ì§‘
âœ… ì œí’ˆ ë¦¬ë·° ë¶„ì„
âœ… ë¶€ë™ì‚° ë§¤ë¬¼ ì •ë³´
âœ… ì±„ìš© ê³µê³  ìˆ˜ì§‘

âš ï¸ ì£¼ì˜ì‚¬í•­:
- robots.txt í™•ì¸ í•„ìˆ˜
- ì„œë¹„ìŠ¤ ì•½ê´€ ì¤€ìˆ˜
- ë„ˆë¬´ ë¹ ë¥´ê²Œ ìš”ì²­ X (ì„œë²„ ë¶€í•˜)
- ê°œì¸ì •ë³´ ìˆ˜ì§‘ ê¸ˆì§€
```

---

### 2. ğŸ¯ XPath (XML Path Language)

**ê°œë…:** ì›¹í˜ì´ì§€ì—ì„œ ì›í•˜ëŠ” ìš”ì†Œë¥¼ "ì£¼ì†Œ"ë¡œ ì°¾ëŠ” ë°©ë²•

**5ì‚´ ì•„ì´ ì„¤ëª…:**
```
í° ì¥ë‚œê° ìƒìì—ì„œ ë¹¨ê°„ ìë™ì°¨ë¥¼ ì°¾ê³  ì‹¶ì–´.

ë°©ë²• 1: ì†ìœ¼ë¡œ í•˜ë‚˜í•˜ë‚˜ ë’¤ì§€ê¸° (ëŠë¦¼)
ë°©ë²• 2: "2ì¸µ 3ë²ˆì§¸ ì¹¸ ì™¼ìª½" ì£¼ì†Œë¡œ ë°”ë¡œ ì°¾ê¸° (ë¹ ë¦„)

XPath = ì£¼ì†Œ ì‹œìŠ¤í…œ!
```

**ì‹¤ìƒí™œ ë¹„ìœ :**
```
ì•„íŒŒíŠ¸ ì£¼ì†Œ:
ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123 (ë„ë¡œ)
â†’ ëŒ€ë¦¼ì•„íŒŒíŠ¸ (ê±´ë¬¼)
â†’ 101ë™ (ë™)
â†’ 505í˜¸ (í˜¸ìˆ˜)

ì›¹í˜ì´ì§€ XPath:
/html (ì‹œì‘)
â†’ /body (ë³¸ë¬¸)
â†’ /div[1] (ì²« ë²ˆì§¸ ë°•ìŠ¤)
â†’ /h1 (ì œëª©)
```

**HTMLê³¼ XPath:**
```html
<!-- ì›¹í˜ì´ì§€ HTML êµ¬ì¡° -->
<html>
  <body>
    <div class="product">
      <h2>ì•„ì´í° 15</h2>
      <span class="price">1,200,000ì›</span>
    </div>
  </body>
</html>
```

**XPathë¡œ ê°€ê²© ì°¾ê¸°:**
```xpath
ë°©ë²• 1 (ì ˆëŒ€ ê²½ë¡œ):
/html/body/div/span[@class='price']

ë°©ë²• 2 (ìƒëŒ€ ê²½ë¡œ - ë” ê°„ë‹¨):
//span[@class='price']
â†’ ì–´ë””ì— ìˆë“  class='price'ì¸ span ì°¾ê¸°

ë°©ë²• 3 (í…ìŠ¤íŠ¸ ê²€ìƒ‰):
//span[contains(text(), 'ì›')]
â†’ 'ì›'ì´ë¼ëŠ” í…ìŠ¤íŠ¸ í¬í•¨í•œ span ì°¾ê¸°
```

**Scraperrì—ì„œ XPath ì‚¬ìš©:**
```
[Scraperr ì¸í„°í˜ì´ìŠ¤]

1ï¸âƒ£ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
2ï¸âƒ£ ì¶”ì¶œí•˜ê³  ì‹¶ì€ ë°ì´í„° ì˜¤ë¥¸ìª½ í´ë¦­
3ï¸âƒ£ "Inspect" (ê²€ì‚¬) ì„ íƒ
4ï¸âƒ£ ê°œë°œì ë„êµ¬ì—ì„œ ìš”ì†Œ ì˜¤ë¥¸ìª½ í´ë¦­
5ï¸âƒ£ "Copy" â†’ "Copy XPath" ì„ íƒ
6ï¸âƒ£ Scraperrì— ë¶™ì—¬ë„£ê¸°!

â†’ ì½”ë”© ëª°ë¼ë„ XPath ìë™ ìƒì„±!
```

---

### 3. ğŸ“Š DOM (Document Object Model)

**ê°œë…:** ì›¹í˜ì´ì§€ì˜ "ì„¤ê³„ë„"

**ì¼ìƒ ë¹„ìœ :**
```
ì§‘ ì§“ê¸°:

ì„¤ê³„ë„ (DOM):
- 1ì¸µ: ê±°ì‹¤, ì£¼ë°©
- 2ì¸µ: ì¹¨ì‹¤1, ì¹¨ì‹¤2
- ê° ë°©ì˜ ìœ„ì¹˜ì™€ í¬ê¸° ëª…ì‹œ

ì‹¤ì œ ì§‘ (ì›¹í˜ì´ì§€):
- ì„¤ê³„ë„ëŒ€ë¡œ ê±´ì„¤ëœ ì§‘
- ëˆˆìœ¼ë¡œ ë³´ê³  ë§Œì§ˆ ìˆ˜ ìˆìŒ

ì›¹í˜ì´ì§€ë„ ë™ì¼!
HTML = ì„¤ê³„ë„ (DOM)
ë¸Œë¼ìš°ì € í™”ë©´ = ì™„ì„±ëœ ì§‘
```

**HTML â†’ DOM ë³€í™˜:**
```html
<!-- HTML ì½”ë“œ (í…ìŠ¤íŠ¸) -->
<div>
  <h1>ì œëª©</h1>
  <p>ë‚´ìš©</p>
</div>
```

```
DOM íŠ¸ë¦¬ (êµ¬ì¡°):
div (ë¶€ëª¨)
â”œâ”€â”€ h1 (ìì‹1) "ì œëª©"
â””â”€â”€ p (ìì‹2) "ë‚´ìš©"

ë§ˆì¹˜ ê°€ê³„ë„ì²˜ëŸ¼!
í• ì•„ë²„ì§€
â”œâ”€â”€ ì•„ë¹ 
â”‚   â”œâ”€â”€ ë‚˜
â”‚   â””â”€â”€ ë™ìƒ
â””â”€â”€ ì‚¼ì´Œ
```

**Scraperrê°€ DOM ì‚¬ìš©í•˜ëŠ” ë°©ë²•:**
```
1. ì›¹í˜ì´ì§€ ì ‘ì†
   â†’ HTML ì½”ë“œ ë‹¤ìš´ë¡œë“œ

2. HTML â†’ DOM ë³€í™˜
   â†’ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ íŒŒì‹±

3. XPathë¡œ ì›í•˜ëŠ” ë…¸ë“œ ì°¾ê¸°
   â†’ //div[@class='price']

4. í…ìŠ¤íŠ¸ ì¶”ì¶œ
   â†’ "1,200,000ì›"

5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
   â†’ MongoDBì— ê¸°ë¡
```

---

### 4. ğŸ”„ í ì‹œìŠ¤í…œ (Queue System)

**ê°œë…:** ì‘ì—…ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ëŠ” ëŒ€ê¸°ì—´

**ì¼ìƒ ë¹„ìœ :**
```
ì€í–‰ ì°½êµ¬:

ê³ ê°ë“¤ì´ ì¤„ ì„œê¸°:
1ë²ˆ: ê¹€ì² ìˆ˜ (ì˜ˆê¸ˆ)
2ë²ˆ: ì´ì˜í¬ (ëŒ€ì¶œ ìƒë‹´)
3ë²ˆ: ë°•ë¯¼ìˆ˜ (í™˜ì „)

ì°½êµ¬ ì§ì›:
- 1ë²ˆë¶€í„° ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
- 2ë²ˆì€ 1ë²ˆ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
- ìƒˆ ê³ ê°ì€ ë§¨ ë’¤ë¡œ

Scraperr íë„ ë™ì¼!
```

**Scraperr í ì‹œìŠ¤í…œ:**
```
[ìŠ¤í¬ë˜í•‘ ì‘ì—… í]

ëŒ€ê¸° ì¤‘:
1. Amazon ì œí’ˆ ê°€ê²© (ìš°ì„ ìˆœìœ„: ë†’ìŒ)
2. eBay íŒë§¤ì ì •ë³´ (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)
3. AliExpress ë¦¬ë·° (ìš°ì„ ìˆœìœ„: ë‚®ìŒ)

ì‹¤í–‰ ì¤‘:
ğŸ”„ Walmart ì¬ê³  ì •ë³´ ìŠ¤í¬ë˜í•‘...
   ì§„í–‰ë¥ : 47% (235/500 í˜ì´ì§€)

ì™„ë£Œ:
âœ… Target í• ì¸ ì •ë³´ (5ë¶„ ì „)
âœ… Best Buy ë°°ì†¡ë¹„ (10ë¶„ ì „)
```

**íì˜ ì¥ì :**
```
ì¥ì  1: ë™ì‹œ ì‹¤í–‰ ë°©ì§€
- ì›¹ì‚¬ì´íŠ¸ì— ë¶€ë‹´ ì•ˆ ì¤Œ
- ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€

ì¥ì  2: ìš°ì„ ìˆœìœ„ ê´€ë¦¬
- ê¸‰í•œ ì‘ì—… ë¨¼ì € ì²˜ë¦¬
- ì¤‘ìš”ë„ì— ë”°ë¼ ìˆœì„œ ì¡°ì •

ì¥ì  3: ì‹¤íŒ¨ ì²˜ë¦¬
- ì‘ì—… ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
- ì˜¤ë¥˜ ë¡œê·¸ ìë™ ê¸°ë¡

ì¥ì  4: ìì› ê´€ë¦¬
- CPU, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©
- ë™ì‹œ ì‘ì—… ìˆ˜ ì œí•œ ê°€ëŠ¥
```

---

### 5. ğŸ¨ CSS Selector vs XPath

**ê°œë…:** ì›¹ ìš”ì†Œ ì°¾ëŠ” ë‘ ê°€ì§€ ë°©ë²•

**ì‹¤ìƒí™œ ë¹„ìœ :**
```
ì¹œêµ¬ ì§‘ ì°¾ê¸°:

ë°©ë²• 1 (ì£¼ì†Œ - XPath):
"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123ë²ˆê¸¸ 45"
â†’ ì •í™•í•œ ìœ„ì¹˜

ë°©ë²• 2 (íŠ¹ì§• - CSS):
"ë¹¨ê°„ ì§€ë¶•, 2ì¸µì§‘, ì •ì› ìˆëŠ” ì§‘"
â†’ íŠ¹ì§•ìœ¼ë¡œ ì°¾ê¸°

ë‘˜ ë‹¤ ê°™ì€ ì§‘ì„ ì°¾ì„ ìˆ˜ ìˆìŒ!
```

**CSS Selector:**
```css
/* í´ë˜ìŠ¤ë¡œ ì°¾ê¸° */
.price  â†’ class="price"ì¸ ìš”ì†Œ

/* IDë¡œ ì°¾ê¸° */
#title  â†’ id="title"ì¸ ìš”ì†Œ

/* íƒœê·¸ë¡œ ì°¾ê¸° */
div  â†’ ëª¨ë“  div ìš”ì†Œ

/* ì¡°í•© */
div.product > span.price
â†’ class="product"ì¸ div ì•ˆì˜
   class="price"ì¸ span
```

**XPath:**
```xpath
/* í´ë˜ìŠ¤ë¡œ ì°¾ê¸° */
//span[@class='price']

/* IDë¡œ ì°¾ê¸° */
//h1[@id='title']

/* íƒœê·¸ë¡œ ì°¾ê¸° */
//div

/* ì¡°í•© */
//div[@class='product']/span[@class='price']
```

**ë¹„êµ:**
```
[ì†ë„]
CSS: âš¡âš¡âš¡ ë¹ ë¦„
XPath: âš¡âš¡ ì•½ê°„ ëŠë¦¼

[í‘œí˜„ë ¥]
CSS: ğŸ¯ ê°„ë‹¨í•œ ì„ íƒ
XPath: ğŸ¯ğŸ¯ğŸ¯ ë³µì¡í•œ ì„ íƒ ê°€ëŠ¥

[í˜¸í™˜ì„±]
CSS: ë¸Œë¼ìš°ì € ìµœì í™”
XPath: XML/HTML ëª¨ë‘ ì§€ì›

[Scraperr ì¶”ì²œ]
â†’ XPath ì‚¬ìš©!
   (ë” ê°•ë ¥í•˜ê³  ìœ ì—°í•¨)
```

---

### 6. ğŸ¤– í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € (Headless Browser)

**ê°œë…:** í™”ë©´ ì—†ì´ ì‹¤í–‰ë˜ëŠ” ì›¹ ë¸Œë¼ìš°ì €

**5ì‚´ ì•„ì´ ì„¤ëª…:**
```
ë³´í†µ ê²Œì„:
- TV í™”ë©´ ë³´ë©´ì„œ ê²Œì„
- ëˆˆìœ¼ë¡œ ìºë¦­í„° í™•ì¸

í—¤ë“œë¦¬ìŠ¤ (ë¨¸ë¦¬ ì—†ëŠ”) ê²Œì„:
- í™”ë©´ ì•ˆ ë³´ê³  ê²Œì„
- ë¡œë´‡ì´ ìë™ìœ¼ë¡œ í”Œë ˆì´
- ê²°ê³¼ë§Œ ì•Œë ¤ì¤Œ

ì›¹ì‚¬ì´íŠ¸ë„ ë§ˆì°¬ê°€ì§€!
```

**ì¼ë°˜ ë¸Œë¼ìš°ì € vs í—¤ë“œë¦¬ìŠ¤:**
```
[ì¼ë°˜ Chrome]
1. ì°½ ì—´ë¦¼ ğŸ‘ï¸
2. ì›¹ì‚¬ì´íŠ¸ ë¡œë”© ë³´ì„
3. ìŠ¤í¬ë¡¤, í´ë¦­ ëˆˆìœ¼ë¡œ í™•ì¸
4. ëŠë¦¼ (í™”ë©´ ê·¸ë¦¬ê¸° ì‹œê°„)

[í—¤ë“œë¦¬ìŠ¤ Chrome]
1. ì°½ ì•ˆ ì—´ë¦¼ (ë°±ê·¸ë¼ìš´ë“œ)
2. ì›¹ì‚¬ì´íŠ¸ ë¡œë”© (ë³´ì´ì§€ ì•ŠìŒ)
3. ìë™ìœ¼ë¡œ ìŠ¤í¬ë¡¤, í´ë¦­
4. ë¹ ë¦„ (í™”ë©´ ì•ˆ ê·¸ë ¤ì„œ)
```

**Scraperrì—ì„œ ì‚¬ìš©:**
```python
# Playwright (í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì €)
browser = playwright.chromium.launch(
    headless=True  # í™”ë©´ ì•ˆ ë„ìš°ê¸°
)

page = browser.new_page()
page.goto('https://example.com')

# JavaScriptë¡œ ë Œë”ë§ëœ ë‚´ìš©ë„ ê°€ì ¸ì˜´!
content = page.content()

# ìŠ¤í¬ë¦°ìƒ·ë„ ê°€ëŠ¥
page.screenshot(path='page.png')
```

**ì¥ì :**
```
ì¥ì  1: ì†ë„
- í™”ë©´ ë Œë”ë§ ì•ˆ í•´ì„œ ë¹ ë¦„
- CPU, GPU ìì› ì ˆì•½

ì¥ì  2: ì„œë²„ ì‹¤í–‰
- í™”ë©´ ì—†ëŠ” ì„œë²„ì—ì„œë„ ì‘ë™
- ìë™í™” ì‘ì—…ì— ìµœì 

ì¥ì  3: JavaScript ì²˜ë¦¬
- Ajax, React ë“± ë™ì  ì½˜í…ì¸ 
- ì¼ë°˜ HTTP ìš”ì²­ìœ¼ë¡œ ì•ˆ ë˜ëŠ” ê²ƒë„ ê°€ëŠ¥

ì¥ì  4: ëŒ€ëŸ‰ ì‘ì—…
- ì—¬ëŸ¬ ë¸Œë¼ìš°ì € ë™ì‹œ ì‹¤í–‰
- ìˆ˜ë°± ê°œ í˜ì´ì§€ ë³‘ë ¬ ì²˜ë¦¬
```

---

### 7. ğŸ“¦ ë„ì»¤ (Docker)

**ê°œë…:** í”„ë¡œê·¸ë¨ì„ "ìƒì"ì— ë„£ì–´ì„œ ì–´ë””ì„œë“  ë˜‘ê°™ì´ ì‹¤í–‰

**ì¼ìƒ ë¹„ìœ :**
```
ì»µë¼ë©´:

ì¼ë°˜ ìš”ë¦¬:
- ì¬ë£Œ ì‚¬ê¸° (ì•¼ì±„, ë©´, ìœ¡ìˆ˜)
- ì¡°ë¦¬ ë„êµ¬ ì¤€ë¹„
- ë ˆì‹œí”¼ ë”°ë¼ ìš”ë¦¬
- ì§‘ë§ˆë‹¤ ë§›ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ

ì»µë¼ë©´:
- ëª¨ë“  ì¬ë£Œê°€ ì»µì— í¬ì¥ë¨
- ëœ¨ê±°ìš´ ë¬¼ë§Œ ë¶€ìœ¼ë©´ ë¨
- ì–´ë””ì„œë“  ë˜‘ê°™ì€ ë§›
- ì‹¤íŒ¨ í™•ë¥  0%

Docker = ì†Œí”„íŠ¸ì›¨ì–´ ì»µë¼ë©´!
```

**ì „í†µ ë°©ì‹ vs Docker:**
```
[ì „í†µ ë°©ì‹]
ì„¤ì¹˜ ê³¼ì •:
1. Python 3.11 ì„¤ì¹˜
2. Node.js 18 ì„¤ì¹˜
3. MongoDB ì„¤ì¹˜
4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
5. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (100ê°œ+)
6. ì„¤ì • íŒŒì¼ ì‘ì„±
â†’ 3ì‹œê°„ ì†Œìš”
â†’ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
â†’ ì»´í“¨í„°ë§ˆë‹¤ ë‹¤ë¦„

[Docker ë°©ì‹]
docker-compose up
â†’ 5ë¶„ ì†Œìš”
â†’ 100% ì‘ë™ ë³´ì¥
â†’ ì–´ë””ì„œë“  ë™ì¼
```

**Scraperr + Docker:**
```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/jaypyles/Scraperr.git
cd Scraperr

# 2. Dockerë¡œ ì‹¤í–‰ (ë!)
make up

# ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ:
# - Python í™˜ê²½ êµ¬ì„±
# - Node.js ì•± ë¹Œë“œ
# - MongoDB ì‹œì‘
# - ëª¨ë“  ì„¤ì • ì™„ë£Œ
# - ì›¹ ì„œë²„ ì‹¤í–‰

# 3. ë¸Œë¼ìš°ì € ì ‘ì†
http://localhost:3000
```

**Dockerì˜ ë§ˆë²•:**
```
[Docker ì»¨í…Œì´ë„ˆ êµ¬ì¡°]

ì»¨í…Œì´ë„ˆ 1: Frontend
- Next.js ì•±
- í¬íŠ¸ 3000

ì»¨í…Œì´ë„ˆ 2: Backend
- FastAPI ì„œë²„
- í¬íŠ¸ 8000

ì»¨í…Œì´ë„ˆ 3: MongoDB
- ë°ì´í„°ë² ì´ìŠ¤
- í¬íŠ¸ 27017

â†’ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
â†’ ì„œë¡œ í†µì‹  ê°€ëŠ¥
â†’ í•˜ë‚˜ ë¬¸ì œ ìƒê²¨ë„ ë‹¤ë¥¸ ê±´ OK
```

---

## ì‹¤ì „ ë‹¨ê³„ë³„ ì˜ˆì œ

### ğŸŒ± ì˜ˆì œ 1: ê¸°ì´ˆ - ì²« ìŠ¤í¬ë˜í•‘ ì‘ì—… (ì´ˆë³´ììš©)

**ì‹œë‚˜ë¦¬ì˜¤:** ì˜¨ë¼ì¸ ì„œì ì—ì„œ ì±… ì œëª©ê³¼ ê°€ê²© ìˆ˜ì§‘í•˜ê¸°

#### 1ë‹¨ê³„: Scraperr ì„¤ì¹˜ ë° ì‹¤í–‰

```bash
# í„°ë¯¸ë„ ì—´ê¸°

# 1. ì €ì¥ì†Œ ë‹¤ìš´ë¡œë“œ
git clone https://github.com/jaypyles/Scraperr.git
cd Scraperr

# 2. Dockerë¡œ ì‹¤í–‰
make up

# ì¶œë ¥:
# Creating network "scraperr_default"...
# Creating scraperr_mongodb_1...
# Creating scraperr_backend_1...
# Creating scraperr_frontend_1...
# âœ“ Started successfully!
```

**ğŸ¤” Dockerê°€ ì—†ìœ¼ë©´?**
```bash
# Docker Desktop ì„¤ì¹˜
# Mac: https://docs.docker.com/desktop/install/mac-install/
# Windows: https://docs.docker.com/desktop/install/windows-install/

# ì„¤ì¹˜ í™•ì¸
docker --version
# ì¶œë ¥: Docker version 24.0.0
```

#### 2ë‹¨ê³„: ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì†

```
ë¸Œë¼ìš°ì € ì—´ê¸°:
http://localhost:3000

[Scraperr í™ˆ í™”ë©´]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ•·ï¸ Scraperr                    â”‚
â”‚                                 â”‚
â”‚  [+ New Scraping Job]           â”‚
â”‚                                 â”‚
â”‚  Recent Jobs:                   â”‚
â”‚  (ì•„ì§ ì—†ìŒ)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3ë‹¨ê³„: ì²« ìŠ¤í¬ë˜í•‘ ì‘ì—… ìƒì„±

```
[+ New Scraping Job] í´ë¦­

[ì„¤ì • í™”ë©´]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job Name:                      â”‚
â”‚  [ì±… ê°€ê²© ìˆ˜ì§‘]                  â”‚
â”‚                                 â”‚
â”‚  Target URL:                    â”‚
â”‚  [https://example-bookstore.com]â”‚
â”‚                                 â”‚
â”‚  XPath Selectors:               â”‚
â”‚  Title: [//h2[@class='book']]   â”‚
â”‚  Price: [//span[@class='price']]â”‚
â”‚                                 â”‚
â”‚  [Start Scraping] ë²„íŠ¼          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**XPath ì°¾ëŠ” ë°©ë²•:**
```
1. ëª©í‘œ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
   https://example-bookstore.com

2. ì±… ì œëª©ì— ì˜¤ë¥¸ìª½ í´ë¦­
   â†’ "ê²€ì‚¬" (Inspect) ì„ íƒ

3. ê°œë°œì ë„êµ¬ ì—´ë¦¼:
   <h2 class="book-title">í•´ë¦¬í¬í„°</h2>
              â†‘ ì—¬ê¸° ì˜¤ë¥¸ìª½ í´ë¦­

4. "Copy" â†’ "Copy XPath" ì„ íƒ
   â†’ //h2[@class='book-title']

5. Scraperrì— ë¶™ì—¬ë„£ê¸°!
```

#### 4ë‹¨ê³„: ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸

```
[Start Scraping] í´ë¦­!

[ì§„í–‰ ìƒí™©]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Status: Running... ğŸ”„          â”‚
â”‚  Progress: 15/50 pages          â”‚
â”‚  Time elapsed: 00:02:35         â”‚
â”‚                                 â”‚
â”‚  [Cancel] [View Log]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... 2ë¶„ í›„ ...

[ì™„ë£Œ!]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Status: Completed âœ…           â”‚
â”‚  Items scraped: 250 books       â”‚
â”‚  Time taken: 00:05:12           â”‚
â”‚                                 â”‚
â”‚  [View Results] [Export CSV]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5ë‹¨ê³„: ê²°ê³¼ í™•ì¸ ë° ë‚´ë³´ë‚´ê¸°

```
[View Results] í´ë¦­

[ê²°ê³¼ í…Œì´ë¸”]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Title              â”‚ Price      â”‚ URL       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í•´ë¦¬í¬í„°ì™€ ë§ˆë²•ì‚¬ì˜ ëŒâ”‚ 15,000ì›   â”‚ /book/1  â”‚
â”‚ ë°˜ì§€ì˜ ì œì™•         â”‚ 18,000ì›   â”‚ /book/2  â”‚
â”‚ ì–´ë¦°ì™•ì            â”‚ 12,000ì›   â”‚ /book/3  â”‚
â”‚ ...                â”‚ ...        â”‚ ...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Export CSV] í´ë¦­ â†’ books.csv ë‹¤ìš´ë¡œë“œ
```

**CSV íŒŒì¼ ë‚´ìš©:**
```csv
title,price,url
í•´ë¦¬í¬í„°ì™€ ë§ˆë²•ì‚¬ì˜ ëŒ,15000,https://example.com/book/1
ë°˜ì§€ì˜ ì œì™•,18000,https://example.com/book/2
ì–´ë¦°ì™•ì,12000,https://example.com/book/3
```

**Excelì—ì„œ ì—´ê¸°:**
```
books.csv ë”ë¸”í´ë¦­
â†’ Excel ìë™ ì‹¤í–‰
â†’ í‘œ í˜•íƒœë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬ë¨!
â†’ ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥ (í‰ê·  ê°€ê²©, ì •ë ¬ ë“±)
```

#### ì „ì²´ íë¦„ ìš”ì•½:

```
[ë°ì´í„° íë¦„]
1. ì›¹ì‚¬ì´íŠ¸
   â†“ (HTML)
2. Scraperr Backend
   â†“ (íŒŒì‹±)
3. XPath ë§¤ì¹­
   â†“ (ì¶”ì¶œ)
4. MongoDB ì €ì¥
   â†“ (ì¡°íšŒ)
5. Frontend í‘œì‹œ
   â†“ (ë‹¤ìš´ë¡œë“œ)
6. CSV íŒŒì¼
```

**ğŸ¤” ìƒê°í•´ë³´ê¸°:**
- ìˆ˜ë™ìœ¼ë¡œ í–ˆë‹¤ë©´ ì–¼ë§ˆë‚˜ ê±¸ë ¸ì„ê¹Œìš”?
- XPathë¥¼ ì§ì ‘ ì‘ì„±í•˜ì§€ ì•Šê³  ë³µì‚¬ë§Œ í–ˆë‚˜ìš”?
- ë‹¤ë¥¸ ì›¹ì‚¬ì´íŠ¸ì—ë„ ë˜‘ê°™ì´ ì ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?

**âœ… ì„±ê³µ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Scraperr ì„¤ì¹˜ ë° ì‹¤í–‰
- [ ] ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì† ì„±ê³µ
- [ ] XPath ì¶”ì¶œ ë°©ë²• ì´í•´
- [ ] ì²« ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ
- [ ] CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í™•ì¸

---

### ğŸŒ¿ ì˜ˆì œ 2: ì¤‘ê¸‰ - ì´ë¯¸ì§€ í¬í•¨ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ (ì‹¤ë¬´ ì ìš©)

**ì‹œë‚˜ë¦¬ì˜¤:** ì´ì»¤ë¨¸ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì œí’ˆëª…, ê°€ê²©, ì´ë¯¸ì§€, ë¦¬ë·° ì ìˆ˜ ìˆ˜ì§‘

#### í”„ë¡œì íŠ¸ ëª©í‘œ:
```
ìˆ˜ì§‘ ëŒ€ìƒ: ì „ìì œí’ˆ ì‡¼í•‘ëª°
ë°ì´í„°:
- ì œí’ˆëª…
- ê°€ê²©
- ì´ë¯¸ì§€ URL
- í‰ì  (ë³„ì )
- ë¦¬ë·° ìˆ˜

ëª©ì : ê²½ìŸì‚¬ ê°€ê²© ë¶„ì„
```

#### 1ë‹¨ê³„: ë³µì¡í•œ XPath ì‘ì„±

**ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„:**
```html
<!-- ì‹¤ì œ HTML ì˜ˆì‹œ -->
<div class="product-card">
  <img src="/images/iphone15.jpg" alt="iPhone 15" />
  <h3 class="product-name">iPhone 15 Pro</h3>
  <div class="price-box">
    <span class="original">1,500,000ì›</span>
    <span class="sale">1,200,000ì›</span>
  </div>
  <div class="rating">
    <span class="stars">â˜…â˜…â˜…â˜…â˜†</span>
    <span class="count">(128)</span>
  </div>
</div>
```

**XPath ì„¤ê³„:**
```xpath
# ì œí’ˆëª…
//h3[@class='product-name']/text()
â†’ "iPhone 15 Pro"

# í• ì¸ ê°€ê²© (ì—†ìœ¼ë©´ ì›ê°€)
//span[@class='sale']/text() | //span[@class='original']/text()
â†’ "1,200,000ì›"

# ì´ë¯¸ì§€ URL
//div[@class='product-card']//img/@src
â†’ "/images/iphone15.jpg"

# í‰ì  (ë³„ ê°œìˆ˜ ì„¸ê¸°)
count(//span[@class='stars']/text()[contains(., 'â˜…')])
â†’ 4

# ë¦¬ë·° ìˆ˜ (ìˆ«ìë§Œ ì¶”ì¶œ)
translate(//span[@class='count']/text(), '()', '')
â†’ "128"
```

#### 2ë‹¨ê³„: Scraperr ì„¤ì • (ê³ ê¸‰)

```yaml
# Job Configuration
name: "ì „ìì œí’ˆ ê°€ê²© ë¹„êµ"
url: "https://electronics-store.com/products"

selectors:
  product_name:
    xpath: "//h3[@class='product-name']/text()"
    type: "text"

  price:
    xpath: "//span[@class='sale']/text() | //span[@class='original']/text()"
    type: "text"
    transform: "extract_numbers"  # 1,200,000ì› â†’ 1200000

  image:
    xpath: "//div[@class='product-card']//img/@src"
    type: "url"
    download: true  # ì´ë¯¸ì§€ ìë™ ë‹¤ìš´ë¡œë“œ

  rating:
    xpath: "count(//span[@class='stars']/text()[contains(., 'â˜…')])"
    type: "number"

  review_count:
    xpath: "translate(//span[@class='count']/text(), '()', '')"
    type: "number"

pagination:
  enabled: true
  next_button: "//a[@class='next-page']"
  max_pages: 10

rate_limit:
  requests_per_second: 2  # ì„œë²„ì— ë¶€ë‹´ ì•ˆ ì£¼ê¸°
  delay_between_pages: 3  # 3ì´ˆ ëŒ€ê¸°
```

#### 3ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬

**ì—¬ëŸ¬ í˜ì´ì§€ ìë™ ìˆ˜ì§‘:**
```
í˜ì´ì§€ 1: products?page=1 (20ê°œ ì œí’ˆ)
í˜ì´ì§€ 2: products?page=2 (20ê°œ ì œí’ˆ)
í˜ì´ì§€ 3: products?page=3 (20ê°œ ì œí’ˆ)
...
í˜ì´ì§€ 10: products?page=10 (20ê°œ ì œí’ˆ)

â†’ ì´ 200ê°œ ì œí’ˆ ìë™ ìˆ˜ì§‘!
```

**Scraperr í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •:**
```python
# ë‚´ë¶€ ë™ì‘ (ì°¸ê³ ìš©)
current_page = 1
max_pages = 10

while current_page <= max_pages:
    # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
    scrape_page(url + f"?page={current_page}")

    # "ë‹¤ìŒ" ë²„íŠ¼ ì°¾ê¸°
    next_button = find_element("//a[@class='next-page']")

    if next_button:
        next_button.click()
        current_page += 1
        time.sleep(3)  # 3ì´ˆ ëŒ€ê¸°
    else:
        break  # ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ
```

#### 4ë‹¨ê³„: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„¤ì •

```
[Scraperr Advanced Settings]

Image Download:
âœ… Enable automatic image download
ğŸ“ Save location: ./downloads/images/
ğŸ·ï¸ Naming: {product_name}_{timestamp}.jpg

ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ:
./downloads/images/
â”œâ”€â”€ iPhone_15_Pro_20240108_143022.jpg
â”œâ”€â”€ Galaxy_S24_20240108_143025.jpg
â”œâ”€â”€ MacBook_Pro_20240108_143028.jpg
â””â”€â”€ ...
```

#### 5ë‹¨ê³„: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```
[ì‹¤í–‰ ì¤‘ ëŒ€ì‹œë³´ë“œ]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job: ì „ìì œí’ˆ ê°€ê²© ë¹„êµ                  â”‚
â”‚  Status: Running ğŸ”„                      â”‚
â”‚                                          â”‚
â”‚  Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% (8/10 pages) â”‚
â”‚  Items: 156 products                     â”‚
â”‚  Images: 142 downloaded                  â”‚
â”‚  Errors: 3 (retrying...)                 â”‚
â”‚                                          â”‚
â”‚  Current: Page 8 - Loading...            â”‚
â”‚  ETA: 00:02:30                           â”‚
â”‚                                          â”‚
â”‚  [Pause] [Cancel] [View Log]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ì‹¤ì‹œê°„ ë¡œê·¸]
14:30:22 - Page 7 completed (20 items)
14:30:25 - Downloading image: iphone15.jpg
14:30:26 - Image saved: ./downloads/iphone15.jpg
14:30:27 - Moving to page 8...
14:30:30 - Page 8 loaded (20 items)
```

#### 6ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

**ìˆ˜ì§‘ëœ ë°ì´í„°:**
```csv
product_name,price,image_path,rating,review_count
iPhone 15 Pro,1200000,./downloads/iphone15.jpg,4,128
Galaxy S24,1100000,./downloads/galaxys24.jpg,5,95
MacBook Pro,2500000,./downloads/macbook.jpg,4,67
```

**Excelì—ì„œ ë¶„ì„:**
```
=AVERAGE(B2:B200)  # í‰ê·  ê°€ê²©
=MAX(D2:D200)      # ìµœê³  í‰ì 
=SUM(E2:E200)      # ì´ ë¦¬ë·° ìˆ˜

í”¼ë²— í…Œì´ë¸”:
- ë¸Œëœë“œë³„ í‰ê·  ê°€ê²©
- í‰ì ë³„ ì œí’ˆ ë¶„í¬
- ê°€ê²©ëŒ€ë³„ ë¦¬ë·° ìˆ˜
```

**ëŒ€ì‹œë³´ë“œ ìƒì„± (ì„ íƒ):**
```python
import pandas as pd
import matplotlib.pyplot as plt

# CSV ì½ê¸°
df = pd.read_csv('products.csv')

# ê°€ê²© ë¶„í¬ ê·¸ë˜í”„
plt.hist(df['price'], bins=20)
plt.title('ì œí’ˆ ê°€ê²© ë¶„í¬')
plt.xlabel('ê°€ê²© (ì›)')
plt.ylabel('ì œí’ˆ ìˆ˜')
plt.show()

# í‰ì ë³„ í‰ê·  ê°€ê²©
df.groupby('rating')['price'].mean().plot(kind='bar')
plt.title('í‰ì ë³„ í‰ê·  ê°€ê²©')
plt.show()
```

#### ì‹¤ë¬´ í™œìš© íŒ:

**1. ìŠ¤ì¼€ì¤„ë§ (ìë™ ì‹¤í–‰):**
```bash
# cronìœ¼ë¡œ ë§¤ì¼ ì•„ì¹¨ 9ì‹œ ì‹¤í–‰
0 9 * * * cd /path/to/scraperr && make run-job electronics

# ë§¤ì£¼ ì›”ìš”ì¼ ìë™ ì—…ë°ì´íŠ¸
0 9 * * 1 cd /path/to/scraperr && make run-job electronics
```

**2. ì•Œë¦¼ ì„¤ì •:**
```yaml
notifications:
  email:
    enabled: true
    recipients: ["team@company.com"]
    on: ["complete", "error"]

  slack:
    enabled: true
    webhook: "https://hooks.slack.com/..."
    channel: "#price-monitoring"
```

**3. ë°ì´í„° ë³€í™” ê°ì§€:**
```python
# ì´ì „ ë°ì´í„°ì™€ ë¹„êµ
old_data = pd.read_csv('products_old.csv')
new_data = pd.read_csv('products_new.csv')

# ê°€ê²© ë³€ë™ ê°ì§€
price_changes = new_data[new_data['price'] != old_data['price']]

if not price_changes.empty:
    send_alert(f"ê°€ê²© ë³€ë™ ê°ì§€: {len(price_changes)}ê°œ ì œí’ˆ")
```

**ğŸ¤” ìƒê°í•´ë³´ê¸°:**
- ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œê°€ ì™œ í•„ìš”í•œê°€ìš”?
- í˜ì´ì§€ë„¤ì´ì…˜ ì—†ì´ ìˆ˜ë™ìœ¼ë¡œ í•œë‹¤ë©´ ì–¼ë§ˆë‚˜ ê±¸ë¦´ê¹Œìš”?
- ë§¤ì¼ ìë™ìœ¼ë¡œ ì‹¤í–‰í•˜ë©´ ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆë‚˜ìš”?

**âœ… ì¤‘ê¸‰ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ë³µì¡í•œ XPath ì‘ì„±
- [ ] í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
- [ ] ì´ë¯¸ì§€ ìë™ ë‹¤ìš´ë¡œë“œ
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê²½í—˜
- [ ] CSV ë°ì´í„° ë¶„ì„
- [ ] ìŠ¤ì¼€ì¤„ë§ ì„¤ì • (ì„ íƒ)

---

### ğŸŒ³ ì˜ˆì œ 3: ê³ ê¸‰ - ë™ì  ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ (ì‹¬í™” í•™ìŠµ)

**ì‹œë‚˜ë¦¬ì˜¤:** JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” SPA(Single Page Application) ìŠ¤í¬ë˜í•‘

#### ë¬¸ì œ ìƒí™©:

**ì¼ë°˜ ìŠ¤í¬ë˜í•‘ì´ ì•ˆ ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸:**
```
ë¬¸ì œ ì›¹ì‚¬ì´íŠ¸ íŠ¹ì§•:
- React, Vue, Angular ë“±ìœ¼ë¡œ ì œì‘
- í˜ì´ì§€ ë¡œë“œ ì‹œ ë¹ˆ HTML
- JavaScriptê°€ ë°ì´í„° ë¡œë”©
- ë¬´í•œ ìŠ¤í¬ë¡¤
- Ajaxë¡œ ë™ì  ë°ì´í„° ìš”ì²­
```

**ì¼ë°˜ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì˜ˆ:**
```python
# ì¼ë°˜ HTTP ìš”ì²­
import requests
html = requests.get('https://spa-website.com').text

print(html)
# ì¶œë ¥:
# <div id="root"></div>  â† ë¹„ì–´ìˆìŒ!
# <script src="app.js"></script>

# XPathë¡œ ì°¾ìœ¼ë ¤ í•´ë„:
# â†’ ë°ì´í„° ì—†ìŒ (JavaScriptê°€ ì•„ì§ ì‹¤í–‰ ì•ˆ ë¨)
```

#### í•´ê²°ì±…: í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € í™œìš©

**Scraperrì˜ Playwright ëª¨ë“œ:**
```yaml
# Advanced Configuration
scraping_mode: "browser"  # HTTP ëŒ€ì‹  ë¸Œë¼ìš°ì € ì‚¬ìš©

browser_options:
  headless: true
  javascript_enabled: true
  wait_for: "networkidle"  # ëª¨ë“  ìš”ì²­ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
  timeout: 30000  # 30ì´ˆ

page_actions:
  - action: "wait"
    selector: "//div[@class='product-list']"
    timeout: 10000

  - action: "scroll"
    direction: "bottom"
    times: 5  # ë¬´í•œ ìŠ¤í¬ë¡¤ 5ë²ˆ

  - action: "click"
    selector: "//button[@class='load-more']"
    optional: true
```

#### 1ë‹¨ê³„: ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬

**ì›¹ì‚¬ì´íŠ¸ ë™ì‘:**
```
ì‚¬ìš©ì ìŠ¤í¬ë¡¤ â†“
â†’ JavaScript ê°ì§€
â†’ Ajax ìš”ì²­: /api/products?page=2
â†’ ìƒˆ ì œí’ˆ 20ê°œ ë¡œë”©
â†’ í™”ë©´ì— ì¶”ê°€

ë°˜ë³µ...
```

**Scraperr ìë™í™”:**
```python
# ë‚´ë¶€ ë™ì‘ (ì°¸ê³ )
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    page = browser.new_page()
    page.goto('https://spa-website.com')

    # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
    page.wait_for_selector('//div[@class="product-list"]')

    # ë¬´í•œ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜
    for i in range(5):
        # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
        page.evaluate('window.scrollTo(0, document.body.scrollHeight)')

        # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        page.wait_for_timeout(2000)

        print(f"ìŠ¤í¬ë¡¤ {i+1}/5 ì™„ë£Œ")

    # ì´ì œ ëª¨ë“  ë°ì´í„° ë¡œë“œë¨!
    html = page.content()
```

#### 2ë‹¨ê³„: Ajax ìš”ì²­ ê°€ë¡œì±„ê¸°

**ê³ ê¸‰ ê¸°ë²•: ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§**
```python
# API ìš”ì²­ ì§ì ‘ ê°€ë¡œì±„ê¸°
page.on('response', lambda response:
    handle_response(response)
    if '/api/products' in response.url
    else None
)

def handle_response(response):
    if response.status == 200:
        data = response.json()
        # API ì‘ë‹µì„ ì§ì ‘ íŒŒì‹±
        # HTML íŒŒì‹±ë³´ë‹¤ í›¨ì”¬ ê¹”ë”!
        for product in data['products']:
            print(f"{product['name']}: {product['price']}")
```

**Scraperr API ì¸í„°ì…‰íŠ¸ ì„¤ì •:**
```yaml
api_intercept:
  enabled: true
  endpoints:
    - pattern: "/api/products*"
      extract:
        products: "$.data.products[*]"
      fields:
        name: "$.name"
        price: "$.price"
        stock: "$.inventory.quantity"
```

#### 3ë‹¨ê³„: ë¡œê·¸ì¸ í•„ìš”í•œ ì‚¬ì´íŠ¸

**ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°:**
```yaml
authentication:
  method: "form"  # ë˜ëŠ” "basic", "oauth"

  login_url: "https://website.com/login"

  credentials:
    username_field: "//input[@name='email']"
    password_field: "//input[@name='password']"
    submit_button: "//button[@type='submit']"

  username: "${ENV:SCRAPER_USERNAME}"  # í™˜ê²½ ë³€ìˆ˜
  password: "${ENV:SCRAPER_PASSWORD}"

  success_indicator: "//div[@class='user-menu']"

  session_management:
    cookies_file: "./cookies.json"
    reuse_session: true  # ì„¸ì…˜ ì¬ì‚¬ìš©
```

**ì‹¤í–‰ ìˆœì„œ:**
```
1. ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†
   â†“
2. ì´ë©”ì¼ ì…ë ¥
   â†“
3. ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
   â†“
4. ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
   â†“
5. ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
   â†“
6. ì¿ í‚¤ ì €ì¥
   â†“
7. ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹œì‘
   â†“
8. ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¿ í‚¤ ì¬ì‚¬ìš©
```

#### 4ë‹¨ê³„: CAPTCHA ìš°íšŒ ì „ëµ

**ì£¼ì˜: í•©ë²•ì  ë°©ë²•ë§Œ!**
```yaml
captcha_handling:
  strategy: "manual"  # ìë™ ìš°íšŒ X

  on_captcha_detected:
    - action: "pause"
    - action: "notify"
      message: "CAPTCHA ë°œê²¬ - ìˆ˜ë™ í•´ê²° í•„ìš”"
    - action: "wait_for_user"

  retry:
    max_attempts: 3
    delay: 60000  # 1ë¶„ ëŒ€ê¸°
```

**ê¶Œì¥ ë°©ë²•:**
```
1. ìš”ì²­ ì†ë„ ì¤„ì´ê¸°
   - rate_limit: 1 request / 5 seconds

2. User-Agent ë‹¤ì–‘í™”
   - ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê¸°

3. ì¿ í‚¤/ì„¸ì…˜ ìœ ì§€
   - ë¡œë´‡ìœ¼ë¡œ ì•ˆ ë³´ì´ê²Œ

4. ì‹œê°„ëŒ€ ë¶„ì‚°
   - í•œêº¼ë²ˆì— ë§ì´ ìš”ì²­ X

5. API ì‚¬ìš© ê³ ë ¤
   - ê³µì‹ APIê°€ ìˆë‹¤ë©´ ì‚¬ìš©
```

#### 5ë‹¨ê³„: ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„

**ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬:**
```yaml
error_handling:
  on_timeout:
    action: "retry"
    max_retries: 3
    backoff: "exponential"  # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ...

  on_404:
    action: "skip"
    log: true

  on_500:
    action: "retry"
    max_retries: 5
    delay: 30000

  on_network_error:
    action: "retry"
    max_retries: 10

  on_parse_error:
    action: "log"
    save_html: true  # ë””ë²„ê¹…ìš© HTML ì €ì¥
    continue: true
```

**ì‹¤ì „ ì˜ˆì‹œ:**
```
[ì˜¤ë¥˜ ë°œìƒ]
14:30:15 - Page 5 loading...
14:30:45 - ERROR: Timeout (30s exceeded)
14:30:45 - Retry 1/3 in 1 second...
14:30:46 - Page 5 loading (retry)...
14:31:16 - ERROR: Timeout again
14:31:16 - Retry 2/3 in 2 seconds...
14:31:18 - Page 5 loading (retry)...
14:31:30 - SUCCESS: Page 5 loaded (23 items)
```

#### 6ë‹¨ê³„: ëŒ€ê·œëª¨ ìŠ¤í¬ë˜í•‘ ìµœì í™”

**ë³‘ë ¬ ì²˜ë¦¬:**
```yaml
performance:
  concurrent_jobs: 5  # ë™ì‹œì— 5ê°œ ì‘ì—…

  worker_pool:
    size: 10  # ì›Œì»¤ 10ê°œ
    distribution: "round-robin"

  caching:
    enabled: true
    ttl: 3600  # 1ì‹œê°„ ìºì‹œ
    storage: "redis"

  database:
    batch_size: 100  # 100ê°œì”© ë¬¶ì–´ì„œ ì €ì¥
    connection_pool: 20
```

**ëª¨ë‹ˆí„°ë§:**
```python
# ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
metrics = {
    "pages_scraped": 1250,
    "items_extracted": 25000,
    "errors": 15,
    "retry_count": 8,
    "avg_page_time": 2.3,  # ì´ˆ
    "cache_hit_rate": 0.78,  # 78%
    "memory_usage": "450MB",
    "cpu_usage": "35%"
}
```

#### 7ë‹¨ê³„: ë²•ì /ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
```yaml
legal_compliance:
  robots_txt:
    check: true
    respect: true
    user_agent: "ScraperBot/1.0 (Educational Purpose)"

  rate_limiting:
    enabled: true
    requests_per_second: 1
    respect_retry_after: true

  terms_of_service:
    acknowledged: true
    purpose: "Research and analysis"

  data_privacy:
    pii_filtering: true  # ê°œì¸ì •ë³´ í•„í„°ë§
    gdpr_compliant: true

  attribution:
    source_url: true
    scrape_date: true
```

**robots.txt í™•ì¸:**
```
https://website.com/robots.txt

User-agent: *
Disallow: /admin/        â† ê¸ˆì§€ ì˜ì—­
Disallow: /private/
Allow: /products/        â† í—ˆìš© ì˜ì—­
Crawl-delay: 5           â† 5ì´ˆ ê°„ê²© ìš”ì²­

â†’ Scraperrê°€ ìë™ìœ¼ë¡œ ì¤€ìˆ˜!
```

#### 8ë‹¨ê³„: ì‹¤ì „ í”„ë¡œì íŠ¸ êµ¬ì¡°

**ì „ë¬¸ê°€ ìˆ˜ì¤€ êµ¬ì„±:**
```
scraperr-project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml              # ê³µí†µ ì„¤ì •
â”‚   â”œâ”€â”€ ecommerce.yaml         # ì´ì»¤ë¨¸ìŠ¤ íŠ¹í™”
â”‚   â”œâ”€â”€ news.yaml              # ë‰´ìŠ¤ ì‚¬ì´íŠ¸
â”‚   â””â”€â”€ social.yaml            # ì†Œì…œ ë¯¸ë””ì–´
â”‚
â”œâ”€â”€ selectors/
â”‚   â”œâ”€â”€ amazon.xpath           # ì•„ë§ˆì¡´ XPath
â”‚   â”œâ”€â”€ ebay.xpath             # eBay XPath
â”‚   â””â”€â”€ common.xpath           # ê³µí†µ ì„ íƒì
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py          # ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ postprocess.py         # í›„ì²˜ë¦¬
â”‚   â””â”€â”€ analyze.py             # ë°ì´í„° ë¶„ì„
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/             # ê°€ê³µ ë°ì´í„°
â”‚   â””â”€â”€ exports/               # ë‚´ë³´ë‚´ê¸°
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ scraperr.log
â”‚   â”œâ”€â”€ errors.log
â”‚   â””â”€â”€ performance.log
â”‚
â””â”€â”€ docker-compose.yml         # ì „ì²´ í™˜ê²½
```

**ìë™í™” íŒŒì´í”„ë¼ì¸:**
```bash
#!/bin/bash
# run-pipeline.sh

# 1. í™˜ê²½ ì‹œì‘
docker-compose up -d

# 2. ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
scraperr run --config configs/ecommerce.yaml

# 3. ë°ì´í„° ê²€ì¦
python scripts/validate.py

# 4. ì „ì²˜ë¦¬
python scripts/preprocess.py

# 5. ë¶„ì„
python scripts/analyze.py

# 6. ë¦¬í¬íŠ¸ ìƒì„±
python scripts/generate_report.py

# 7. ì•Œë¦¼ ì „ì†¡
python scripts/send_notification.py

# 8. ì •ë¦¬
docker-compose down
```

**ğŸ¤” ìƒê°í•´ë³´ê¸°:**
- ì •ì  ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì  ì›¹ì‚¬ì´íŠ¸ì˜ ì°¨ì´ëŠ”?
- API ì¸í„°ì…‰íŠ¸ê°€ HTML íŒŒì‹±ë³´ë‹¤ ë‚˜ì€ ì´ìœ ëŠ”?
- ì–´ë–¤ ìƒí™©ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ê°€ í•„ìš”í•œê°€ìš”?
- ë²•ì  ë¬¸ì œë¥¼ í”¼í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?

**âœ… ê³ ê¸‰ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ë™ì  ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
- [ ] ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
- [ ] API ì¸í„°ì…‰íŠ¸ ê²½í—˜
- [ ] ë¡œê·¸ì¸ ìë™í™”
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ êµ¬í˜„
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
- [ ] ë²•ì  ì»´í”Œë¼ì´ì–¸ìŠ¤ í™•ì¸
- [ ] ì „ë¬¸ê°€ ìˆ˜ì¤€ í”„ë¡œì íŠ¸ êµ¬ì¡°

---

## ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì´ì»¤ë¨¸ìŠ¤ ê°€ê²© ëª¨ë‹ˆí„°ë§ - ë§ˆì¼€í„° ìˆ˜ì—°

**ë°°ê²½:**
- ì „ìì œí’ˆ ìœ í†µ íšŒì‚¬ ë§ˆì¼€íŒ…íŒ€
- ê²½ìŸì‚¬ 50ê°œ ì‹¤ì‹œê°„ ê°€ê²© ì¶”ì 
- ë§¤ì¼ ê°€ê²© ë³€ë™ ë³´ê³ ì„œ ì‘ì„±

**Before Scraperr:**
```
ì—…ë¬´ í”„ë¡œì„¸ìŠ¤:
1. ì•„ì¹¨ 9ì‹œ: ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ 50ê°œ ë°©ë¬¸
2. ì£¼ìš” ì œí’ˆ 100ê°œ ê°€ê²© ìˆ˜ë™ í™•ì¸
3. Excel ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì…ë ¥
4. ì–´ì œ ê°€ê²©ê³¼ ë¹„êµ
5. ë³€ë™ ì‚¬í•­ ë³´ê³ ì„œ ì‘ì„±
â†’ ì†Œìš” ì‹œê°„: 3ì‹œê°„
â†’ ì‹¤ìˆ˜: ë¹ˆë²ˆí•¨
â†’ ìŠ¤íŠ¸ë ˆìŠ¤: ê·¹ì‹¬í•¨
```

**After Scraperr:**
```yaml
# price-monitoring.yaml
name: "Daily Price Monitor"
schedule: "0 9 * * *"  # ë§¤ì¼ ì•„ì¹¨ 9ì‹œ

targets:
  - name: "Competitor A"
    url: "https://competitor-a.com/electronics"
    selectors:
      product: "//div[@class='product']"
      name: ".//h3/text()"
      price: ".//span[@class='price']/text()"

  # ... ê²½ìŸì‚¬ 50ê°œ ...

actions:
  - compare_with_yesterday
  - highlight_changes
  - generate_report
  - send_email

notifications:
  email:
    to: ["team@company.com"]
    subject: "ì¼ì¼ ê°€ê²© ë³€ë™ ë¦¬í¬íŠ¸"
    template: "price_report.html"
```

**ê²°ê³¼:**
```
ìë™ ì‹¤í–‰:
- 09:00: ìŠ¤í¬ë˜í•‘ ì‹œì‘
- 09:15: ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ
- 09:20: ë³´ê³ ì„œ ìë™ ìƒì„±
- 09:21: ì´ë©”ì¼ ë°œì†¡

ìˆ˜ì—°ì˜ ìƒˆë¡œìš´ ì•„ì¹¨:
- 09:00: ì¶œê·¼
- 09:30: ë³´ê³ ì„œ í™•ì¸ (ì´ë¯¸ ì™„ì„±ë¨!)
- 10:00: ì „ëµ íšŒì˜ (3ì‹œê°„ ì ˆì•½!)

íš¨ê³¼:
â° 3ì‹œê°„ â†’ 15ë¶„
ğŸ“Š ì •í™•ë„ 99.9%
ğŸ“§ ìë™ ì•Œë¦¼
ğŸ’° ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ ê°€ê²© ì „ëµ
```

**ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ:**
```
[ì‹¤ì‹œê°„ ê°€ê²© ëŒ€ì‹œë³´ë“œ]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê°€ê²© ë³€ë™ TOP 10                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ iPhone 15: -50,000ì› (â†“4.2%)           â”‚
â”‚ Galaxy S24: +30,000ì› (â†‘2.8%)          â”‚
â”‚ MacBook: ë³€ë™ ì—†ìŒ                       â”‚
â”‚ ...                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì•Œë¦¼: Competitor Aê°€ 10ê°œ ì œí’ˆ í• ì¸!     â”‚
â”‚ ëŒ€ì‘ í•„ìš”: ê²½ìŸë ¥ í™•ë³´ ì „ëµ ìˆ˜ë¦½          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë¶€ë™ì‚° ë°ì´í„° ìˆ˜ì§‘ - ë°ì´í„° ë¶„ì„ê°€ ì¤€í˜¸

**ë°°ê²½:**
- ë¶€ë™ì‚° íˆ¬ì ë¶„ì„ ìŠ¤íƒ€íŠ¸ì—…
- ë§¤ë¬¼ ì •ë³´ ì‹¤ì‹œê°„ ìˆ˜ì§‘
- AI ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë°ì´í„°

**ìˆ˜ì§‘ ëŒ€ì´í„°:**
```
ë¶€ë™ì‚° í¬í„¸ 5ê°œ:
- ë„¤ì´ë²„ ë¶€ë™ì‚°
- ì§ë°©
- ë‹¤ë°©
- í˜¸ê°±ë…¸ë…¸
- í”¼í„°íŒ¬

ìˆ˜ì§‘ ì •ë³´:
- ì£¼ì†Œ
- ë©´ì 
- ê°€ê²© (ë§¤ë§¤/ì „ì„¸/ì›”ì„¸)
- ì¸µìˆ˜
- ë°©í–¥
- ì˜µì…˜ (ì£¼ì°¨, ì—˜ë¦¬ë² ì´í„°)
- ì‚¬ì§„
- ë“±ë¡ì¼
```

**Scraperr ì„¤ì •:**
```yaml
# real-estate-scraper.yaml
targets:
  naver:
    url: "https://land.naver.com/"
    search:
      region: "ê°•ë‚¨êµ¬"
      type: "apartment"
    selectors:
      address: "//div[@class='address']/text()"
      price: "//span[@class='price']/text()"
      area: "//span[@class='area']/text()"
      floor: "//span[@class='floor']/text()"
      images: "//img[@class='photo']/@src"

  jikbang:
    url: "https://www.jikbang.com/"
    api_mode: true  # API ì‚¬ìš©
    endpoint: "/api/apartments"

  # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë“¤...

data_pipeline:
  - deduplicate: true  # ì¤‘ë³µ ì œê±°
  - geocoding: true    # ì¢Œí‘œ ë³€í™˜
  - price_normalize: true  # ê°€ê²© ì •ê·œí™”
  - image_download: true

export:
  database: "postgresql://localhost/realestate"
  table: "listings"
  update_strategy: "upsert"  # ì—…ë°ì´íŠ¸ ë˜ëŠ” ì‚½ì…
```

**ë°ì´í„° í™œìš©:**
```python
# ë¶„ì„ ì˜ˆì‹œ
import pandas as pd

# Scraperrê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ë¡œë“œ
df = pd.read_sql('listings', con=engine)

# 1. ì§€ì—­ë³„ í‰ê·  ê°€ê²©
df.groupby('region')['price'].mean()

# 2. ê°€ê²© ì¶”ì„¸ ë¶„ì„
df.groupby('date')['price'].mean().plot()

# 3. AI ëª¨ë¸ í•™ìŠµ
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(df[features], df['price'])

# 4. ê°€ê²© ì˜ˆì¸¡
predicted_price = model.predict(new_listing)
```

**ì‹¤ì œ íš¨ê³¼:**
```
ë°ì´í„° ê·œëª¨:
- ì¼ì¼ ì‹ ê·œ ë§¤ë¬¼: 5,000ê±´
- ëˆ„ì  ë°ì´í„°: 500ë§Œ ê±´
- ì´ë¯¸ì§€: 2,000ë§Œ ì¥

AI ëª¨ë¸ ì„±ëŠ¥:
- ê°€ê²© ì˜ˆì¸¡ ì •í™•ë„: 92%
- íˆ¬ì ìˆ˜ìµë¥ : +15%
- ì‹œì¥ íŠ¸ë Œë“œ ì˜ˆì¸¡: 85%

ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:
- ë°ì´í„° ìˆ˜ì§‘ ìë™í™”
- ì‹¤ì‹œê°„ ì‹œì¥ ë¶„ì„
- íˆ¬ìì ì˜ì‚¬ê²°ì • ì§€ì›
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ - ë¯¸ë””ì–´ ë¦¬ì„œì²˜ ì§€ì˜

**ë°°ê²½:**
- ì—¬ë¡  ì¡°ì‚¬ ê¸°ê´€
- ì¼ì¼ ë‰´ìŠ¤ 2,000ê°œ ë¶„ì„
- í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„

**ìˆ˜ì§‘ ì „ëµ:**
```yaml
# news-aggregator.yaml
name: "Daily News Aggregator"

sources:
  - name: "Naver News"
    url: "https://news.naver.com/"
    categories: ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”"]

  - name: "Daum News"
    url: "https://news.daum.net/"

  # ... 10ê°œ ì–¸ë¡ ì‚¬ ...

selectors:
  article:
    title: "//h2[@class='headline']/text()"
    content: "//div[@class='article-body']//text()"
    author: "//span[@class='author']/text()"
    date: "//time/@datetime"
    category: "//span[@class='category']/text()"

filtering:
  keywords: ["AI", "ê¸°ìˆ ", "ìŠ¤íƒ€íŠ¸ì—…"]
  date_range: "today"
  min_length: 500  # ìµœì†Œ 500ì

nlp_processing:
  sentiment_analysis: true
  keyword_extraction: true
  entity_recognition: true
  summarization: true
```

**ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:**
```python
# NLP ë¶„ì„
from transformers import pipeline

# ê°ì • ë¶„ì„
sentiment = pipeline("sentiment-analysis")
result = sentiment("ì´ ê¸°ì‚¬ëŠ” ê¸ì •ì ì¸ ë‚´ìš©ì…ë‹ˆë‹¤.")

# í‚¤ì›Œë“œ ì¶”ì¶œ
from keybert import KeyBERT
kw_model = KeyBERT()
keywords = kw_model.extract_keywords(article_text, top_n=10)

# ìš”ì•½
summarizer = pipeline("summarization")
summary = summarizer(article_text, max_length=150)

# ê²°ê³¼ ì €ì¥
save_to_database({
    'title': title,
    'content': content,
    'summary': summary,
    'sentiment': result,
    'keywords': keywords
})
```

**ëŒ€ì‹œë³´ë“œ ì‹œê°í™”:**
```
[íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì˜¤ëŠ˜ì˜ í•« í‚¤ì›Œë“œ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. AI          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (1,234íšŒ)  â”‚
â”‚ 2. ChatGPT     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   (987íšŒ)    â”‚
â”‚ 3. ë°˜ë„ì²´      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     (756íšŒ)    â”‚
â”‚ 4. ìŠ¤íƒ€íŠ¸ì—…    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       (543íšŒ)    â”‚
â”‚ 5. íˆ¬ì        â–ˆâ–ˆâ–ˆâ–ˆ         (421íšŒ)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ê°ì • ë¶„ì„                                â”‚
â”‚ ê¸ì •: 60% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚
â”‚ ì¤‘ë¦½: 30% â–ˆâ–ˆâ–ˆâ–ˆ                          â”‚
â”‚ ë¶€ì •: 10% â–ˆâ–ˆ                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 4: í•™ìˆ  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ - ì—°êµ¬ì› ë¯¼ì„

**ë°°ê²½:**
- ëŒ€í•™ì› ë°•ì‚¬ê³¼ì •
- ë¬¸í—Œ ë¦¬ë·°ë¥¼ ìœ„í•œ ë…¼ë¬¸ ìˆ˜ì§‘
- ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„

**ìˆ˜ì§‘ ëŒ€ìƒ:**
```
í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤:
- Google Scholar
- arXiv
- PubMed
- IEEE Xplore
- ACM Digital Library

ë©”íƒ€ë°ì´í„°:
- ì œëª©
- ì €ì
- ì´ˆë¡
- í‚¤ì›Œë“œ
- ì¸ìš© ìˆ˜
- ì¶œíŒ ì—°ë„
- í•™ìˆ ì§€
- DOI
```

**Scraperr ì„¤ì •:**
```yaml
# academic-scraper.yaml
name: "Research Paper Aggregator"

search_query: "machine learning healthcare"
date_range: "2020-2024"

sources:
  google_scholar:
    url: "https://scholar.google.com/scholar"
    params:
      q: "${SEARCH_QUERY}"
      hl: "en"
      as_ylo: "2020"
      as_yhi: "2024"

    selectors:
      paper: "//div[@class='gs_ri']"
      title: ".//h3/a/text()"
      authors: ".//div[@class='gs_a']/text()"
      abstract: ".//div[@class='gs_rs']/text()"
      citations: ".//div[@class='gs_fl']//a[contains(text(), 'Cited')]/text()"
      pdf: ".//div[@class='gs_or_ggsm']//a/@href"

  arxiv:
    api: true
    endpoint: "http://export.arxiv.org/api/query"
    params:
      search_query: "${SEARCH_QUERY}"
      max_results: 1000

post_processing:
  - parse_citations
  - download_pdfs
  - extract_references
  - build_citation_network
```

**ë„¤íŠ¸ì›Œí¬ ë¶„ì„:**
```python
import networkx as nx
import matplotlib.pyplot as plt

# ì¸ìš© ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
G = nx.DiGraph()

for paper in papers:
    G.add_node(paper['id'], title=paper['title'])
    for ref in paper['references']:
        G.add_edge(paper['id'], ref)

# ì¤‘ìš” ë…¼ë¬¸ ì°¾ê¸° (PageRank)
pagerank = nx.pagerank(G)
top_papers = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]

# ì‹œê°í™”
nx.draw(G, with_labels=True, node_color='lightblue')
plt.show()
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 5: ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„ - ë§ˆì¼€í„° í˜œì§„

**ë°°ê²½:**
- ë¸Œëœë“œ ë§¤ë‹ˆì§€ë¨¼íŠ¸ íšŒì‚¬
- ì‹¤ì‹œê°„ ë¸Œëœë“œ ëª¨ë‹ˆí„°ë§
- ê²½ìŸì‚¬ ì†Œì…œ ë¯¸ë””ì–´ ë¶„ì„

**ìˆ˜ì§‘ ë°ì´í„°:**
```
í”Œë«í¼:
- Twitter (X)
- Instagram (ê³µê°œ ê³„ì •)
- Reddit
- Product Hunt
- Hacker News

ì§€í‘œ:
- ê²Œì‹œë¬¼ ìˆ˜
- ì¢‹ì•„ìš”/ëŒ“ê¸€/ê³µìœ 
- í•´ì‹œíƒœê·¸ íŠ¸ë Œë“œ
- ê°ì • ë¶„ì„
- ì¸í”Œë£¨ì–¸ì„œ ë©˜ì…˜
```

**Scraperr ì„¤ì •:**
```yaml
# social-media-monitor.yaml
brand: "OurBrand"
competitors: ["CompA", "CompB", "CompC"]

twitter:
  keywords: ["#OurBrand", "@OurBrand"]
  track_mentions: true
  sentiment: true

  selectors:
    tweet: "//article[@data-testid='tweet']"
    text: ".//div[@data-testid='tweetText']/text()"
    likes: ".//button[@data-testid='like']/@aria-label"
    retweets: ".//button[@data-testid='retweet']/@aria-label"

instagram:
  accounts: ["@ourbrand_official"]
  hashtags: ["#OurBrand"]

  metrics:
    - followers_count
    - engagement_rate
    - post_frequency

alerts:
  - condition: "sentiment < -0.5"
    action: "send_urgent_email"
    message: "ë¶€ì •ì  ë©˜ì…˜ ê¸‰ì¦!"

  - condition: "mentions > 1000"
    action: "send_notification"
    message: "ë²„ì¦ˆ ë°œìƒ!"
```

**ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ:**
```
[ë¸Œëœë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì˜¤ëŠ˜ì˜ ë©˜ì…˜: 1,234 (+45% vs ì–´ì œ)        â”‚
â”‚ ê°ì • ì§€ìˆ˜: 0.65 (ê¸ì •ì )                 â”‚
â”‚ Top í•´ì‹œíƒœê·¸: #OurBrand, #Innovation     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”¥ ë²„ì¦ˆ ì•Œë¦¼:                            â”‚
â”‚ - ì¸í”Œë£¨ì–¸ì„œ Aë‹˜ì´ ì œí’ˆ ë¦¬ë·° (10K ì¢‹ì•„ìš”)â”‚
â”‚ - Reddit ì¸ê¸° ê²Œì‹œë¬¼ 1ìœ„!                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âš ï¸ ì£¼ì˜ í•„ìš”:                            â”‚
â”‚ - ê³ ê° ë¶ˆë§Œ íŠ¸ìœ— ì¦ê°€ (ë°°ì†¡ ì§€ì—° ê´€ë ¨)   â”‚
â”‚ - ê²½ìŸì‚¬ B ì‹ ì œí’ˆ ì¶œì‹œ ë°˜ì‘ ê¸ì •ì         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ìë™ ëŒ€ì‘:**
```python
# ìë™ ëŒ€ì‘ ì‹œìŠ¤í…œ
if negative_sentiment > 0.7:
    alert_customer_service()
    prepare_response_template()
    track_issue_resolution()

if viral_post_detected:
    notify_marketing_team()
    prepare_engagement_strategy()
    amplify_positive_content()
```

---

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ì±…

#### ë¬¸ì œ 1: "XPathê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”"

**ì¦ìƒ:**
```
XPath: //div[@class='price']/text()
ê²°ê³¼: ë°ì´í„° ì—†ìŒ (ë¹ˆ ë°°ì—´)
```

**ì›ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
```
â–¡ ì›¹í˜ì´ì§€ê°€ JavaScriptë¡œ ë Œë”ë§ë˜ë‚˜ìš”?
â–¡ í´ë˜ìŠ¤ ì´ë¦„ì´ ì •í™•í•œê°€ìš”? (ê³µë°±, ëŒ€ì†Œë¬¸ì)
â–¡ ìš”ì†Œê°€ iframe ì•ˆì— ìˆë‚˜ìš”?
â–¡ ë™ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ìš”ì†Œì¸ê°€ìš”?
```

**í•´ê²°ì±… 1: ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ë¡œ í™•ì¸**
```
1. ì›¹í˜ì´ì§€ì—ì„œ F12 (ê°œë°œì ë„êµ¬)
2. Elements íƒ­
3. Ctrl+F (ê²€ìƒ‰)
4. XPath ì…ë ¥: //div[@class='price']
5. ê²°ê³¼ í™•ì¸:
   - 0 results â†’ XPath ì˜ëª»ë¨
   - 1 of 5 â†’ ì—¬ëŸ¬ ê°œ ìˆìŒ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
```

**í•´ê²°ì±… 2: CSS Selector ì‹œë„**
```xpath
# XPath (ì‘ë™ ì•ˆ í•¨)
//div[@class='price']/text()

# CSS Selector (ì‹œë„)
div.price

# Scraperr ì„¤ì •
selector_type: "css"
selector: "div.price"
```

**í•´ê²°ì±… 3: ë¸Œë¼ìš°ì € ëª¨ë“œ ì‚¬ìš©**
```yaml
# JavaScript ë Œë”ë§ í•„ìš”í•œ ê²½ìš°
scraping_mode: "browser"
wait_for: "//div[@class='price']"
timeout: 10000
```

---

#### ë¬¸ì œ 2: "ìŠ¤í¬ë˜í•‘ì´ ë„ˆë¬´ ëŠë ¤ìš”"

**ì¦ìƒ:**
```
100ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
ì˜ˆìƒ ì‹œê°„: 10ë¶„
ì‹¤ì œ ì‹œê°„: 2ì‹œê°„ ğŸ˜±
```

**ì›ì¸:**
```
1. í˜ì´ì§€ ë¡œë”© ì‹œê°„ì´ ê¹€
2. ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ì§€, CSS)
3. JavaScript ì‹¤í–‰ ëŒ€ê¸°
4. ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‹¤í–‰
```

**í•´ê²°ì±… 1: ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨**
```yaml
browser_options:
  block_resources:
    - "image"
    - "stylesheet"
    - "font"
    - "media"

# ì†ë„ í–¥ìƒ: 60% â†’ 80%
```

**í•´ê²°ì±… 2: ë³‘ë ¬ ì²˜ë¦¬**
```yaml
performance:
  concurrent_workers: 5  # 5ê°œ ë™ì‹œ ì‹¤í–‰

# ì‹œê°„ ë‹¨ì¶•: 2ì‹œê°„ â†’ 24ë¶„
```

**í•´ê²°ì±… 3: ìºì‹±**
```yaml
caching:
  enabled: true
  ttl: 3600  # 1ì‹œê°„

# ë°˜ë³µ ì‹¤í–‰ ì‹œ ì†ë„: 10ë°° í–¥ìƒ
```

**í•´ê²°ì±… 4: HTTP ëª¨ë“œë¡œ ì „í™˜**
```yaml
# ê°€ëŠ¥í•˜ë©´ ë¸Œë¼ìš°ì € ëŒ€ì‹  HTTP ì‚¬ìš©
scraping_mode: "http"  # browser â†’ http

# ì†ë„ í–¥ìƒ: 10ë°°
```

---

#### ë¬¸ì œ 3: "403 Forbidden ì˜¤ë¥˜"

**ì¦ìƒ:**
```
HTTP Error 403: Forbidden
Access Denied
```

**ì›ì¸:**
```
ì›¹ì‚¬ì´íŠ¸ê°€ ë´‡ìœ¼ë¡œ ì¸ì‹:
- User-Agentê°€ Python
- ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­
- ì¿ í‚¤/ì„¸ì…˜ ì—†ìŒ
- IP ë¸”ë™ë¦¬ìŠ¤íŠ¸
```

**í•´ê²°ì±… 1: User-Agent ì„¤ì •**
```yaml
headers:
  User-Agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  Accept: "text/html,application/xhtml+xml"
  Accept-Language: "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
```

**í•´ê²°ì±… 2: ì†ë„ ì œí•œ**
```yaml
rate_limit:
  requests_per_second: 1  # ì´ˆë‹¹ 1ê°œ
  delay_between_pages: 3   # 3ì´ˆ ëŒ€ê¸°
```

**í•´ê²°ì±… 3: ì„¸ì…˜ ìœ ì§€**
```yaml
session:
  enabled: true
  cookies_file: "./cookies.json"

# ì²« ë°©ë¬¸ í›„ ì¿ í‚¤ ì €ì¥
# ë‹¤ìŒ ë°©ë¬¸ ì‹œ ì¬ì‚¬ìš©
```

**í•´ê²°ì±… 4: í”„ë¡ì‹œ ì‚¬ìš©**
```yaml
proxy:
  enabled: true
  type: "rotating"
  list:
    - "http://proxy1:8080"
    - "http://proxy2:8080"

# IP ë¡œí…Œì´ì…˜ìœ¼ë¡œ ì°¨ë‹¨ ìš°íšŒ
```

---

#### ë¬¸ì œ 4: "ë°ì´í„°ê°€ ì¤‘ë³µë¼ìš”"

**ì¦ìƒ:**
```
ê°™ì€ ì œí’ˆì´ ì—¬ëŸ¬ ë²ˆ ì €ì¥ë¨:
- iPhone 15 (5íšŒ)
- Galaxy S24 (3íšŒ)
```

**ì›ì¸:**
```
1. í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë³µ
2. ê³ ìœ  í‚¤ ì—†ìŒ
3. ì¬ì‹¤í–‰ ì‹œ ì¤‘ë³µ ì €ì¥
```

**í•´ê²°ì±… 1: ì¤‘ë³µ ì œê±° ì„¤ì •**
```yaml
data_processing:
  deduplicate: true
  unique_key: "product_id"  # ë˜ëŠ” "url"
```

**í•´ê²°ì±… 2: ë°ì´í„°ë² ì´ìŠ¤ UPSERT**
```yaml
export:
  database: "postgresql://..."
  table: "products"
  strategy: "upsert"  # ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
  conflict_column: "product_id"
```

**í•´ê²°ì±… 3: ìŠ¤í¬ë˜í•‘ ì „ í™•ì¸**
```python
# ì»¤ìŠ¤í…€ ìŠ¤í¬ë¦½íŠ¸
def before_scrape(url):
    # ì´ë¯¸ ìŠ¤í¬ë˜í•‘í–ˆëŠ”ì§€ í™•ì¸
    if url in already_scraped:
        return "skip"
    return "proceed"
```

---

#### ë¬¸ì œ 5: "MongoDB ì—°ê²° ì˜¤ë¥˜"

**ì¦ìƒ:**
```
MongoDBConnectionError: Connection refused
Cannot connect to MongoDB at localhost:27017
```

**ì›ì¸:**
```
1. MongoDB ì„œë²„ ì‹¤í–‰ ì•ˆ ë¨
2. í¬íŠ¸ ì¶©ëŒ
3. ì¸ì¦ ì‹¤íŒ¨
4. ë°©í™”ë²½ ì°¨ë‹¨
```

**í•´ê²°ì±… 1: MongoDB ìƒíƒœ í™•ì¸**
```bash
# Docker ì»¨í…Œì´ë„ˆ í™•ì¸
docker ps

# ì¶œë ¥ì— mongodb ìˆì–´ì•¼ í•¨
# CONTAINER ID   IMAGE       STATUS
# abc123         mongo:latest   Up 5 minutes

# ì—†ìœ¼ë©´ ì‹œì‘
docker-compose up -d mongodb
```

**í•´ê²°ì±… 2: ì—°ê²° ì •ë³´ í™•ì¸**
```yaml
# .env íŒŒì¼
MONGODB_URI=mongodb://localhost:27017/scraperr
MONGODB_USER=admin
MONGODB_PASSWORD=password123

# Scraperr ì„¤ì •
database:
  type: "mongodb"
  uri: "${MONGODB_URI}"
  username: "${MONGODB_USER}"
  password: "${MONGODB_PASSWORD}"
```

**í•´ê²°ì±… 3: ë°©í™”ë²½ í™•ì¸**
```bash
# Mac
sudo lsof -i :27017

# ì•„ë¬´ê²ƒë„ ì•ˆ ë‚˜ì˜¤ë©´ í¬íŠ¸ ì—´ê¸°
sudo ufw allow 27017
```

---

#### ë¬¸ì œ 6: "ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜"

**ì¦ìƒ:**
```
MemoryError: Unable to allocate array
Process killed (OOM)
```

**ì›ì¸:**
```
ëŒ€ëŸ‰ ë°ì´í„° í•œ ë²ˆì— ë¡œë”©:
- 10,000ê°œ ì´ë¯¸ì§€ ë™ì‹œ ë‹¤ìš´ë¡œë“œ
- ê±°ëŒ€í•œ HTML íŒŒì‹±
- ë©”ëª¨ë¦¬ì— ëª¨ë“  ê²°ê³¼ ì €ì¥
```

**í•´ê²°ì±… 1: ë°°ì¹˜ ì²˜ë¦¬**
```yaml
data_processing:
  batch_size: 100  # 100ê°œì”© ì²˜ë¦¬

# 10,000ê°œë¥¼ 100ê°œì”© 100ë²ˆ ì²˜ë¦¬
```

**í•´ê²°ì±… 2: ìŠ¤íŠ¸ë¦¬ë°**
```python
# í•œ ë²ˆì— ë‹¤ ë¡œë“œ (ë‚˜ì¨)
all_data = load_all()  # MemoryError!

# ìŠ¤íŠ¸ë¦¬ë° (ì¢‹ìŒ)
for item in stream_data():
    process(item)
    save(item)
    # ë©”ëª¨ë¦¬ í•´ì œ
```

**í•´ê²°ì±… 3: ë¦¬ì†ŒìŠ¤ ì œí•œ**
```yaml
resource_limits:
  max_memory: "2GB"
  max_images_in_queue: 50
```

---

#### ë¬¸ì œ 7: "ì¸ì½”ë”© ì˜¤ë¥˜ (í•œê¸€ ê¹¨ì§)"

**ì¦ìƒ:**
```
ì œí’ˆëª…: "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ 15"
ì •ìƒ: "ì•„ì´í° 15"
```

**ì›ì¸:**
```
ì¸ì½”ë”© ë¶ˆì¼ì¹˜:
ì›¹í˜ì´ì§€: EUC-KR
Scraperr: UTF-8
```

**í•´ê²°ì±… 1: ì¸ì½”ë”© ëª…ì‹œ**
```yaml
encoding:
  input: "euc-kr"  # ì›¹í˜ì´ì§€ ì¸ì½”ë”©
  output: "utf-8"  # ì €ì¥ ì¸ì½”ë”©
```

**í•´ê²°ì±… 2: ìë™ ê°ì§€**
```yaml
encoding:
  auto_detect: true

# chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ìë™ ê°ì§€
```

**í•´ê²°ì±… 3: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •**
```sql
-- PostgreSQL
CREATE DATABASE scraperr
WITH ENCODING 'UTF8'
LC_COLLATE = 'ko_KR.UTF-8'
LC_CTYPE = 'ko_KR.UTF-8';

-- MongoDB
# ìë™ìœ¼ë¡œ UTF-8 ì‚¬ìš©
```

---

#### ë¬¸ì œ 8: "Docker ë¹Œë“œ ì‹¤íŒ¨"

**ì¦ìƒ:**
```bash
$ make up
Error: Cannot connect to Docker daemon
```

**ì›ì¸:**
```
1. Docker Desktop ì‹¤í–‰ ì•ˆ ë¨
2. Docker ê¶Œí•œ ì—†ìŒ
3. í¬íŠ¸ ì¶©ëŒ
```

**í•´ê²°ì±… 1: Docker ì‹œì‘**
```bash
# Mac
open -a Docker

# ë˜ëŠ” Docker Desktop ì•„ì´ì½˜ í´ë¦­

# í™•ì¸
docker ps
# ì •ìƒì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
```

**í•´ê²°ì±… 2: ê¶Œí•œ ì„¤ì • (Linux)**
```bash
# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER

# ë¡œê·¸ì•„ì›ƒ í›„ ì¬ë¡œê·¸ì¸
# ë˜ëŠ” ì„¸ì…˜ ê°±ì‹ 
newgrp docker

# í™•ì¸
docker ps
```

**í•´ê²°ì±… 3: í¬íŠ¸ í™•ì¸**
```bash
# 3000ë²ˆ í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
lsof -i :3000

# ì‚¬ìš© ì¤‘ì´ë©´ ì¢…ë£Œ
kill -9 <PID>

# ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
# docker-compose.yml ìˆ˜ì •
ports:
  - "3001:3000"  # 3001ë¡œ ë³€ê²½
```

---

## ê³ ê¸‰ í™œìš© íŒ¨í„´

### íŒ¨í„´ 1: ì¦ë¶„ ìŠ¤í¬ë˜í•‘ (Incremental Scraping)

**ê°œë…:** ì „ì²´ê°€ ì•„ë‹Œ ë³€ê²½ ì‚¬í•­ë§Œ ìˆ˜ì§‘

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
```
ì¼ì¼ ë‰´ìŠ¤ ì‚¬ì´íŠ¸:
- ì „ì²´ ìŠ¤í¬ë˜í•‘: 10,000ê°œ ê¸°ì‚¬ (2ì‹œê°„)
- ì¦ë¶„ ìŠ¤í¬ë˜í•‘: ì‹ ê·œ 200ê°œë§Œ (5ë¶„)
```

**êµ¬í˜„:**
```yaml
# incremental-scraper.yaml
scraping_strategy: "incremental"

checkpoint:
  enabled: true
  storage: "./checkpoints/last_scrape.json"

  criteria:
    - field: "publish_date"
      operator: ">"
      value: "${LAST_SCRAPE_TIME}"

  - field: "article_id"
      operator: "not in"
      value: "${SCRAPED_IDS}"

on_complete:
  - update_checkpoint
  - save_scrape_time
```

**Python êµ¬í˜„ ì˜ˆì‹œ:**
```python
import json
from datetime import datetime

# ë§ˆì§€ë§‰ ìŠ¤í¬ë˜í•‘ ì‹œê°„ ë¡œë“œ
try:
    with open('checkpoint.json', 'r') as f:
        checkpoint = json.load(f)
        last_scrape = checkpoint['last_scrape_time']
except:
    last_scrape = '2000-01-01'  # ì²˜ìŒ ì‹¤í–‰

# ì‹ ê·œ í•­ëª©ë§Œ í•„í„°ë§
new_items = []
for item in all_items:
    if item['date'] > last_scrape:
        new_items.append(item)

# ì²˜ë¦¬
process(new_items)

# ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
checkpoint = {
    'last_scrape_time': datetime.now().isoformat(),
    'items_scraped': len(new_items)
}
with open('checkpoint.json', 'w') as f:
    json.dump(checkpoint, f)
```

---

### íŒ¨í„´ 2: ë©€í‹°ì†ŒìŠ¤ ë°ì´í„° í†µí•©

**ê°œë…:** ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°

**ì‹œë‚˜ë¦¬ì˜¤:**
```
ì œí’ˆ ì •ë³´ í†µí•©:
- ì‚¬ì´íŠ¸ A: ê°€ê²©
- ì‚¬ì´íŠ¸ B: ë¦¬ë·°
- ì‚¬ì´íŠ¸ C: ì¬ê³ 
â†’ í†µí•© ë°ì´í„°ë² ì´ìŠ¤
```

**ì„¤ì •:**
```yaml
# multi-source-integration.yaml
sources:
  price_source:
    url: "https://store-a.com"
    selector: "//span[@class='price']"
    field: "price"

  review_source:
    url: "https://reviews-b.com"
    selector: "//div[@class='rating']"
    field: "rating"

  stock_source:
    url: "https://inventory-c.com"
    api: true
    endpoint: "/api/stock"
    field: "stock"

integration:
  key: "product_id"  # í†µí•© í‚¤
  strategy: "merge"

  output:
    database: "postgresql://..."
    table: "unified_products"
```

**ë°ì´í„° ë§¤í•‘:**
```python
# ë°ì´í„° ì •ê·œí™” ë° í†µí•©
def integrate_data():
    # ì†ŒìŠ¤ 1: ê°€ê²©
    prices = scrape_source('price_source')
    # [{'id': 'P001', 'price': 10000}, ...]

    # ì†ŒìŠ¤ 2: ë¦¬ë·°
    reviews = scrape_source('review_source')
    # [{'id': 'P001', 'rating': 4.5}, ...]

    # ì†ŒìŠ¤ 3: ì¬ê³ 
    stocks = scrape_source('stock_source')
    # [{'id': 'P001', 'stock': 50}, ...]

    # í†µí•©
    integrated = {}
    for price in prices:
        pid = price['id']
        integrated[pid] = {'price': price['price']}

    for review in reviews:
        pid = review['id']
        if pid in integrated:
            integrated[pid]['rating'] = review['rating']

    for stock in stocks:
        pid = stock['id']
        if pid in integrated:
            integrated[pid]['stock'] = stock['stock']

    return list(integrated.values())
```

---

### íŒ¨í„´ 3: ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤í¬ë˜í•‘

**ê°œë…:** íŠ¹ì • ì¡°ê±´ ë°œìƒ ì‹œ ìë™ ì‹¤í–‰

**íŠ¸ë¦¬ê±° ì˜ˆì‹œ:**
```
1. ê°€ê²© ë³€ë™ ê°ì§€
   â†’ ì¦‰ì‹œ ìŠ¤í¬ë˜í•‘
   â†’ ì•Œë¦¼ ë°œì†¡

2. ìƒˆ ì œí’ˆ ì¶œì‹œ
   â†’ ì •ë³´ ìˆ˜ì§‘
   â†’ ë°ì´í„°ë² ì´ìŠ¤ ì¶”ê°€

3. ì¬ê³  ë¶€ì¡±
   â†’ ê²½ìŸì‚¬ ì¬ê³  í™•ì¸
   â†’ ê°€ê²© ì¡°ì • ì œì•ˆ
```

**ì„¤ì •:**
```yaml
# event-driven-scraper.yaml
events:
  price_drop:
    monitor:
      url: "https://store.com/iphone"
      selector: "//span[@class='price']"
      interval: 3600  # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬

    trigger:
      condition: "price < 1000000"  # 100ë§Œì› ë¯¸ë§Œ
      action:
        - scrape_competitor_prices
        - send_notification
        - update_recommendation

  new_product:
    monitor:
      url: "https://store.com/new-arrivals"
      selector: "//div[@class='product-card']"
      interval: 7200

    trigger:
      condition: "count > previous_count"
      action:
        - scrape_product_details
        - analyze_market_position
        - generate_report
```

**Webhook í†µí•©:**
```python
from flask import Flask, request
app = Flask(__name__)

@app.route('/webhook/price-alert', methods=['POST'])
def price_alert():
    data = request.json
    product_id = data['product_id']

    # ì¦‰ì‹œ ìŠ¤í¬ë˜í•‘ ì‹œì‘
    scraperr.run_job(f'emergency-scrape-{product_id}')

    return {'status': 'triggered'}
```

---

### íŒ¨í„´ 4: AI ê¸°ë°˜ ë°ì´í„° ê²€ì¦

**ê°œë…:** ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ AIë¡œ ê²€ì¦

**ê²€ì¦ í•­ëª©:**
```
1. ê°€ê²© ì´ìƒì¹˜ íƒì§€
   - í‰ì†Œ 10ë§Œì› â†’ ê°‘ìê¸° 1ì–µì›?
   - ì˜¤ë¥˜ ê°€ëŠ¥ì„± ë†’ìŒ

2. ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸
   - ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€
   - ê¹¨ì§„ ì´ë¯¸ì§€

3. í…ìŠ¤íŠ¸ í’ˆì§ˆ
   - ì˜ë¯¸ ì—†ëŠ” ë¬¸ìì—´
   - HTML íƒœê·¸ í¬í•¨
```

**AI ê²€ì¦ íŒŒì´í”„ë¼ì¸:**
```python
from sklearn.ensemble import IsolationForest

# 1. ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ í•™ìŠµ
prices = [item['price'] for item in historical_data]
model = IsolationForest()
model.fit(prices)

# 2. ì‹ ê·œ ë°ì´í„° ê²€ì¦
for new_item in scraped_data:
    prediction = model.predict([[new_item['price']]])

    if prediction == -1:  # ì´ìƒì¹˜
        flag_for_review(new_item)
        notify_admin(f"ì´ìƒ ê°€ê²©: {new_item['price']}")

# 3. ì´ë¯¸ì§€ ê²€ì¦
from PIL import Image

for item in scraped_data:
    img = Image.open(item['image_path'])

    # í¬ê¸° í™•ì¸
    if img.width < 100 or img.height < 100:
        item['quality_score'] = 'low'

    # ì„ ëª…ë„ í™•ì¸
    if is_blurry(img):
        item['quality_score'] = 'blurry'

# 4. í…ìŠ¤íŠ¸ ê²€ì¦
import re

for item in scraped_data:
    text = item['description']

    # HTML íƒœê·¸ í¬í•¨ ì—¬ë¶€
    if re.search(r'<[^>]+>', text):
        text = strip_html(text)

    # ìµœì†Œ ê¸¸ì´
    if len(text) < 50:
        item['quality_score'] = 'too_short'
```

---

### íŒ¨í„´ 5: ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì•„í‚¤í…ì²˜

**ê°œë…:** ì—¬ëŸ¬ ì„œë²„ì—ì„œ ë™ì‹œ ìŠ¤í¬ë˜í•‘

**ì‹œë‚˜ë¦¬ì˜¤:**
```
ëŒ€ê·œëª¨ ìŠ¤í¬ë˜í•‘:
- ëª©í‘œ: 100ë§Œ ê°œ í˜ì´ì§€
- ë‹¨ì¼ ì„œë²„: 10ì¼ ì†Œìš”
- 10ê°œ ì„œë²„: 1ì¼ ì™„ë£Œ
```

**ì•„í‚¤í…ì²˜:**
```
[Master Server]
     â†“ ì‘ì—… ë¶„ë°°
[Worker 1] [Worker 2] [Worker 3] ... [Worker 10]
     â†“ ê²°ê³¼ ìˆ˜ì§‘
[Central Database]
```

**êµ¬í˜„:**
```yaml
# master-config.yaml
coordinator:
  mode: "master"
  workers: 10

  task_distribution:
    strategy: "dynamic"  # ë™ì  ë¡œë“œ ë°¸ëŸ°ì‹±
    queue: "redis://localhost:6379"

# worker-config.yaml
coordinator:
  mode: "worker"
  master: "http://master-server:8000"

  resources:
    cpu: 4
    memory: "8GB"
    concurrent_jobs: 5
```

**Redis í:**
```python
import redis
from rq import Queue

# Master: ì‘ì—… ì¶”ê°€
queue = Queue(connection=redis.Redis())

for url in urls:
    queue.enqueue(scrape_page, url)

# Worker: ì‘ì—… ì²˜ë¦¬
from rq import Worker

worker = Worker(['default'], connection=redis.Redis())
worker.work()
```

---

### íŒ¨í„´ 6: ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ ìŠ¤íŠ¸ë¦¼

**ê°œë…:** ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì²˜ë¦¬

**ì‚¬ìš© ì‚¬ë¡€:**
```
ì‹¤ì‹œê°„ ì£¼ì‹ ê°€ê²©:
- 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
- ì¦‰ì‹œ ë¶„ì„ í•„ìš”
- ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ í‘œì‹œ
```

**Kafka í†µí•©:**
```yaml
# streaming-scraper.yaml
output:
  type: "kafka"
  topic: "stock-prices"
  bootstrap_servers: "localhost:9092"

scraping:
  mode: "continuous"
  interval: 1000  # 1ì´ˆ
  targets:
    - url: "https://finance.yahoo.com/quote/AAPL"
      selector: "//span[@data-symbol='AAPL']"
```

**Kafka Consumer:**
```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'stock-prices',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    price_data = message.value

    # ì‹¤ì‹œê°„ ì²˜ë¦¬
    analyze(price_data)
    update_dashboard(price_data)
    check_alerts(price_data)
```

---

## ìš©ì–´ ì‚¬ì „

### A

**API (Application Programming Interface)**
```
ì‰¬ìš´ ì„¤ëª…: í”„ë¡œê·¸ë¨ë¼ë¦¬ ëŒ€í™”í•˜ëŠ” ë°©ë²•
ì¼ìƒ ë¹„ìœ : ë ˆìŠ¤í† ë‘ ë©”ë‰´íŒ

ì„¤ëª…:
- ì›¹ì‚¬ì´íŠ¸ê°€ ë°ì´í„°ë¥¼ ì£¼ëŠ” ê³µì‹ì ì¸ ë°©ë²•
- HTML ìŠ¤í¬ë˜í•‘ë³´ë‹¤ ì•ˆì •ì 
- êµ¬ì¡°í™”ëœ ë°ì´í„° (JSON, XML)

ì˜ˆì‹œ:
# API ìš”ì²­
GET https://api.store.com/products
â†’ JSON ì‘ë‹µ:
{
  "products": [
    {"id": 1, "name": "iPhone", "price": 1000000}
  ]
}

ì¥ì :
âœ… ë¹ ë¦„
âœ… ê¹”ë”í•œ ë°ì´í„°
âœ… ê³µì‹ ì§€ì›

ë‹¨ì :
âŒ API ì—†ëŠ” ì‚¬ì´íŠ¸ ë§ìŒ
âŒ ìš”ì²­ ì œí•œ ìˆì„ ìˆ˜ ìˆìŒ
```

---

### B

**Bot (ë´‡)**
```
ì‰¬ìš´ ì„¤ëª…: ìë™ìœ¼ë¡œ ì‘ë™í•˜ëŠ” í”„ë¡œê·¸ë¨
ì¼ìƒ ë¹„ìœ : ë¡œë´‡ ì²­ì†Œê¸°

ì¢…ë¥˜:
1. ì¢‹ì€ ë´‡:
   - ê²€ìƒ‰ì—”ì§„ í¬ë¡¤ëŸ¬ (Google Bot)
   - ê°€ê²© ëª¨ë‹ˆí„°ë§
   - ë°ì´í„° ë¶„ì„

2. ë‚˜ìœ ë´‡:
   - ìŠ¤íŒ¸ ë´‡
   - DDoS ê³µê²©
   - í‹°ì¼“ ë§¤í¬ë¡œ

ScraperrëŠ”:
- ì¢‹ì€ ë´‡
- í•©ë²•ì  ë°ì´í„° ìˆ˜ì§‘
- ì›¹ì‚¬ì´íŠ¸ ê·œì¹™ ì¤€ìˆ˜
```

---

### C

**Crawling (í¬ë¡¤ë§)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ì‚¬ì´íŠ¸ë¥¼ ëŒì•„ë‹¤ë‹ˆë©° ë§í¬ ë”°ë¼ê°€ê¸°
ì¼ìƒ ë¹„ìœ : ë„ì„œê´€ì—ì„œ ì±… ì°¾ê¸°

vs Scraping:
Crawling = ì—¬í–‰ (ì‚¬ì´íŠ¸ íƒìƒ‰)
Scraping = ìˆ˜ì§‘ (ë°ì´í„° ì¶”ì¶œ)

ì˜ˆì‹œ:
1. ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
2. ë§í¬ ìˆ˜ì§‘
3. ê° ë§í¬ ë°©ë¬¸
4. ë” ë§ì€ ë§í¬ ë°œê²¬
5. ë°˜ë³µ...

Scraperrì˜ í¬ë¡¤ë§:
spider_mode: true
max_depth: 3  # 3ë‹¨ê³„ê¹Œì§€
follow_links: "//a[@class='product']"
```

---

**CSS Selector (CSS ì„ íƒì)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ ìš”ì†Œë¥¼ ì°¾ëŠ” ë°©ë²• (XPathì˜ ëŒ€ì•ˆ)
ë¬¸ë²•ì´ ë” ê°„ë‹¨í•¨

ê¸°ë³¸ ë¬¸ë²•:
.classname  â†’ classë¡œ ì°¾ê¸°
#idname     â†’ idë¡œ ì°¾ê¸°
tag         â†’ íƒœê·¸ë¡œ ì°¾ê¸°
[attr=val]  â†’ ì†ì„±ìœ¼ë¡œ ì°¾ê¸°

ì˜ˆì‹œ:
HTML:
<div class="price">1,000ì›</div>

CSS Selector:
.price  ë˜ëŠ”  div.price

XPath (ë¹„êµ):
//div[@class='price']

ì–¸ì œ ì‚¬ìš©:
- ê°„ë‹¨í•œ ì„ íƒ
- ë¸Œë¼ìš°ì € ì¹œí™”ì 
- ì†ë„ ì¤‘ìš”í•  ë•Œ
```

---

### D

**DOM (Document Object Model)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹í˜ì´ì§€ì˜ ê°€ê³„ë„
ì¼ìƒ ë¹„ìœ : ê°€ì¡± ê´€ê³„ë„

êµ¬ì¡°:
document (ìµœìƒìœ„)
â”œâ”€â”€ html
    â”œâ”€â”€ head
    â”‚   â”œâ”€â”€ title
    â”‚   â””â”€â”€ meta
    â””â”€â”€ body
        â”œâ”€â”€ div
        â”‚   â”œâ”€â”€ h1
        â”‚   â””â”€â”€ p
        â””â”€â”€ div

ìš©ì–´:
- ë¶€ëª¨ (Parent): ìœ„ ìš”ì†Œ
- ìì‹ (Child): ì•„ë˜ ìš”ì†Œ
- í˜•ì œ (Sibling): ê°™ì€ ë ˆë²¨

ì™œ ì¤‘ìš”í•œê°€:
ìŠ¤í¬ë˜í•‘ = DOM íŠ¸ë¦¬ íƒìƒ‰
```

---

### E

**ETL (Extract, Transform, Load)**
```
ì‰¬ìš´ ì„¤ëª…: ë°ì´í„° ì²˜ë¦¬ì˜ 3ë‹¨ê³„
ì¼ìƒ ë¹„ìœ : ìš”ë¦¬ ê³¼ì •

Extract (ì¶”ì¶œ):
- ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
- Scraperrì˜ ì—­í• 

Transform (ë³€í™˜):
- ë°ì´í„° ì •ì œ (HTML íƒœê·¸ ì œê±°)
- í˜•ì‹ ë³€í™˜ (ë‚ ì§œ, ìˆ«ì)
- ì¤‘ë³µ ì œê±°

Load (ì ì¬):
- ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
- CSV íŒŒì¼ ìƒì„±

ì „ì²´ íë¦„:
ì›¹ì‚¬ì´íŠ¸ â†’ ìŠ¤í¬ë˜í•‘ â†’ ì •ì œ â†’ ë°ì´í„°ë² ì´ìŠ¤
```

---

### F

**FastAPI**
```
ì‰¬ìš´ ì„¤ëª…: ë¹ ë¥¸ Python ì›¹ í”„ë ˆì„ì›Œí¬
Scraperrì˜ ë°±ì—”ë“œ

íŠ¹ì§•:
- ë§¤ìš° ë¹ ë¦„ âš¡
- ìë™ API ë¬¸ì„œ
- ë¹„ë™ê¸° ì§€ì›
- íƒ€ì… ì²´í¬

Scraperrì—ì„œ ì—­í• :
í”„ë¡ íŠ¸ì—”ë“œ â†” FastAPI â†” ìŠ¤í¬ë˜í•‘ ì—”ì§„
         ìš”ì²­        ì‘ì—… ì‹¤í–‰
```

---

### H

**HTML (HyperText Markup Language)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹í˜ì´ì§€ì˜ ë¼ˆëŒ€
ì¼ìƒ ë¹„ìœ : ê±´ë¬¼ ì„¤ê³„ë„

êµ¬ì¡°:
<html>
  <body>
    <h1>ì œëª©</h1>
    <p>ë‚´ìš©</p>
  </body>
</html>

íƒœê·¸ (Tag):
- <h1>: í° ì œëª©
- <p>: ë¬¸ë‹¨
- <div>: êµ¬ì—­
- <span>: ì‘ì€ í…ìŠ¤íŠ¸

ì†ì„± (Attribute):
<div class="price" id="main">

ìŠ¤í¬ë˜í•‘ì˜ ëª©í‘œ:
HTMLì—ì„œ ì›í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
```

---

### J

**JSON (JavaScript Object Notation)**
```
ì‰¬ìš´ ì„¤ëª…: ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ëŠ” í˜•ì‹
ì¼ìƒ ë¹„ìœ : ì •ë¦¬ëœ ë©”ëª¨

í˜•ì‹:
{
  "name": "iPhone 15",
  "price": 1200000,
  "stock": 50,
  "tags": ["ìŠ¤ë§ˆíŠ¸í°", "ì• í”Œ"]
}

vs CSV:
JSON:
- ê³„ì¸µ êµ¬ì¡° ê°€ëŠ¥
- ë°°ì—´, ê°ì²´ í‘œí˜„
- APIì— ì£¼ë¡œ ì‚¬ìš©

CSV:
- í‘œ í˜•íƒœë§Œ
- ê°„ë‹¨í•¨
- Excelì—ì„œ ì—´ê¸° ì‰¬ì›€

Scraperr ì‚¬ìš©:
export_format: "json"
```

---

### M

**MongoDB**
```
ì‰¬ìš´ ì„¤ëª…: ë¬¸ì„œ ì €ì¥ ë°ì´í„°ë² ì´ìŠ¤
ì¼ìƒ ë¹„ìœ : íŒŒì¼ ìºë¹„ë‹›

vs ì „í†µì  DB (MySQL):
MySQL: í‘œ í˜•íƒœ (í–‰, ì—´)
MongoDB: ë¬¸ì„œ í˜•íƒœ (JSON)

ì¥ì :
- ìœ ì—°í•œ êµ¬ì¡°
- ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‰¬ì›€
- ëŒ€ìš©ëŸ‰ ë°ì´í„°

Scraperrì—ì„œ:
ìŠ¤í¬ë˜í•‘ ê²°ê³¼ â†’ MongoDB
ì›¹ UIì—ì„œ ì¡°íšŒ
```

---

### N

**Next.js**
```
ì‰¬ìš´ ì„¤ëª…: React ê¸°ë°˜ ì›¹ í”„ë ˆì„ì›Œí¬
Scraperrì˜ í”„ë¡ íŠ¸ì—”ë“œ

íŠ¹ì§•:
- ë¹ ë¥¸ í˜ì´ì§€ ë¡œë”©
- SEO ì¹œí™”ì 
- ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§

Scraperrì—ì„œ ì—­í• :
ì‚¬ìš©ì â†” Next.js UI â†” FastAPI
      ë³´ê¸°/ì„¤ì •      ë°ì´í„°
```

---

### P

**Pagination (í˜ì´ì§€ë„¤ì´ì…˜)**
```
ì‰¬ìš´ ì„¤ëª…: ì—¬ëŸ¬ í˜ì´ì§€ë¡œ ë‚˜ë‰œ ê²°ê³¼
ì¼ìƒ ë¹„ìœ : ì±…ì˜ í˜ì´ì§€

í˜•íƒœ:
1. ë²ˆí˜¸: [1] [2] [3] [ë‹¤ìŒ]
2. ë¬´í•œ ìŠ¤í¬ë¡¤: ê³„ì† ë¡œë”©
3. "ë”ë³´ê¸°" ë²„íŠ¼

ìŠ¤í¬ë˜í•‘ ì „ëµ:
ë°©ë²• 1: í˜ì´ì§€ ë²ˆí˜¸ ì¦ê°€
products?page=1
products?page=2
...

ë°©ë²• 2: "ë‹¤ìŒ" ë²„íŠ¼ í´ë¦­
//a[@class='next']

Scraperr ì„¤ì •:
pagination:
  type: "numbered"
  max_pages: 10
```

---

**Playwright**
```
ì‰¬ìš´ ì„¤ëª…: ë¸Œë¼ìš°ì € ìë™í™” ë„êµ¬
ì¼ìƒ ë¹„ìœ : ë¡œë´‡ì´ ë¸Œë¼ìš°ì € ì¡°ì‘

ê¸°ëŠ¥:
- ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
- í´ë¦­, ì…ë ¥, ìŠ¤í¬ë¡¤
- JavaScript ì‹¤í–‰
- ìŠ¤í¬ë¦°ìƒ·

vs Selenium:
Playwright: ë¹ ë¦„, í˜„ëŒ€ì 
Selenium: ì˜¤ë˜ë¨, ë„ë¦¬ ì‚¬ìš©

Scraperrì—ì„œ:
ë™ì  ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ì— í•„ìˆ˜
```

---

### R

**Rate Limiting (ì†ë„ ì œí•œ)**
```
ì‰¬ìš´ ì„¤ëª…: ìš”ì²­ ì†ë„ ì¡°ì ˆ
ì¼ìƒ ë¹„ìœ : ì²œì²œíˆ ê±·ê¸°

ì™œ í•„ìš”:
1. ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸°
2. ì°¨ë‹¨ ë°©ì§€
3. ì˜ˆì˜

ì„¤ì •:
rate_limit:
  requests_per_second: 2
  delay_between_pages: 3

ì˜ë¯¸:
- ì´ˆë‹¹ 2ê°œ ìš”ì²­
- í˜ì´ì§€ ì‚¬ì´ 3ì´ˆ ëŒ€ê¸°
```

---

**Regex (ì •ê·œí‘œí˜„ì‹)**
```
ì‰¬ìš´ ì„¤ëª…: í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
ì¼ìƒ ë¹„ìœ : ê²€ìƒ‰ ë§ˆë²•

ì˜ˆì‹œ:
\d+        â†’ ìˆ«ì
[0-9]+     â†’ ìˆ«ì (ê°™ìŒ)
\w+        â†’ ë‹¨ì–´
.+         â†’ ëª¨ë“  ê¸€ì

í™œìš©:
ê°€ê²© ì¶”ì¶œ:
"1,200,000ì›" â†’ 1200000

ì „í™”ë²ˆí˜¸:
"010-1234-5678" â†’ 01012345678

Scraperr:
transform: "regex"
pattern: "\d+"
```

---

**robots.txt**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ê·œì¹™
ì¼ìƒ ë¹„ìœ : "ì¶œì…ê¸ˆì§€" í‘œì§€íŒ

ìœ„ì¹˜:
https://website.com/robots.txt

ë‚´ìš©:
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /products/
Crawl-delay: 5

ì˜ë¯¸:
- admin, private ê¸ˆì§€
- products í—ˆìš©
- 5ì´ˆ ê°„ê²© ìš”ì²­

Scraperr:
ìë™ìœ¼ë¡œ robots.txt í™•ì¸
ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ìœ¤ë¦¬ì  ë¬¸ì œ
```

---

### S

**Scraping (ìŠ¤í¬ë˜í•‘)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
ì¼ìƒ ë¹„ìœ : ì‹ ë¬¸ì—ì„œ ê¸°ì‚¬ ì˜¤ë¦¬ê¸°

vs Crawling:
Scraping: ë°ì´í„° ìˆ˜ì§‘
Crawling: ì‚¬ì´íŠ¸ íƒìƒ‰

í•©ë²•ì„±:
âœ… ê³µê°œëœ ì •ë³´
âœ… robots.txt ì¤€ìˆ˜
âœ… ì ì ˆí•œ ì†ë„
âŒ ê°œì¸ì •ë³´
âŒ ë¡œê·¸ì¸ í•„ìš” ë°ì´í„° (í—ˆê°€ ì—†ì´)
```

---

**Spider (ìŠ¤íŒŒì´ë”)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ì‚¬ì´íŠ¸ë¥¼ ëŒì•„ë‹¤ë‹ˆëŠ” ë´‡
ì¼ìƒ ë¹„ìœ : ê±°ë¯¸ì¤„ì²˜ëŸ¼ ì—°ê²° ë”°ë¼ê°€ê¸°

ë™ì‘:
1. ì‹œì‘ URL
2. í˜ì´ì§€ ë¶„ì„
3. ë§í¬ ì¶”ì¶œ
4. ë§í¬ ë°©ë¬¸
5. ë°˜ë³µ

Scraperrì˜ ìŠ¤íŒŒì´ë”:
spider_mode: true
follow_links: true
max_depth: 3
```

---

### U

**User-Agent**
```
ì‰¬ìš´ ì„¤ëª…: "ë‚˜ëŠ” ì´ëŸ° ë¸Œë¼ìš°ì €ì˜ˆìš”" ì†Œê°œ
ì¼ìƒ ë¹„ìœ : ëª…í•¨

ì˜ˆì‹œ:
Mozilla/5.0 (Windows NT 10.0) Chrome/120.0

êµ¬ì„±:
- OS: Windows 10
- ë¸Œë¼ìš°ì €: Chrome 120

ì™œ ì¤‘ìš”:
ì„œë²„ê°€ ë´‡ì¸ì§€ íŒë‹¨í•˜ëŠ” ê¸°ì¤€
Python ê¸°ë³¸ User-Agent = ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ

Scraperr ì„¤ì •:
headers:
  User-Agent: "Mozilla/5.0..."
```

---

### W

**Webhook**
```
ì‰¬ìš´ ì„¤ëª…: ìë™ ì•Œë¦¼
ì¼ìƒ ë¹„ìœ : ì´ˆì¸ì¢…

ë™ì‘:
ì´ë²¤íŠ¸ ë°œìƒ â†’ Webhook í˜¸ì¶œ
ì˜ˆ: ìŠ¤í¬ë˜í•‘ ì™„ë£Œ â†’ Slack ë©”ì‹œì§€

Scraperr ì‚¬ìš©:
notifications:
  webhook:
    url: "https://hooks.slack.com/..."
    on: ["complete", "error"]
```

---

### X

**XPath (XML Path Language)**
```
ì‰¬ìš´ ì„¤ëª…: ì›¹ ìš”ì†Œ ì£¼ì†Œ
ì¼ìƒ ë¹„ìœ : ìš°í¸ ì£¼ì†Œ

ë¬¸ë²•:
//      ëª¨ë“  ê³³ì—ì„œ ê²€ìƒ‰
/       ì§ì ‘ ìì‹ë§Œ
[@]     ì†ì„± ì¡°ê±´
text()  í…ìŠ¤íŠ¸ë§Œ

ì˜ˆì‹œ:
//div[@class='price']/text()
â†’ class='price'ì¸ divì˜ í…ìŠ¤íŠ¸

ì¶•ì•½:
//div[@class='price']//span
â†’ price div ì•ˆì˜ ëª¨ë“  span

Scraperrì˜ í•µì‹¬:
ëª¨ë“  ë°ì´í„° ì¶”ì¶œì˜ ê¸°ë³¸
```

---

### Y

**YAML (YAML Ain't Markup Language)**
```
ì‰¬ìš´ ì„¤ëª…: ì„¤ì • íŒŒì¼ í˜•ì‹
ì¼ìƒ ë¹„ìœ : ë ˆì‹œí”¼

íŠ¹ì§•:
- ì½ê¸° ì‰¬ì›€
- ë“¤ì—¬ì“°ê¸°ë¡œ êµ¬ì¡° í‘œí˜„
- ì£¼ì„ ê°€ëŠ¥ (#)

ì˜ˆì‹œ:
name: "My Scraper"
targets:
  - url: "https://example.com"
    selector: "//div[@class='item']"

Scraperrì—ì„œ:
ëª¨ë“  ì„¤ì •ì„ YAMLë¡œ ì‘ì„±
```

---

## ë§ˆë¬´ë¦¬í•˜ë©°

### ğŸ“ í•µì‹¬ ìš”ì•½

**Scraperrë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ:**
"ì½”ë”© ì—†ì´ í´ë¦­ë§Œìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ë¥¼ ìë™ ìˆ˜ì§‘í•˜ê³ , ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì£¼ëŠ” ì…€í”„ í˜¸ìŠ¤íŒ… ë„êµ¬"

**ê¼­ ê¸°ì–µí•  5ê°€ì§€:**
1. **ë…¸ì½”ë“œ ìŠ¤í¬ë˜í•‘** â†’ ê°œë°œì ì•„ë‹ˆì–´ë„ OK
2. **XPath ìë™ ìƒì„±** â†’ ë³µì‚¬ë§Œ í•˜ë©´ ë¨
3. **ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬** â†’ ìˆ˜ì²œ í˜ì´ì§€ ìë™í™”
4. **ë‹¤ì–‘í•œ ë‚´ë³´ë‚´ê¸°** â†’ CSV, JSON, DB
5. **ìœ¤ë¦¬ì  ì‚¬ìš©** â†’ robots.txt ì¤€ìˆ˜ í•„ìˆ˜

---

### ğŸš€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ê¸°

**10ë¶„ í€µ ìŠ¤íƒ€íŠ¸:**
```bash
# 1. ë‹¤ìš´ë¡œë“œ
git clone https://github.com/jaypyles/Scraperr.git
cd Scraperr

# 2. ì‹¤í–‰ (Docker í•„ìš”)
make up

# 3. ë¸Œë¼ìš°ì € ì ‘ì†
http://localhost:3000

# 4. ì²« ìŠ¤í¬ë˜í•‘ ì‘ì—… ìƒì„±!
```

**ì²« ì£¼ë§ ëª©í‘œ:**
- [ ] Scraperr ì„¤ì¹˜ ë° ì‹¤í–‰
- [ ] ê°„ë‹¨í•œ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì„±ê³µ
- [ ] CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
- [ ] XPath ì‚¬ìš©ë²• ì´í•´
- [ ] ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ êµ¬ìƒ

---

### ğŸ’¬ ë„ì›€ ë°›ê¸°

**ê³µì‹ ìë£Œ:**
- [GitHub Repository](https://github.com/jaypyles/Scraperr)
- [Documentation](https://scraperr-docs.pages.dev/)
- [Discord Community](https://discord.gg/89q7scsGEK)

**í•™ìŠµ ë¡œë“œë§µ:**
```
Week 1: ê¸°ì´ˆ
- Scraperr ì„¤ì¹˜
- ì²« ìŠ¤í¬ë˜í•‘ ì‘ì—…
- XPath ê¸°ë³¸

Week 2: ì‹¤ë¬´
- í˜ì´ì§€ë„¤ì´ì…˜
- ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
- ë°ì´í„° ì •ì œ

Week 3: ê³ ê¸‰
- ë™ì  ì›¹ì‚¬ì´íŠ¸
- API ì¸í„°ì…‰íŠ¸
- ìë™í™” ìŠ¤ì¼€ì¤„ë§

Week 4: ë§ˆìŠ¤í„°
- ë¶„ì‚° ìŠ¤í¬ë˜í•‘
- AI ê²€ì¦
- í”„ë¡œë•ì…˜ ë°°í¬
```

---

### ğŸ¯ ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ë°ì´í„° ìˆ˜ì§‘ ìë™í™” ì²´í¬:**
- [ ] ìˆ˜ë™ ì‘ì—… ì‹œê°„ 50% ì´ìƒ ê°ì†Œ
- [ ] ì •í™•ë„ 95% ì´ìƒ
- [ ] ë²•ì /ìœ¤ë¦¬ì  ë¬¸ì œ ì—†ìŒ
- [ ] ìŠ¤ì¼€ì¤„ ìë™ ì‹¤í–‰ ì„¤ì •
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ êµ¬í˜„
- [ ] ë°ì´í„° ê²€ì¦ íŒŒì´í”„ë¼ì¸
- [ ] íŒ€ê³¼ ì§€ì‹ ê³µìœ  ì™„ë£Œ

**ëª¨ë‘ ì²´í¬ë˜ë©´ Scraperr ë§ˆìŠ¤í„°! ğŸ‰**

---

### ğŸŒŸ ë‹¤ìŒ ë‹¨ê³„

**ê´€ë ¨ ë„êµ¬ í•™ìŠµ:**
- **BeautifulSoup**: Python ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Selenium**: ë¸Œë¼ìš°ì € ìë™í™”
- **Scrapy**: í”„ë¡œí˜ì…”ë„ ìŠ¤í¬ë˜í•‘ í”„ë ˆì„ì›Œí¬
- **Apache Airflow**: ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**ì‹¬í™” ì£¼ì œ:**
- ëŒ€ê·œëª¨ ë¶„ì‚° ìŠ¤í¬ë˜í•‘
- ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° íŒŒì´í”„ë¼ì¸
- ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
- ìŠ¤í¬ë˜í•‘ ë²•ë¥  ë° ìœ¤ë¦¬

---

## ì—°ê²°ëœ ë…¸íŠ¸
- [[ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ì´ˆ]]
- [[XPath ì™„ë²½ ê°€ì´ë“œ]]
- [[ë°ì´í„° ìˆ˜ì§‘ ìë™í™”]]
- [[Docker ì»¨í…Œì´ë„ˆ ê´€ë¦¬]]
- [[FastAPI ë°±ì—”ë“œ ê°œë°œ]]
- [[MongoDB ë°ì´í„°ë² ì´ìŠ¤]]
- [[ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸]]

---

**ë§ˆì§€ë§‰ ì¡°ì–¸:**
> "ScraperrëŠ” ë‹¨ìˆœí•œ ë„êµ¬ê°€ ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì˜ ë¯¼ì£¼í™”ì…ë‹ˆë‹¤.
>
> ê°œë°œìê°€ ì•„ë‹ˆì–´ë„, ë³µì¡í•œ ì½”ë“œë¥¼ ëª°ë¼ë„, ëˆ„êµ¬ë‚˜ ì›¹ì˜ ë°©ëŒ€í•œ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
>
> í•˜ì§€ë§Œ ê°•ë ¥í•œ ë„êµ¬ì¸ ë§Œí¼, ì±…ì„ê° ìˆê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. robots.txtë¥¼ ì¡´ì¤‘í•˜ê³ , ì„œë¹„ìŠ¤ ì•½ê´€ì„ ì¤€ìˆ˜í•˜ë©°, ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ë§ˆì„¸ìš”.
>
> ìœ¤ë¦¬ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ë©´, ScraperrëŠ” ë‹¹ì‹ ì˜ ì—…ë¬´ë¥¼ í˜ì‹ í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ìµœê³ ì˜ íŒŒíŠ¸ë„ˆê°€ ë  ê²ƒì…ë‹ˆë‹¤!"

**Happy Scraping! ğŸ•·ï¸âœ¨**